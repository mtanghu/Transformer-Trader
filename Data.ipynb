{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Timedelta\n",
    "import numpy as np\n",
    "\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = pd.read_csv('data/OANDA/EUR_USD.csv')\n",
    "\n",
    "fx['datetime'] = pd.to_datetime(fx['datetime'], infer_datetime_format = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx.duplicated(subset = 'datetime').any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to eastern time so that day light savings can be removed then back to UTC\n",
    "fx['datetime'] = fx['datetime'].dt.tz_localize(\n",
    "    'US/Eastern', ambiguous = fx['datetime'].astype(bool), nonexistent='shift_forward'\n",
    ")\n",
    "fx['datetime'] = fx['datetime'].dt.tz_convert(None) - Timedelta('5 hours')\n",
    "assert not fx.duplicated(subset = 'datetime').any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "- Step 1: linearly interpolate gaps (shoot, not sure how to do this in parallel besides filling in all gaps and then getting rid of friday -> sunday\n",
    "- Step 2: so we will go from 21:00 to 21:00 from sunday to friday\n",
    "    - make a \"trading day\" feature starting from 21:00 of \n",
    "    - get rid of stragler days with no trades at end and beginning if the don't match up right\n",
    "- Step 3: subtract the day's open at 21:00 from each day (group by day easy) -- let's not honestly...\n",
    "- Step 4: make relative volume indicator using comparing real volume to groupby day's MA -- yeah I mean for now let's not\n",
    "- Step 5: 60min HLC\n",
    "\n",
    "- Step 6: create train, validation and test datasets\n",
    "- Step 7: divide each column by their std (ungrouped) USING ONLY TRAINING SET MAKING SURE TO SAVE THIS NUMBER -- not for now\n",
    "\n",
    "\n",
    "Make JSON:\n",
    "- make the weekday and relative volume a feature\n",
    "- make inputs_embeds a 2d array containing OHLCV and label 2D array of HLC (no day of week)\n",
    "- store as JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linearly interpolate time gaps\n",
    "fx = fx.set_index('datetime').asfreq('1min')\n",
    "fx['close'] = fx['close'].fillna(method = 'ffill')\n",
    "fx['volume'] = fx['volume'].fillna(0) # no volume in gaps (according to first rate data people)\n",
    "\n",
    "# fill open high and low with most recent close\n",
    "fx = fx.fillna({\n",
    "    'open': fx['close'],\n",
    "    'high': fx['close'],\n",
    "    'low': fx['close']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the \"trading day\" as from starting on 9pm UTC of the previous day ending at 8:59pm UTC\n",
    "fx['day'] = fx.index.dayofweek \n",
    "fx.loc[fx.between_time('22:00', '23:59').index, 'day'] += 1\n",
    "fx['day'] = fx['day'] % 7 # sunday = 6, then + 1 would be 7 but we want that roll over to monday = 0\n",
    "\n",
    "# ordinal trading day since start of data (just for help in preprocessing)\n",
    "fx['ordinal_day'] = (fx['day'] != fx['day'].shift()).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at distribution of how many 0 volume minute\n",
    "(fx['volume'] == 0.).groupby(fx['ordinal_day']).sum().hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's just arbitrary say there should be less than 100 zero volume -- shouldn't really mattter in the end\n",
    "voluminous_index = (fx['volume'] == 0.).groupby(fx['ordinal_day']).filter(lambda x: x.sum() < 200).index\n",
    "fx = fx.loc[voluminous_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>ordinal_day</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-01-02 21:59:00</th>\n",
       "      <td>1.35475</td>\n",
       "      <td>1.35475</td>\n",
       "      <td>1.35475</td>\n",
       "      <td>1.35475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-02 22:00:00</th>\n",
       "      <td>1.35475</td>\n",
       "      <td>1.35475</td>\n",
       "      <td>1.35475</td>\n",
       "      <td>1.35475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-02 22:01:00</th>\n",
       "      <td>1.35475</td>\n",
       "      <td>1.35475</td>\n",
       "      <td>1.35475</td>\n",
       "      <td>1.35475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-03 21:59:00</th>\n",
       "      <td>1.34640</td>\n",
       "      <td>1.34650</td>\n",
       "      <td>1.34630</td>\n",
       "      <td>1.34650</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-03 22:00:00</th>\n",
       "      <td>1.34650</td>\n",
       "      <td>1.34660</td>\n",
       "      <td>1.34640</td>\n",
       "      <td>1.34640</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-30 22:00:00</th>\n",
       "      <td>0.99563</td>\n",
       "      <td>0.99567</td>\n",
       "      <td>0.99552</td>\n",
       "      <td>0.99562</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-30 22:01:00</th>\n",
       "      <td>0.99563</td>\n",
       "      <td>0.99568</td>\n",
       "      <td>0.99555</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-31 21:59:00</th>\n",
       "      <td>0.98838</td>\n",
       "      <td>0.98839</td>\n",
       "      <td>0.98837</td>\n",
       "      <td>0.98837</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-31 22:00:00</th>\n",
       "      <td>0.98837</td>\n",
       "      <td>0.98840</td>\n",
       "      <td>0.98836</td>\n",
       "      <td>0.98840</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-10-31 22:01:00</th>\n",
       "      <td>0.98841</td>\n",
       "      <td>0.98841</td>\n",
       "      <td>0.98830</td>\n",
       "      <td>0.98835</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13680 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       close     high      low     open  volume  day  \\\n",
       "datetime                                                               \n",
       "2005-01-02 21:59:00  1.35475  1.35475  1.35475  1.35475     0.0    6   \n",
       "2005-01-02 22:00:00  1.35475  1.35475  1.35475  1.35475     0.0    0   \n",
       "2005-01-02 22:01:00  1.35475  1.35475  1.35475  1.35475     0.0    0   \n",
       "2005-01-03 21:59:00  1.34640  1.34650  1.34630  1.34650     6.0    0   \n",
       "2005-01-03 22:00:00  1.34650  1.34660  1.34640  1.34640     7.0    1   \n",
       "...                      ...      ...      ...      ...     ...  ...   \n",
       "2022-10-30 22:00:00  0.99563  0.99567  0.99552  0.99562    86.0    0   \n",
       "2022-10-30 22:01:00  0.99563  0.99568  0.99555  0.99560    94.0    0   \n",
       "2022-10-31 21:59:00  0.98838  0.98839  0.98837  0.98837     7.0    0   \n",
       "2022-10-31 22:00:00  0.98837  0.98840  0.98836  0.98840    16.0    1   \n",
       "2022-10-31 22:01:00  0.98841  0.98841  0.98830  0.98835    43.0    1   \n",
       "\n",
       "                     ordinal_day  \n",
       "datetime                          \n",
       "2005-01-02 21:59:00            1  \n",
       "2005-01-02 22:00:00            2  \n",
       "2005-01-02 22:01:00            2  \n",
       "2005-01-03 21:59:00            2  \n",
       "2005-01-03 22:00:00            3  \n",
       "...                          ...  \n",
       "2022-10-30 22:00:00         6512  \n",
       "2022-10-30 22:01:00         6512  \n",
       "2022-10-31 21:59:00         6512  \n",
       "2022-10-31 22:00:00         6513  \n",
       "2022-10-31 22:01:00         6513  \n",
       "\n",
       "[13680 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick sanity check of data \n",
    "fx.between_time('21:59', '22:01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pandas import Timedelta\n",
    "import numpy as np\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset, concatenate_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start with processing all CSVs to datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(filename, periods = [5, 10, 15, 20, 30, 45, 60, 90, 120], num_cuts = 10, return_df = False):\n",
    "    fx = pd.read_csv(filename)\n",
    "    \n",
    "    fx['datetime'] = pd.to_datetime(fx['datetime'], infer_datetime_format = True)\n",
    "    fx = fx[fx['datetime'].dt.year >= 2009].reset_index(drop = True)\n",
    "    \n",
    "    # convert to eastern time so that day light savings can be removed then back to UTC\n",
    "    fx['datetime'] = fx['datetime'].dt.tz_localize(\n",
    "        'US/Eastern', ambiguous = fx['datetime'].astype(bool), nonexistent='shift_forward'\n",
    "    )\n",
    "    fx['datetime'] = fx['datetime'].dt.tz_convert(None) - Timedelta('5 hours')\n",
    "    fx = fx.drop_duplicates(subset = 'datetime', keep = 'last')\n",
    "    assert not fx.duplicated(subset = 'datetime').any()\n",
    "    \n",
    "    # Linearly interpolate time gaps\n",
    "    fx = fx.set_index('datetime').asfreq('1min')\n",
    "    fx['close'] = fx['close'].fillna(method = 'ffill')\n",
    "    fx['volume'] = fx['volume'].fillna(0) # no volume in gaps (according to first rate data people)\n",
    "\n",
    "    # fill open high and low with most recent close\n",
    "    fx = fx.fillna({\n",
    "        'open': fx['close'],\n",
    "        'high': fx['close'],\n",
    "        'low': fx['close']\n",
    "    })\n",
    "    \n",
    "    # define the \"trading day\" as from starting on 9pm UTC of the previous day ending at 8:59pm UTC\n",
    "    fx['day'] = fx.index.dayofweek \n",
    "    fx.loc[fx.between_time('22:00', '23:59').index, 'day'] += 1\n",
    "    fx['day'] = fx['day'] % 7 # sunday = 6, then + 1 would be 7 but we want that roll over to monday = 0\n",
    "\n",
    "    # ordinal trading day since start of data (just for help in preprocessing)\n",
    "    fx['ordinal_day'] = (fx['day'] != fx['day'].shift()).cumsum()\n",
    "    \n",
    "    # let's just arbitrary say there should be less than 200 zero volume -- shouldn't really mattter in the end\n",
    "    voluminous_index = (fx['volume'] == 0.).groupby(fx['ordinal_day']).filter(lambda x: x.sum() < 400).index\n",
    "    fx = fx.loc[voluminous_index]\n",
    "\n",
    "    # make deciles for possible outcomes\n",
    "    # this may violate test-train split principles but require significant memorization from model\n",
    "    futures = {}\n",
    "    for i in periods:\n",
    "        futures_name = f'future_diff{i}'\n",
    "        future_diff = fx['close'].shift(-i) - fx['close']\n",
    "        futures[futures_name] = future_diff\n",
    "\n",
    "    futures_df = pd.DataFrame(futures)\n",
    "    future_cols = futures_df.columns\n",
    "    fx = pd.concat((fx, futures_df), axis = 1)\n",
    "\n",
    "    # clean up data odditity of future columns coming from the future of non-consecutive days (i.e. no overnight trading)\n",
    "    fx['day_break'] = ((fx['day'] != (fx['day']).shift(-1)) & (fx['day'] != (fx['day'] - 1).shift(-1)))\n",
    "    for i in periods:\n",
    "        futures_name = f'future_diff{i}'\n",
    "        no_futures = fx[futures_name][\n",
    "            fx.groupby('ordinal_day')['day_break'].transform('any')\n",
    "        ].between_time(f'{21 - i // 60}:{59 - i % 60}', '21:59').index\n",
    "        fx.loc[no_futures, futures_name] = 0\n",
    "    \n",
    "#     std_future_cols = \"std_\" + future_cols\n",
    "#     fx[std_future_cols] = fx[future_cols] / fx.iloc[:int(len(fx) * .9)][future_cols].std()\n",
    "    \n",
    "    # form labels after overnight correction\n",
    "    labels = {}\n",
    "    cuts = {}\n",
    "    for i in periods:\n",
    "        futures_name = f'future_diff{i}'\n",
    "        labels_name = f'label{i}'\n",
    "        \n",
    "        labels[labels_name] = pd.qcut(fx[futures_name], num_cuts, labels = False)\n",
    "        cuts[i] = fx[futures_name].groupby(labels[labels_name]).median()\n",
    "        \n",
    "    labels_df = pd.DataFrame(labels)\n",
    "    labels_df = labels_df.fillna(-100).astype(int)\n",
    "    label_cols = labels_df.columns\n",
    "    fx = pd.concat((fx, labels_df), axis = 1)\n",
    "    \n",
    "    \n",
    "    # fix overnight labels to be -100 (i.e. ignored)\n",
    "    for i in periods:\n",
    "        labels_name = f'label{i}'\n",
    "        no_futures = fx[futures_name][\n",
    "            fx.groupby('ordinal_day')['day_break'].transform('any')\n",
    "        ].between_time(f'{21 - i // 60}:{59 - i % 60}', '21:59').index\n",
    "        fx.loc[no_futures, labels_name] = -100\n",
    "\n",
    "    # will be returned at the end so we know what the categories represent\n",
    "    cuts = pd.DataFrame(cuts)\n",
    "    \n",
    "    features = ['open', 'high', 'low', 'close']\n",
    "    \n",
    "    # de mean prices by subtracting the first close of the day\n",
    "    fx[features] = fx[features].subtract(\n",
    "        fx.groupby('ordinal_day')['close'].transform('first'), axis = 0\n",
    "    )\n",
    "\n",
    "    # de standardize based on first 90% of data (to avoid test train split issue)\n",
    "    stds = fx.iloc[:int(len(fx) * .9)][features].std(axis = 0)\n",
    "    fx[features] = fx[features].div(stds, axis = 1)\n",
    "\n",
    "    # get rid of first day and last day due to data incompleteness\n",
    "    fx = fx.drop(fx[fx['ordinal_day'] == fx['ordinal_day'].min()].index)\n",
    "    fx = fx.drop(fx[fx['ordinal_day'] == fx['ordinal_day'].max()].index)\n",
    "    \n",
    "    if return_df:\n",
    "        return fx\n",
    "\n",
    "    ohlcv = fx[features].values.reshape(-1, 1440, len(features))\n",
    "    labels = fx[label_cols].values.reshape(-1, 1440, len(periods))\n",
    "    future = fx[future_cols].values.reshape(-1, 1440, len(periods))\n",
    "#     std_future = fx[std_future_cols].values.reshape(-1, 1440, len(periods))\n",
    "    \n",
    "    # save some memory\n",
    "    del fx\n",
    "    \n",
    "#     ds = Dataset.from_dict({\"ohlcv\": ohlcv, \"labels\": labels, \"future\": future, \"std_future\": std_future})\n",
    "    ds = Dataset.from_dict({\"ohlcv\": ohlcv, \"labels\": labels, \"future\": future})\n",
    "    \n",
    "    return ds, cuts, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AUD_CAD.csv',\n",
       " 'AUD_JPY.csv',\n",
       " 'AUD_NZD.csv',\n",
       " 'AUD_USD.csv',\n",
       " 'EUR_AUD.csv',\n",
       " 'EUR_CHF.csv',\n",
       " 'EUR_GBP.csv',\n",
       " 'EUR_JPY.csv',\n",
       " 'EUR_NZD.csv',\n",
       " 'EUR_USD.csv',\n",
       " 'GBP_AUD.csv',\n",
       " 'GBP_CAD.csv',\n",
       " 'GBP_CHF.csv',\n",
       " 'GBP_JPY.csv',\n",
       " 'GBP_USD.csv',\n",
       " 'NZD_JPY.csv',\n",
       " 'NZD_USD.csv',\n",
       " 'USD_CAD.csv',\n",
       " 'USD_CHF.csv',\n",
       " 'USD_JPY.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = './data/OANDA/'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "majors = [\n",
    "    'AUD_USD.csv',\n",
    "    'EUR_CHF.csv',\n",
    "    'EUR_GBP.csv',\n",
    "    'EUR_JPY.csv',\n",
    "    'EUR_USD.csv',\n",
    "    'GBP_JPY.csv',\n",
    "    'GBP_USD.csv',\n",
    "    'USD_CAD.csv',\n",
    "    'USD_CHF.csv',\n",
    "    'USD_JPY.csv'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USD_JPY.csv\r"
     ]
    }
   ],
   "source": [
    "# for file in majors: # if only majors are wanted\n",
    "for file in os.listdir(data_dir):\n",
    "    dataset, cuts, stds = make_dataset(data_dir + file)\n",
    "    dataset.save_to_disk(f\"data/OANDA_DS/{file[:7]}.ds\")\n",
    "    cuts.to_csv(f\"data/OANDA_DS/{file[:7]}.ds/cuts.csv\", index = False)\n",
    "    stds.to_csv(f\"data/OANDA_DS/{file[:7]}.ds/stds.csv\", index = False)\n",
    "    print(file, end = '\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat all datasets (done in 2 stages due to memory contraints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AUD_CAD.ds',\n",
       " 'AUD_JPY.ds',\n",
       " 'AUD_NZD.ds',\n",
       " 'AUD_USD.ds',\n",
       " 'EUR_AUD.ds',\n",
       " 'EUR_CHF.ds',\n",
       " 'EUR_GBP.ds',\n",
       " 'EUR_JPY.ds',\n",
       " 'EUR_NZD.ds',\n",
       " 'EUR_USD.ds',\n",
       " 'GBP_AUD.ds',\n",
       " 'GBP_CAD.ds',\n",
       " 'GBP_CHF.ds',\n",
       " 'GBP_JPY.ds',\n",
       " 'GBP_USD.ds',\n",
       " 'NZD_JPY.ds',\n",
       " 'NZD_USD.ds',\n",
       " 'USD_CAD.ds',\n",
       " 'USD_CHF.ds',\n",
       " 'USD_JPY.ds']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = './data/OANDA_DS/'\n",
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only majors for now\n",
    "# eurusd at the end (as it will be used for the validation set)\n",
    "filenames = [\n",
    "    'AUD_USD.ds',\n",
    "    'EUR_CHF.ds',\n",
    "    'EUR_GBP.ds',\n",
    "    'EUR_JPY.ds',\n",
    "    'GBP_JPY.ds',\n",
    "    'GBP_USD.ds',\n",
    "    'USD_CAD.ds',\n",
    "    'USD_CHF.ds',\n",
    "    'USD_JPY.ds',\n",
    "    'EUR_USD.ds'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # top pairs with eurusd at the end\n",
    "# filenames = [\n",
    "#     'AUD_CAD.ds',\n",
    "#     'AUD_JPY.ds',\n",
    "#     'AUD_NZD.ds',\n",
    "#     'AUD_USD.ds',\n",
    "#     'EUR_AUD.ds',\n",
    "#     'EUR_CHF.ds',\n",
    "#     'EUR_GBP.ds',\n",
    "#     'EUR_JPY.ds',\n",
    "#     'EUR_NZD.ds',\n",
    "#     'GBP_AUD.ds',\n",
    "#     'GBP_CAD.ds',\n",
    "#     'GBP_CHF.ds',\n",
    "#     'GBP_JPY.ds',\n",
    "#     'GBP_USD.ds',\n",
    "#     'NZD_JPY.ds',\n",
    "#     'NZD_USD.ds',\n",
    "#     'USD_CAD.ds',\n",
    "#     'USD_CHF.ds',\n",
    "#     'USD_JPY.ds',\n",
    "#     'EUR_USD.ds'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ds = [datasets.load_from_disk(data_dir + filename) for filename in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = concatenate_datasets(list_ds)\n",
    "all_datasets.save_to_disk(\"data/fx_days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['ohlcv', 'labels', 'future'],\n",
       "    num_rows: 35329\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.load_from_disk(\"data/fx_days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
