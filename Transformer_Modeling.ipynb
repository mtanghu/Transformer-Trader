{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import PreTrainedModel\n",
    "from trader_models import SRUTrader, SRUConfig, SGConvConfig, SGConvTrader, MegaConfig, MegaTrader\n",
    "import datasets\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds):\n",
    "    soft_profit, soft_trade = preds.predictions\n",
    "    abs_trade = np.abs(soft_trade)\n",
    "    abs_trade = abs_trade.astype('float64') # half precision will cause the sum to overflow on next line\n",
    "    trades = abs_trade.sum()\n",
    "    \n",
    "    day_profits = soft_profit.sum(axis = (1, 2))\n",
    "    \n",
    "    metrics = {\n",
    "        'day profit': day_profits.mean(),\n",
    "        'day sharpe': day_profits.mean() / day_profits.std(),\n",
    "        'trade %': trades * 100 / soft_profit.size,\n",
    "        \n",
    "        'full trade %': (abs_trade >= .7).mean() * 100,\n",
    "        'full trade accuracy': (soft_profit[abs_trade >= .7] > 0).mean() * 100,\n",
    "        'full trade g/l': soft_profit[(abs_trade >= .7) & (soft_profit > 0)].mean()\n",
    "                          / -soft_profit[(abs_trade >= .7) & (soft_profit < 0)].mean(),\n",
    "        \n",
    "        'medium trade %': ((abs_trade < .7) & (abs_trade >= .4)).mean() * 100,\n",
    "        'medium trade accuracy': (soft_profit[(abs_trade < .7) & (abs_trade >= .4)] > 0).mean() * 100,\n",
    "        'medium trade g/l': soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit > 0)].mean()\n",
    "                            / -soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit < 0)].mean(),       \n",
    "        \n",
    "        'small trade %': ((abs_trade < .4) & (abs_trade >= .2)).mean() * 100,\n",
    "        'small trade accuracy': (soft_profit[(abs_trade < .4) & (abs_trade >= .2)] > 0).mean() * 100,\n",
    "        'small trade g/l': soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit > 0)].mean()\n",
    "                            / -soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit < 0)].mean(),        \n",
    "    }\n",
    "    \n",
    "    # round the metrics\n",
    "    metrics = {k: np.format_float_positional(v, precision = 4) for k, v in metrics.items()}\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = Dataset.load_from_disk('data/fx_days')\n",
    "\n",
    "# make splits\n",
    "split = fx.train_test_split(.003, shuffle = False)\n",
    "valid_test = split['test'].train_test_split(.3, shuffle = False)\n",
    "fx = DatasetDict({\n",
    "    'train': split['train'],\n",
    "    'validation': valid_test['train'],\n",
    "    'test': valid_test['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ohlcv', 'labels', 'future'],\n",
       "        num_rows: 35213\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ohlcv', 'labels', 'future'],\n",
       "        num_rows: 74\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ohlcv', 'labels', 'future'],\n",
       "        num_rows: 32\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    logging_strategy = \"steps\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    logging_steps = 200,\n",
    "    eval_steps = 200,\n",
    "    save_steps = 10000,\n",
    "    report_to = \"none\",\n",
    "    learning_rate = 1e-3,\n",
    "    weight_decay = .01,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    warmup_ratio = .05,\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    max_grad_norm = 1,\n",
    "#     fp16 = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 13 layers\n"
     ]
    }
   ],
   "source": [
    "config = SGConvConfig(\n",
    "    n_embd = 320, n_head = 1, hidden_dropout_prob = 0\n",
    ")\n",
    "\n",
    "model = SGConvTrader(config)\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = fx['train'],\n",
    "    eval_dataset = fx['validation'],\n",
    "    compute_metrics = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4402' max='4402' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4402/4402 41:26, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.210500</td>\n",
       "      <td>2.171260</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.0719</td>\n",
       "      <td>5.3201</td>\n",
       "      <td>0.4756</td>\n",
       "      <td>1.2936</td>\n",
       "      <td>0.7324</td>\n",
       "      <td>0.5752</td>\n",
       "      <td>8.1581</td>\n",
       "      <td>1.1005</td>\n",
       "      <td>0.9064</td>\n",
       "      <td>30.9789</td>\n",
       "      <td>1.1299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.170500</td>\n",
       "      <td>2.160216</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.3422</td>\n",
       "      <td>6.9082</td>\n",
       "      <td>0.8293</td>\n",
       "      <td>0.0251</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.1713</td>\n",
       "      <td>2.4954</td>\n",
       "      <td>0.8416</td>\n",
       "      <td>1.1363</td>\n",
       "      <td>49.9541</td>\n",
       "      <td>1.2737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.168800</td>\n",
       "      <td>2.160419</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.1641</td>\n",
       "      <td>7.8999</td>\n",
       "      <td>0.8098</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.2321</td>\n",
       "      <td>0.3131</td>\n",
       "      <td>5.2614</td>\n",
       "      <td>0.7012</td>\n",
       "      <td>3.2679</td>\n",
       "      <td>51.7262</td>\n",
       "      <td>0.9372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.162600</td>\n",
       "      <td>2.153858</td>\n",
       "      <td>-0.0048</td>\n",
       "      <td>-0.0378</td>\n",
       "      <td>6.3985</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.3099</td>\n",
       "      <td>1.1583</td>\n",
       "      <td>0.5256</td>\n",
       "      <td>14.5606</td>\n",
       "      <td>0.9703</td>\n",
       "      <td>1.4190</td>\n",
       "      <td>55.1988</td>\n",
       "      <td>0.7953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.155600</td>\n",
       "      <td>2.152036</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>6.4033</td>\n",
       "      <td>1.1714</td>\n",
       "      <td>4.1570</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.5486</td>\n",
       "      <td>37.9966</td>\n",
       "      <td>0.7135</td>\n",
       "      <td>1.8075</td>\n",
       "      <td>53.5275</td>\n",
       "      <td>0.7664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.161800</td>\n",
       "      <td>2.148250</td>\n",
       "      <td>0.0249</td>\n",
       "      <td>0.3081</td>\n",
       "      <td>6.5952</td>\n",
       "      <td>1.4711</td>\n",
       "      <td>0.3473</td>\n",
       "      <td>0.1748</td>\n",
       "      <td>0.317</td>\n",
       "      <td>60.0987</td>\n",
       "      <td>1.0863</td>\n",
       "      <td>2.4341</td>\n",
       "      <td>61.0906</td>\n",
       "      <td>0.9494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.153100</td>\n",
       "      <td>2.147601</td>\n",
       "      <td>0.0361</td>\n",
       "      <td>0.2121</td>\n",
       "      <td>8.5314</td>\n",
       "      <td>1.4307</td>\n",
       "      <td>0.2988</td>\n",
       "      <td>1.0577</td>\n",
       "      <td>0.2738</td>\n",
       "      <td>62.7951</td>\n",
       "      <td>0.7730</td>\n",
       "      <td>3.4826</td>\n",
       "      <td>59.5719</td>\n",
       "      <td>0.9755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.158700</td>\n",
       "      <td>2.147372</td>\n",
       "      <td>-0.0039</td>\n",
       "      <td>-0.0418</td>\n",
       "      <td>6.9273</td>\n",
       "      <td>1.2688</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.2213</td>\n",
       "      <td>4.3827</td>\n",
       "      <td>0.2958</td>\n",
       "      <td>2.2188</td>\n",
       "      <td>53.5598</td>\n",
       "      <td>0.8021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.161500</td>\n",
       "      <td>2.148000</td>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.2717</td>\n",
       "      <td>6.3906</td>\n",
       "      <td>1.4182</td>\n",
       "      <td>1.9778</td>\n",
       "      <td>3.6387</td>\n",
       "      <td>0.5749</td>\n",
       "      <td>62.2778</td>\n",
       "      <td>1.3274</td>\n",
       "      <td>2.2574</td>\n",
       "      <td>60.0166</td>\n",
       "      <td>1.0657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.153700</td>\n",
       "      <td>2.143505</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.2621</td>\n",
       "      <td>7.5424</td>\n",
       "      <td>1.4698</td>\n",
       "      <td>3.3698</td>\n",
       "      <td>0.5353</td>\n",
       "      <td>1.4021</td>\n",
       "      <td>62.7649</td>\n",
       "      <td>1.1608</td>\n",
       "      <td>3.9239</td>\n",
       "      <td>57.2066</td>\n",
       "      <td>0.9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.155400</td>\n",
       "      <td>2.141653</td>\n",
       "      <td>0.0389</td>\n",
       "      <td>0.3555</td>\n",
       "      <td>6.9689</td>\n",
       "      <td>1.4902</td>\n",
       "      <td>0.8676</td>\n",
       "      <td>1.1632</td>\n",
       "      <td>0.3681</td>\n",
       "      <td>63.8810</td>\n",
       "      <td>0.9017</td>\n",
       "      <td>2.1250</td>\n",
       "      <td>62.4681</td>\n",
       "      <td>0.9356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.153700</td>\n",
       "      <td>2.146128</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.1889</td>\n",
       "      <td>7.2357</td>\n",
       "      <td>1.4238</td>\n",
       "      <td>0.5932</td>\n",
       "      <td>0.2415</td>\n",
       "      <td>0.9415</td>\n",
       "      <td>57.3707</td>\n",
       "      <td>1.1089</td>\n",
       "      <td>2.8615</td>\n",
       "      <td>57.1585</td>\n",
       "      <td>0.9363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.151100</td>\n",
       "      <td>2.144964</td>\n",
       "      <td>0.0275</td>\n",
       "      <td>0.3172</td>\n",
       "      <td>7.0741</td>\n",
       "      <td>1.425</td>\n",
       "      <td>1.9684</td>\n",
       "      <td>1.4257</td>\n",
       "      <td>0.7486</td>\n",
       "      <td>66.2906</td>\n",
       "      <td>1.3606</td>\n",
       "      <td>2.6462</td>\n",
       "      <td>61.8882</td>\n",
       "      <td>1.0091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.147400</td>\n",
       "      <td>2.143468</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.3825</td>\n",
       "      <td>6.6906</td>\n",
       "      <td>1.5142</td>\n",
       "      <td>2.6098</td>\n",
       "      <td>3.4337</td>\n",
       "      <td>1.3664</td>\n",
       "      <td>64.301</td>\n",
       "      <td>1.4086</td>\n",
       "      <td>3.0323</td>\n",
       "      <td>57.8041</td>\n",
       "      <td>1.0586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.143000</td>\n",
       "      <td>2.144000</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.3968</td>\n",
       "      <td>6.7070</td>\n",
       "      <td>1.4933</td>\n",
       "      <td>1.4454</td>\n",
       "      <td>3.2272</td>\n",
       "      <td>0.7705</td>\n",
       "      <td>63.8381</td>\n",
       "      <td>1.3251</td>\n",
       "      <td>2.3704</td>\n",
       "      <td>61.6505</td>\n",
       "      <td>1.0427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.150100</td>\n",
       "      <td>2.143867</td>\n",
       "      <td>0.0413</td>\n",
       "      <td>0.4524</td>\n",
       "      <td>7.7395</td>\n",
       "      <td>1.6211</td>\n",
       "      <td>7.2168</td>\n",
       "      <td>3.0946</td>\n",
       "      <td>1.9193</td>\n",
       "      <td>61.1344</td>\n",
       "      <td>1.1724</td>\n",
       "      <td>3.4442</td>\n",
       "      <td>56.5499</td>\n",
       "      <td>1.0828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.148800</td>\n",
       "      <td>2.141060</td>\n",
       "      <td>0.0507</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>7.86</td>\n",
       "      <td>1.6633</td>\n",
       "      <td>7.8987</td>\n",
       "      <td>2.0997</td>\n",
       "      <td>1.337</td>\n",
       "      <td>66.8850</td>\n",
       "      <td>1.2211</td>\n",
       "      <td>3.1566</td>\n",
       "      <td>60.3508</td>\n",
       "      <td>1.0750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.147500</td>\n",
       "      <td>2.141384</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.4441</td>\n",
       "      <td>7.2507</td>\n",
       "      <td>1.5270</td>\n",
       "      <td>3.9604</td>\n",
       "      <td>3.4903</td>\n",
       "      <td>1.0248</td>\n",
       "      <td>64.4078</td>\n",
       "      <td>1.1422</td>\n",
       "      <td>2.7959</td>\n",
       "      <td>61.0577</td>\n",
       "      <td>1.0336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.146600</td>\n",
       "      <td>2.141815</td>\n",
       "      <td>0.0471</td>\n",
       "      <td>0.461</td>\n",
       "      <td>7.4397</td>\n",
       "      <td>1.5463</td>\n",
       "      <td>3.8031</td>\n",
       "      <td>4.6073</td>\n",
       "      <td>1.0208</td>\n",
       "      <td>66.8846</td>\n",
       "      <td>1.2172</td>\n",
       "      <td>2.7361</td>\n",
       "      <td>61.8064</td>\n",
       "      <td>1.0491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.144400</td>\n",
       "      <td>2.140702</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.46</td>\n",
       "      <td>7.6006</td>\n",
       "      <td>1.6464</td>\n",
       "      <td>7.5364</td>\n",
       "      <td>2.9991</td>\n",
       "      <td>1.4158</td>\n",
       "      <td>67.3000</td>\n",
       "      <td>1.1915</td>\n",
       "      <td>2.9524</td>\n",
       "      <td>60.7452</td>\n",
       "      <td>1.0891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.148500</td>\n",
       "      <td>2.140399</td>\n",
       "      <td>0.0388</td>\n",
       "      <td>0.4419</td>\n",
       "      <td>7.5572</td>\n",
       "      <td>1.6162</td>\n",
       "      <td>6.271</td>\n",
       "      <td>3.3312</td>\n",
       "      <td>1.3841</td>\n",
       "      <td>67.184</td>\n",
       "      <td>1.1918</td>\n",
       "      <td>3.0274</td>\n",
       "      <td>60.7701</td>\n",
       "      <td>1.0799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.148000</td>\n",
       "      <td>2.140455</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.4272</td>\n",
       "      <td>7.5338</td>\n",
       "      <td>1.6079</td>\n",
       "      <td>5.6939</td>\n",
       "      <td>3.7648</td>\n",
       "      <td>1.3517</td>\n",
       "      <td>67.1912</td>\n",
       "      <td>1.2</td>\n",
       "      <td>3.0048</td>\n",
       "      <td>60.7766</td>\n",
       "      <td>1.0807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-bb1c47517fd0>:16: RuntimeWarning: Mean of empty slice.\n",
      "  'full trade g/l': soft_profit[(abs_trade >= .7) & (soft_profit > 0)].mean()\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:17: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade >= .7) & (soft_profit < 0)].mean(),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4402, training_loss=2.156417083003639, metrics={'train_runtime': 2489.5752, 'train_samples_per_second': 14.144, 'train_steps_per_second': 1.768, 'total_flos': 0.0, 'train_loss': 2.156417083003639, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oanda data\n",
    "\n",
    "# SGCONV transformer architecture! lr of 1e-3, batch size 8 hidden size 320, 1 head\n",
    "# NO dropout, weight decay\n",
    "# NO diagonal attention allowed, NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# ce loss with conditioned kelly betting\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.evaluate(fx['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('srupp.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quick prediction test to ensure model isn't cheating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SRUTrader.from_pretrained('srupp.model', config = config).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for day in range(4):\n",
    "    # cut data short so no backwards flow of info\n",
    "    test_day = torch.tensor(fx['validation']['ohlcv'][day][:]).unsqueeze(0).cuda()\n",
    "    test_futures = torch.tensor(fx['validation']['future'][day][:]).unsqueeze(0).cuda()\n",
    "    after = torch.tensor(fx['validation']['ohlcv'][day][2*60:3*60])\n",
    "    with torch.no_grad():\n",
    "        # no access to futures\n",
    "        pred = model(test_day)[0][2*60]\n",
    "    torch.cuda.empty_cache()\n",
    "#     if (pred.abs() >= .9).any():\n",
    "    if True:\n",
    "        print(day)\n",
    "        print((pred.cpu() * 100).round())\n",
    "        \n",
    "        plt.plot(after.select(dim = 1, index = -1))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for day in range(4):\n",
    "    # cut data short so no backwards flow of info\n",
    "    test_day = torch.tensor(fx['validation']['ohlcv'][day][:121]).unsqueeze(0).cuda()\n",
    "    after = torch.tensor(fx['validation']['ohlcv'][day][120:240])\n",
    "    with torch.no_grad():\n",
    "        # no access to futures\n",
    "        pred = model(test_day)[0][120]\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "#     if (pred.abs() >= .9).any():\n",
    "    if True:\n",
    "        print(day)\n",
    "#         plt.pcolormesh(pred.cpu().unsqueeze(0))\n",
    "        plt.show()\n",
    "        print((pred.cpu() * 100).round())\n",
    "        \n",
    "        plt.plot(after.select(dim = 1, index = -1))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: graph these instead of showing raw data (maybe even on same plot or at least side by side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_profit, soft_trade = trainer.predict(fx['validation']).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(soft_profit.sum(axis = (1, 2)) < 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_profit.sum(axis = (1, 2)).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(soft_profit.sum(axis = (1, 2)), bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full trade percent on 24 hours, ignoring last hour, it makes most trades in london and ny sessions (esp overlap)\n",
    "((np.abs(soft_trade) > .2).mean(axis = (0, 2)).reshape(-1, 60).mean(axis = 1) * 100).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full trade accuracy on 24 hours\n",
    "np.nanmean(np.where(np.abs(soft_trade) > .2, soft_profit > 0, np.nan), axis = (0, 2)).reshape(-1, 60).mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full trade profit on 24 hours\n",
    "np.nanmean(np.where(np.abs(soft_trade) > .3, soft_profit, np.nan), axis = (0, 2)).reshape(-1, 60).mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent full trades on all timeframes\n",
    "(np.abs(soft_trade) > .2).mean(axis = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full trade accuracy on all timeframes\n",
    "np.nanmean(np.where(np.abs(soft_trade) > .2, soft_profit > 0, np.nan), axis = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full trade gain on all timeframes\n",
    "np.nanmean(np.where((np.abs(soft_trade) > .3) & (soft_profit > 0), soft_profit, np.nan), axis = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full trade loss on all timeframes\n",
    "np.nanmean(np.where((np.abs(soft_trade) > .3) & (soft_profit < 0), soft_profit, np.nan), axis = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del trainer\n",
    "# del model\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGConv Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
