{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Model, GPT2Config, PreTrainedModel\n",
    "from sru import SRUpp\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN GOAL: GET SETUP WITH PAPER TRADING AND POLYGON\n",
    "First just manually get polygon data for maybe yesterday and format it for the model and get some prediction (in parallel at first, then try it sequentially)\n",
    "- need an inference optional parameter that'll return early"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competitive edge techniques:\n",
    "- Custom loss function based on profit with trade sizing (more of a conceptual innovation)\n",
    "- multiloss to integrate shorter and longer trades (practical innovation to unlock longer times and more data and better gradients)\n",
    "- best models -- most people don't really understand how transformers even work let alone sru\n",
    "- the way I normalize the data in preprocessing may be better?\n",
    "\n",
    "- right now model is quite greedy (for my own sanity), but it's totally possible to consider more of a <50% accuracy model but just with higher upside\n",
    "- the linear loss is odd to say the least in some sense having a strong bias for \"opportunity cost\"\n",
    "\n",
    "TODO:\n",
    "- trickier goal, do usd/jpy and usd/gbp, should they be integrated all at once though? (would be tricky to handle multiple datastreams) may leave this for later after setting up paper trading\n",
    "    - would be fine to just use it as a transfer learning tool though\n",
    "    - yeah could just simply harvest their data by concatenating them to the front of the dataset (and validation set would still be eurusd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRUTrader(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        config.initializer_range = 1 / math.sqrt(config.n_embd)\n",
    "        \n",
    "        self.embed = nn.Linear(5, config.n_embd, bias = False)\n",
    "        self.position_embeddings = nn.Embedding(config.n_positions, config.n_embd)\n",
    "        self.norm = nn.LayerNorm(config.n_embd)\n",
    "        self.sru = SRUpp(input_size = config.n_embd,\n",
    "                         hidden_size = config.n_embd,\n",
    "                         proj_size = 4 * config.n_embd, # paper says 8 is better, but working with memory contraints\n",
    "                         num_layers = 10, # paper seemed to have tuned to find this to work\n",
    "                         dropout = .01,\n",
    "                         attn_dropout = .01,\n",
    "                         rescale = True,\n",
    "                         layer_norm = True,\n",
    "                         num_heads = config.n_head,\n",
    "                         attention_every_n_layers = 2)\n",
    "        self.trade = nn.Linear(config.n_embd, 120, bias = False)\n",
    "\n",
    "\n",
    "    def forward(self, ohlcv, future):\n",
    "        # manual positional embeddings\n",
    "        batch_size, seq_length, _ = ohlcv.shape\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=ohlcv.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        \n",
    "        embed = self.norm(self.embed(ohlcv) + position_embeddings)\n",
    "        embed = torch.permute(embed, (1, 0, 2)) # sequence first for SRU\n",
    "        hidden = torch.permute(self.sru(embed)[0], (1, 0, 2))\n",
    "        \n",
    "        soft_trade = self.trade(hidden)\n",
    "        \n",
    "        # sharpe information\n",
    "        soft_trade = torch.tanh(soft_trade)\n",
    "        soft_profit = soft_trade * future\n",
    "        \n",
    "        # the exp is so that loss is purely positive and minimizes toward 0 (also losses have more loss than profit)\n",
    "        loss_ppl = torch.square(((-soft_profit + future.abs()))).mean()\n",
    "        \n",
    "        # penalty for big trades (to stop trading from happening with no profit)\n",
    "        trade_penalty = soft_trade.abs().mean()\n",
    "        \n",
    "        loss = loss_ppl + .1 * trade_penalty # .1 means that a 100% position must make at least .1 of a std to offset loss\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'profits': soft_profit,\n",
    "            'trades': soft_trade,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds):\n",
    "    soft_profit, soft_trade = preds.predictions\n",
    "    abs_trade = np.abs(soft_trade)\n",
    "    trades = abs_trade.sum()\n",
    "    \n",
    "    day_profits = soft_profit.sum(axis = (1, 2))\n",
    "    \n",
    "    metrics = {\n",
    "        'day sharpe': day_profits.mean() / day_profits.std(),\n",
    "        'trade %': trades * 100 / soft_profit.size,\n",
    "        \n",
    "        'full trade %': (abs_trade >= .9).mean() * 100,\n",
    "        'full trade accuracy': (soft_profit[abs_trade >= .9] > 0).mean() * 100,\n",
    "        'full trade g/l': soft_profit[(abs_trade >= .9) & (soft_profit > 0)].mean()\n",
    "                          / -soft_profit[(abs_trade >= .9) & (soft_profit < 0)].mean(),\n",
    "        \n",
    "        'medium trade %': ((abs_trade < .9) & (abs_trade > .5)).mean() * 100,\n",
    "        'medium trade accuracy': (soft_profit[(abs_trade < .9) & (abs_trade > .5)] > 0).mean() * 100,\n",
    "        'medium trade g/l': soft_profit[(abs_trade < .9) & (abs_trade > .5) & (soft_profit > 0)].mean()\n",
    "                            / -soft_profit[(abs_trade < .9) & (abs_trade > .5) & (soft_profit < 0)].mean(),\n",
    "    }\n",
    "    \n",
    "    # round the metrics\n",
    "    metrics = {k: np.format_float_positional(v, precision = 2) for k, v in metrics.items()}\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eurusd = Dataset.load_from_disk('data/EURUSD_day.ds')\n",
    "\n",
    "# make splits\n",
    "split = eurusd.train_test_split(.05, shuffle = False)\n",
    "valid_test = split['test'].train_test_split(.5, shuffle = False)\n",
    "eurusd = DatasetDict({\n",
    "    'train': split['train'],\n",
    "    'validation': valid_test['train'],\n",
    "    'test': valid_test['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    logging_strategy = \"steps\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    logging_steps = 100,\n",
    "    eval_steps = 100,\n",
    "    report_to = \"none\",\n",
    "    learning_rate = 1e-4,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    warmup_ratio = .05,\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    per_device_eval_batch_size = 1,\n",
    "    max_grad_norm = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(\n",
    "    n_embd = 320, n_positions = 2000, n_head = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SRUTrader(config)\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = eurusd['train'],\n",
    "    eval_dataset = eurusd['validation'],\n",
    "    compute_metrics = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3270\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3270\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3270' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3270/3270 25:48, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.798400</td>\n",
       "      <td>0.573184</td>\n",
       "      <td>1.13</td>\n",
       "      <td>37.88</td>\n",
       "      <td>1.85</td>\n",
       "      <td>55.38</td>\n",
       "      <td>2.89</td>\n",
       "      <td>29.91</td>\n",
       "      <td>51.87</td>\n",
       "      <td>1.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.056900</td>\n",
       "      <td>0.514235</td>\n",
       "      <td>1.17</td>\n",
       "      <td>35.91</td>\n",
       "      <td>6.50</td>\n",
       "      <td>59.73</td>\n",
       "      <td>3.4</td>\n",
       "      <td>20.68</td>\n",
       "      <td>50.04</td>\n",
       "      <td>1.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.432800</td>\n",
       "      <td>0.441220</td>\n",
       "      <td>1.39</td>\n",
       "      <td>37.12</td>\n",
       "      <td>5.2</td>\n",
       "      <td>62.28</td>\n",
       "      <td>3.23</td>\n",
       "      <td>24.46</td>\n",
       "      <td>60.56</td>\n",
       "      <td>1.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.526500</td>\n",
       "      <td>0.341141</td>\n",
       "      <td>1.35</td>\n",
       "      <td>35.65</td>\n",
       "      <td>4.89</td>\n",
       "      <td>69.58</td>\n",
       "      <td>5.05</td>\n",
       "      <td>22.06</td>\n",
       "      <td>65.10</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.472600</td>\n",
       "      <td>0.325495</td>\n",
       "      <td>1.39</td>\n",
       "      <td>34.75</td>\n",
       "      <td>3.47</td>\n",
       "      <td>75.81</td>\n",
       "      <td>6.44</td>\n",
       "      <td>23.08</td>\n",
       "      <td>65.96</td>\n",
       "      <td>2.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.326200</td>\n",
       "      <td>0.312801</td>\n",
       "      <td>1.74</td>\n",
       "      <td>45.38</td>\n",
       "      <td>8.63</td>\n",
       "      <td>75.60</td>\n",
       "      <td>4.43</td>\n",
       "      <td>33.81</td>\n",
       "      <td>65.92</td>\n",
       "      <td>1.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.446300</td>\n",
       "      <td>0.310042</td>\n",
       "      <td>1.77</td>\n",
       "      <td>46.73</td>\n",
       "      <td>11.82</td>\n",
       "      <td>73.21</td>\n",
       "      <td>3.45</td>\n",
       "      <td>31.97</td>\n",
       "      <td>66.44</td>\n",
       "      <td>1.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.378700</td>\n",
       "      <td>0.323889</td>\n",
       "      <td>1.66</td>\n",
       "      <td>36.43</td>\n",
       "      <td>4.45</td>\n",
       "      <td>85.65</td>\n",
       "      <td>6.66</td>\n",
       "      <td>25.77</td>\n",
       "      <td>73.08</td>\n",
       "      <td>2.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.334600</td>\n",
       "      <td>0.250180</td>\n",
       "      <td>1.95</td>\n",
       "      <td>43.6</td>\n",
       "      <td>10.61</td>\n",
       "      <td>84.86</td>\n",
       "      <td>4.8</td>\n",
       "      <td>29.85</td>\n",
       "      <td>70.54</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.370600</td>\n",
       "      <td>0.240688</td>\n",
       "      <td>1.92</td>\n",
       "      <td>42.35</td>\n",
       "      <td>10.56</td>\n",
       "      <td>83.97</td>\n",
       "      <td>4.88</td>\n",
       "      <td>27.72</td>\n",
       "      <td>72.53</td>\n",
       "      <td>2.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.277200</td>\n",
       "      <td>0.228431</td>\n",
       "      <td>2.15</td>\n",
       "      <td>50.11</td>\n",
       "      <td>14.78</td>\n",
       "      <td>85.65</td>\n",
       "      <td>4.67</td>\n",
       "      <td>34.72</td>\n",
       "      <td>70.94</td>\n",
       "      <td>1.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.261400</td>\n",
       "      <td>0.207366</td>\n",
       "      <td>2.18</td>\n",
       "      <td>45.04</td>\n",
       "      <td>12.57</td>\n",
       "      <td>86.55</td>\n",
       "      <td>4.51</td>\n",
       "      <td>29.25</td>\n",
       "      <td>77.52</td>\n",
       "      <td>2.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.242800</td>\n",
       "      <td>0.203466</td>\n",
       "      <td>2.47</td>\n",
       "      <td>52.41</td>\n",
       "      <td>16.94</td>\n",
       "      <td>88.05</td>\n",
       "      <td>4.04</td>\n",
       "      <td>35.19</td>\n",
       "      <td>75.26</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.184500</td>\n",
       "      <td>0.171175</td>\n",
       "      <td>2.29</td>\n",
       "      <td>43.32</td>\n",
       "      <td>9.67</td>\n",
       "      <td>92.38</td>\n",
       "      <td>7.23</td>\n",
       "      <td>30.23</td>\n",
       "      <td>84.24</td>\n",
       "      <td>2.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.198100</td>\n",
       "      <td>0.159092</td>\n",
       "      <td>2.33</td>\n",
       "      <td>50.38</td>\n",
       "      <td>16.12</td>\n",
       "      <td>91.8</td>\n",
       "      <td>5.66</td>\n",
       "      <td>33.14</td>\n",
       "      <td>81.24</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.236400</td>\n",
       "      <td>0.151982</td>\n",
       "      <td>2.43</td>\n",
       "      <td>49.92</td>\n",
       "      <td>14.47</td>\n",
       "      <td>92.76</td>\n",
       "      <td>6.83</td>\n",
       "      <td>34.35</td>\n",
       "      <td>84.11</td>\n",
       "      <td>2.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.178100</td>\n",
       "      <td>0.138450</td>\n",
       "      <td>2.35</td>\n",
       "      <td>51.50</td>\n",
       "      <td>16.70</td>\n",
       "      <td>93.01</td>\n",
       "      <td>6.43</td>\n",
       "      <td>33.92</td>\n",
       "      <td>83.72</td>\n",
       "      <td>2.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.175100</td>\n",
       "      <td>0.134724</td>\n",
       "      <td>2.34</td>\n",
       "      <td>47.12</td>\n",
       "      <td>13.75</td>\n",
       "      <td>93.07</td>\n",
       "      <td>6.42</td>\n",
       "      <td>30.87</td>\n",
       "      <td>87.19</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.147400</td>\n",
       "      <td>0.130227</td>\n",
       "      <td>2.39</td>\n",
       "      <td>52.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>93.42</td>\n",
       "      <td>5.91</td>\n",
       "      <td>33.22</td>\n",
       "      <td>85.28</td>\n",
       "      <td>2.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.131654</td>\n",
       "      <td>2.36</td>\n",
       "      <td>49.94</td>\n",
       "      <td>15.93</td>\n",
       "      <td>94.21</td>\n",
       "      <td>5.68</td>\n",
       "      <td>32.47</td>\n",
       "      <td>86.71</td>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.150400</td>\n",
       "      <td>0.129030</td>\n",
       "      <td>2.35</td>\n",
       "      <td>50.62</td>\n",
       "      <td>16.23</td>\n",
       "      <td>93.35</td>\n",
       "      <td>6.41</td>\n",
       "      <td>32.98</td>\n",
       "      <td>86.96</td>\n",
       "      <td>2.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.146400</td>\n",
       "      <td>0.122467</td>\n",
       "      <td>2.35</td>\n",
       "      <td>50.19</td>\n",
       "      <td>17.06</td>\n",
       "      <td>94.15</td>\n",
       "      <td>6.15</td>\n",
       "      <td>31.68</td>\n",
       "      <td>87.68</td>\n",
       "      <td>2.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.118045</td>\n",
       "      <td>2.36</td>\n",
       "      <td>48.35</td>\n",
       "      <td>15.91</td>\n",
       "      <td>94.84</td>\n",
       "      <td>6.67</td>\n",
       "      <td>30.34</td>\n",
       "      <td>89.23</td>\n",
       "      <td>2.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.125200</td>\n",
       "      <td>0.116023</td>\n",
       "      <td>2.38</td>\n",
       "      <td>50.34</td>\n",
       "      <td>17.37</td>\n",
       "      <td>94.75</td>\n",
       "      <td>6.48</td>\n",
       "      <td>31.56</td>\n",
       "      <td>89.05</td>\n",
       "      <td>2.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.138500</td>\n",
       "      <td>0.113275</td>\n",
       "      <td>2.37</td>\n",
       "      <td>49.26</td>\n",
       "      <td>16.65</td>\n",
       "      <td>95.00</td>\n",
       "      <td>6.68</td>\n",
       "      <td>30.86</td>\n",
       "      <td>89.88</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.137200</td>\n",
       "      <td>0.112562</td>\n",
       "      <td>2.37</td>\n",
       "      <td>50.06</td>\n",
       "      <td>16.88</td>\n",
       "      <td>94.57</td>\n",
       "      <td>6.14</td>\n",
       "      <td>31.58</td>\n",
       "      <td>89.88</td>\n",
       "      <td>2.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.121700</td>\n",
       "      <td>0.110899</td>\n",
       "      <td>2.38</td>\n",
       "      <td>50.15</td>\n",
       "      <td>17.24</td>\n",
       "      <td>95.02</td>\n",
       "      <td>6.47</td>\n",
       "      <td>31.27</td>\n",
       "      <td>90.18</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.119400</td>\n",
       "      <td>0.111035</td>\n",
       "      <td>2.38</td>\n",
       "      <td>50.51</td>\n",
       "      <td>17.42</td>\n",
       "      <td>95.05</td>\n",
       "      <td>6.01</td>\n",
       "      <td>31.54</td>\n",
       "      <td>89.98</td>\n",
       "      <td>2.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.109402</td>\n",
       "      <td>2.38</td>\n",
       "      <td>49.93</td>\n",
       "      <td>17.05</td>\n",
       "      <td>95.11</td>\n",
       "      <td>6.28</td>\n",
       "      <td>31.19</td>\n",
       "      <td>90.66</td>\n",
       "      <td>2.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.132000</td>\n",
       "      <td>0.109338</td>\n",
       "      <td>2.37</td>\n",
       "      <td>49.75</td>\n",
       "      <td>16.93</td>\n",
       "      <td>95.11</td>\n",
       "      <td>6.31</td>\n",
       "      <td>31.08</td>\n",
       "      <td>90.69</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.126500</td>\n",
       "      <td>0.108838</td>\n",
       "      <td>2.38</td>\n",
       "      <td>50.38</td>\n",
       "      <td>17.46</td>\n",
       "      <td>95.22</td>\n",
       "      <td>6.34</td>\n",
       "      <td>31.33</td>\n",
       "      <td>90.54</td>\n",
       "      <td>2.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.123400</td>\n",
       "      <td>0.108713</td>\n",
       "      <td>2.38</td>\n",
       "      <td>50.22</td>\n",
       "      <td>17.32</td>\n",
       "      <td>95.18</td>\n",
       "      <td>6.34</td>\n",
       "      <td>31.28</td>\n",
       "      <td>90.64</td>\n",
       "      <td>2.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1500\n",
      "Configuration saved in ./results\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2000\n",
      "Configuration saved in ./results\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2500\n",
      "Configuration saved in ./results\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-3000\n",
      "Configuration saved in ./results\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3270, training_loss=0.2732733577763269, metrics={'train_runtime': 1551.6159, 'train_samples_per_second': 2.107, 'train_steps_per_second': 2.107, 'total_flos': 0.0, 'train_loss': 0.2732733577763269, 'epoch': 1.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sru lr of 1e-4, hidden size 320, 5 heads\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 87\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.11858043819665909,\n",
       " 'eval_day sharpe': '2.67',\n",
       " 'eval_trade %': '52.93',\n",
       " 'eval_full trade %': '19.35',\n",
       " 'eval_full trade accuracy': '95.11',\n",
       " 'eval_full trade g/l': '6.63',\n",
       " 'eval_medium trade %': '32.86',\n",
       " 'eval_medium trade accuracy': '89.90',\n",
       " 'eval_medium trade g/l': '2.68',\n",
       " 'eval_runtime': 17.1238,\n",
       " 'eval_samples_per_second': 5.081,\n",
       " 'eval_steps_per_second': 5.081,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eurusd['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "soft_profit, soft_trade = trainer.predict(eurusd['validation']).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00249839, 0.01800549, 0.03796673, 0.07796512, 0.03417797,\n",
       "       0.01741602, 0.02248062, 0.00574774, 0.04024386, 0.10422804,\n",
       "       0.11214309, 0.12038921, 0.12152778, 0.11953973, 0.11725129,\n",
       "       0.14244348, 0.14758236, 0.17693637, 0.14784884, 0.0772626 ,\n",
       "       0.04732397, 0.04191053, 0.08761789, 0.26417474])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full trade percent on 24 hours, ignoring last hour, it makes most trades in london and ny sessions (esp overlap)\n",
    "(soft_trade > .9).mean(axis = (0, 2)).reshape(-1, 60).mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-9c9c0563ebcf>:2: RuntimeWarning: Mean of empty slice\n",
      "  np.nanmean(np.where(soft_trade >= .9, soft_profit > 0, np.nan), axis = (0, 2)).reshape(-1, 60).mean(axis = 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([       nan, 0.98071417, 0.95146088, 0.99030557, 0.99806648,\n",
       "       0.98027491, 0.97749938,        nan, 0.99290491, 0.99647212,\n",
       "       0.99536351, 0.99196472, 0.99540797, 0.99237283, 0.99370732,\n",
       "       0.99259902, 0.99310819, 0.99533136, 0.99444364, 0.99727577,\n",
       "       0.99690271, 0.97558482, 0.76396494, 0.81481913])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full trade accuracy on 24 hours\n",
    "np.nanmean(np.where(soft_trade >= .9, soft_profit > 0, np.nan), axis = (0, 2)).reshape(-1, 60).mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00062984, 0.00098514, 0.00163114, 0.00394864, 0.00737242,\n",
       "       0.01115149, 0.01350937, 0.01660207, 0.01845123, 0.02241602,\n",
       "       0.02708333, 0.02858527, 0.03628068, 0.03681363, 0.03950258,\n",
       "       0.04244186, 0.04646318, 0.05859981, 0.04962855, 0.04767442,\n",
       "       0.05016957, 0.06216085, 0.05637112, 0.0620155 , 0.06960594,\n",
       "       0.05977875, 0.07156008, 0.06825743, 0.07321544, 0.07039729,\n",
       "       0.07865795, 0.07989341, 0.07865795, 0.07973191, 0.08538437,\n",
       "       0.08732235, 0.08049903, 0.08968831, 0.08921996, 0.0912064 ,\n",
       "       0.08502907, 0.0845365 , 0.09761789, 0.09752907, 0.08401163,\n",
       "       0.09277293, 0.09887758, 0.0970365 , 0.10746932, 0.10379522,\n",
       "       0.09903101, 0.09917636, 0.11875807, 0.09421835, 0.10753391,\n",
       "       0.10094477, 0.09886143, 0.11103036, 0.10372255, 0.10862403,\n",
       "       0.09933786, 0.08993863, 0.10190568, 0.1123385 , 0.11434916,\n",
       "       0.09837694, 0.11026324, 0.09916828, 0.09843346, 0.10160691,\n",
       "       0.10784884, 0.09940245, 0.10044412, 0.10874516, 0.10713017,\n",
       "       0.10398094, 0.10801034, 0.1095365 , 0.11086079, 0.10102552,\n",
       "       0.10173611, 0.11657784, 0.11207203, 0.11791828, 0.10652455,\n",
       "       0.11493863, 0.11255652, 0.10233366, 0.11613372, 0.10496609,\n",
       "       0.09847384, 0.09971738, 0.10272933, 0.10435239, 0.103125  ,\n",
       "       0.11723999, 0.10891473, 0.10109819, 0.10880975, 0.1036418 ,\n",
       "       0.11404231, 0.10208333, 0.11452681, 0.1056686 , 0.1099887 ,\n",
       "       0.10098514, 0.10643572, 0.10128391, 0.11758721, 0.1155604 ,\n",
       "       0.11206395, 0.10577358, 0.10714632, 0.1124031 , 0.11458333,\n",
       "       0.11379199, 0.11619025, 0.11024709, 0.1160449 , 0.10629845])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percent full trades on all timeframes\n",
    "(soft_trade > .9).mean(axis = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57692308, 0.79508197, 0.74752475, 0.78936605, 0.83461117,\n",
       "       0.79942071, 0.8475792 , 0.89396887, 0.89234136, 0.90706052,\n",
       "       0.91234347, 0.91497175, 0.92343646, 0.93200263, 0.94582993,\n",
       "       0.94387367, 0.94160584, 0.94708557, 0.94093719, 0.95240515,\n",
       "       0.95364558, 0.95336451, 0.95745595, 0.95755208, 0.96183295,\n",
       "       0.96528434, 0.96513202, 0.95646516, 0.95489136, 0.96295022,\n",
       "       0.96294015, 0.96371538, 0.96427472, 0.96283168, 0.95867221,\n",
       "       0.96467542, 0.96509178, 0.96668767, 0.96578876, 0.96644533,\n",
       "       0.96980057, 0.96609036, 0.96947638, 0.96944858, 0.96837755,\n",
       "       0.96744712, 0.97035525, 0.9703753 , 0.96934405, 0.97067061,\n",
       "       0.96738421, 0.96604787, 0.96940233, 0.96588961, 0.96568296,\n",
       "       0.96536277, 0.9664298 , 0.96465455, 0.96699105, 0.96966994,\n",
       "       0.96610307, 0.96525409, 0.9663233 , 0.97211041, 0.97111786,\n",
       "       0.96651071, 0.96235811, 0.96229949, 0.96423298, 0.96177382,\n",
       "       0.96922731, 0.96458164, 0.9634215 , 0.96621371, 0.96065425,\n",
       "       0.96443271, 0.96575957, 0.96616292, 0.96583874, 0.95979538,\n",
       "       0.96031431, 0.95850939, 0.96310973, 0.96226803, 0.96452395,\n",
       "       0.96395953, 0.96405768, 0.95770536, 0.96280072, 0.96084314,\n",
       "       0.96055761, 0.9616973 , 0.95928313, 0.95805927, 0.95787331,\n",
       "       0.96087885, 0.96181791, 0.95607029, 0.96096475, 0.95613557,\n",
       "       0.96034837, 0.95775985, 0.96185574, 0.96049213, 0.95778577,\n",
       "       0.95514153, 0.95903194, 0.95383879, 0.96209312, 0.96184753,\n",
       "       0.95957631, 0.95915719, 0.95779637, 0.95783046, 0.95835095,\n",
       "       0.96139654, 0.95955244, 0.95861715, 0.96040637, 0.95745974])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full trade accuracy on all timeframes\n",
    "np.nanmean(np.where(soft_trade > .9, soft_profit > 0, np.nan), axis = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78205128, 0.91803279, 0.89108911, 0.87730061, 0.88061336,\n",
       "       0.8602462 , 0.88882247, 0.92169261, 0.92122538, 0.93407781,\n",
       "       0.94007156, 0.94463277, 0.94858669, 0.95459531, 0.96381848,\n",
       "       0.96385084, 0.96159194, 0.9656883 , 0.96485519, 0.97323848,\n",
       "       0.97392564, 0.97453884, 0.97679416, 0.97565104, 0.98027842,\n",
       "       0.98514116, 0.98386369, 0.97823258, 0.97716996, 0.98336774,\n",
       "       0.98439585, 0.98352537, 0.98531978, 0.98440348, 0.98099111,\n",
       "       0.98529684, 0.98655833, 0.98775547, 0.98895828, 0.98902169,\n",
       "       0.99059829, 0.98930175, 0.9887501 , 0.98998179, 0.99183007,\n",
       "       0.99129602, 0.99028175, 0.99067987, 0.990683  , 0.99152015,\n",
       "       0.99054142, 0.99096238, 0.99014075, 0.99357216, 0.99068859,\n",
       "       0.99208063, 0.9933023 , 0.99156364, 0.99268198, 0.99435028,\n",
       "       0.99504146, 0.99640869, 0.99627575, 0.99647786, 0.99576301,\n",
       "       0.99720923, 0.99502014, 0.99568439, 0.99803117, 0.99547008,\n",
       "       0.99730458, 0.99699431, 0.99598038, 0.99680701, 0.994573  ,\n",
       "       0.99611711, 0.99730861, 0.99734611, 0.99759633, 0.99608345,\n",
       "       0.99706326, 0.99328115, 0.99668564, 0.99520646, 0.99795331,\n",
       "       0.99648728, 0.99720209, 0.99589679, 0.99694062, 0.99784599,\n",
       "       0.99811398, 0.99870435, 0.99748467, 0.99659522, 0.99741602,\n",
       "       0.99621186, 0.9981465 , 0.9971246 , 0.99829314, 0.99688352,\n",
       "       0.99681371, 0.99770606, 0.99774378, 0.99893015, 0.99713677,\n",
       "       0.99720134, 0.99734466, 0.99641234, 0.99759648, 0.99769408,\n",
       "       0.99798242, 0.99931292, 0.99894491, 0.99827586, 0.99866103,\n",
       "       0.9988646 , 0.99881854, 0.99875485, 0.99860831, 0.99893649])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full trade accuracy on all timeframes (include 0) shockingly makes it about 100% accurate\n",
    "np.nanmean(np.where(soft_trade > .9, soft_profit >= 0, np.nan), axis = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.272327 , 7.1456485, 5.4958863, 3.1692212, 2.350186 , 1.8899122,\n",
       "       1.8555994, 1.8093182, 1.7990168, 1.6801865, 1.5942628, 1.5921887,\n",
       "       1.4734232, 1.4953012, 1.4823666, 1.4641556, 1.414876 , 1.2949023,\n",
       "       1.4083811, 1.4606198, 1.4457611, 1.3268101, 1.3967084, 1.35698  ,\n",
       "       1.2948686, 1.3998835, 1.294979 , 1.3205708, 1.280796 , 1.3243719,\n",
       "       1.2567165, 1.2532634, 1.2755669, 1.2672589, 1.2286527, 1.2305788,\n",
       "       1.2767869, 1.2183542, 1.2173533, 1.2164502, 1.2627718, 1.2669466,\n",
       "       1.199666 , 1.200153 , 1.2807001, 1.2337393, 1.2036744, 1.2152404,\n",
       "       1.1645474, 1.1802653, 1.2096075, 1.2066774, 1.1184528, 1.2373323,\n",
       "       1.1721324, 1.2106881, 1.2246623, 1.1632986, 1.2058973, 1.1839378,\n",
       "       1.2311872, 1.2883145, 1.2189245, 1.1759139, 1.1639053, 1.251327 ,\n",
       "       1.1859996, 1.2410449, 1.2556417, 1.2325155, 1.20284  , 1.2485127,\n",
       "       1.2461162, 1.2091612, 1.2154613, 1.234623 , 1.2176992, 1.2074447,\n",
       "       1.2080635, 1.2544149, 1.2632424, 1.1867762, 1.2058834, 1.186919 ,\n",
       "       1.2406586, 1.1982819, 1.219715 , 1.2709523, 1.2057655, 1.2688315,\n",
       "       1.2955333, 1.2990885, 1.2762113, 1.2724093, 1.2820802, 1.2117022,\n",
       "       1.2493092, 1.3014746, 1.2596998, 1.2913567, 1.2338412, 1.2961575,\n",
       "       1.2302288, 1.2763883, 1.260752 , 1.3086094, 1.2800128, 1.306867 ,\n",
       "       1.227018 , 1.233915 , 1.2526102, 1.2869959, 1.2769105, 1.2435079,\n",
       "       1.2423824, 1.2454145, 1.2389429, 1.2646984, 1.2297502, 1.2794905],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full trade profit on all timeframes\n",
    "np.nanmean(np.where(soft_trade > .9, soft_profit, np.nan), axis = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to srupp.model\n",
      "Configuration saved in srupp.model\\config.json\n",
      "Model weights saved in srupp.model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('srupp.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del trainer\n",
    "# del model\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gpt2 experiements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Trader(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # use levine 2020 layer numbers\n",
    "        n_layer = round((math.log(config.n_embd) - 5.039) / 5.55e-2)\n",
    "        n_layer = max(1, n_layer)\n",
    "        print(f'Using {n_layer} layers')\n",
    "        config.n_layer = n_layer\n",
    "        \n",
    "        config.initializer_range = 1 / math.sqrt(config.n_embd)\n",
    "        \n",
    "        self.embed = nn.Linear(5, config.n_embd, bias = False)\n",
    "        self.norm = nn.LayerNorm(config.n_embd)\n",
    "        self.gpt = GPT2Model(config)\n",
    "        self.trade = nn.Linear(config.n_embd, 120, bias = False)\n",
    "\n",
    "\n",
    "    def forward(self, ohlcv, future):\n",
    "        embed = self.norm(self.embed(ohlcv))\n",
    "        hidden = self.gpt(inputs_embeds = embed).last_hidden_state\n",
    "        \n",
    "        soft_trade = self.trade(hidden)\n",
    "        \n",
    "        # sharpe information\n",
    "        soft_trade = torch.tanh(soft_trade)\n",
    "        soft_profit = soft_trade * future\n",
    "        \n",
    "        # the exp is so that loss is purely positive and minimizes toward 0 (also losses have more loss than profit)\n",
    "        loss_ppl = torch.square(((-soft_profit + future.abs()))).mean()\n",
    "        \n",
    "        # penalty for big trades (to stop trading from happening with no profit)\n",
    "        trade_penalty = soft_trade.abs().mean()\n",
    "        \n",
    "        loss = loss_ppl + .1 * trade_penalty # .1 means that a 100% position must make at least .1 of a std to offset loss\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'profits': soft_profit,\n",
    "            'trades': soft_trade,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(\n",
    "    n_embd = 384, n_head = 6, vocab_size = 0, n_positions = 2000,\n",
    "    resid_pdrop = .01, embd_pdrop = .01, attn_pdrop = .01, # low dropout since only using 1 epoch training and to make model more robust to data issues (.1 has worse loss, accuracy & t-score)\n",
    "    summary_first_dropout = 0, summary_proj_to_labels = False,\n",
    "    scale_attn_by_inverse_layer_idx = True, use_cache = False\n",
    ")\n",
    "model = GPT2Trader(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3270\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3270\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3270' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3270/3270 31:22, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.739100</td>\n",
       "      <td>0.576555</td>\n",
       "      <td>1.18</td>\n",
       "      <td>55.53</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>68.10</td>\n",
       "      <td>52.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.038700</td>\n",
       "      <td>0.508294</td>\n",
       "      <td>1.57</td>\n",
       "      <td>25.15</td>\n",
       "      <td>0.04</td>\n",
       "      <td>53.85</td>\n",
       "      <td>19.25</td>\n",
       "      <td>56.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.546400</td>\n",
       "      <td>0.482877</td>\n",
       "      <td>1.60</td>\n",
       "      <td>33.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>43.69</td>\n",
       "      <td>32.39</td>\n",
       "      <td>53.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.691100</td>\n",
       "      <td>0.527884</td>\n",
       "      <td>1.36</td>\n",
       "      <td>42.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>61.84</td>\n",
       "      <td>34.89</td>\n",
       "      <td>54.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.639200</td>\n",
       "      <td>0.453005</td>\n",
       "      <td>1.28</td>\n",
       "      <td>27.61</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>19.7</td>\n",
       "      <td>57.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.492500</td>\n",
       "      <td>0.488201</td>\n",
       "      <td>1.82</td>\n",
       "      <td>38.68</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>34.53</td>\n",
       "      <td>54.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.654900</td>\n",
       "      <td>0.472404</td>\n",
       "      <td>1.43</td>\n",
       "      <td>36.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.99</td>\n",
       "      <td>31.99</td>\n",
       "      <td>54.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.555600</td>\n",
       "      <td>0.428319</td>\n",
       "      <td>1.41</td>\n",
       "      <td>21.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42.86</td>\n",
       "      <td>17.26</td>\n",
       "      <td>58.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.554300</td>\n",
       "      <td>0.445764</td>\n",
       "      <td>1.26</td>\n",
       "      <td>25.83</td>\n",
       "      <td>0.16</td>\n",
       "      <td>52.64</td>\n",
       "      <td>15.78</td>\n",
       "      <td>59.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.662100</td>\n",
       "      <td>0.395642</td>\n",
       "      <td>1.37</td>\n",
       "      <td>19.72</td>\n",
       "      <td>4.11</td>\n",
       "      <td>77.08</td>\n",
       "      <td>8.08</td>\n",
       "      <td>68.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.471200</td>\n",
       "      <td>0.408307</td>\n",
       "      <td>1.36</td>\n",
       "      <td>24.43</td>\n",
       "      <td>2.35</td>\n",
       "      <td>83.77</td>\n",
       "      <td>6.88</td>\n",
       "      <td>74.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.479100</td>\n",
       "      <td>0.375257</td>\n",
       "      <td>1.23</td>\n",
       "      <td>16.63</td>\n",
       "      <td>5.12</td>\n",
       "      <td>79.26</td>\n",
       "      <td>6.15</td>\n",
       "      <td>77.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.465000</td>\n",
       "      <td>0.391933</td>\n",
       "      <td>1.18</td>\n",
       "      <td>25.64</td>\n",
       "      <td>6.06</td>\n",
       "      <td>78.17</td>\n",
       "      <td>6.44</td>\n",
       "      <td>77.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.391800</td>\n",
       "      <td>0.364388</td>\n",
       "      <td>1.32</td>\n",
       "      <td>10.43</td>\n",
       "      <td>6.19</td>\n",
       "      <td>77.89</td>\n",
       "      <td>6.54</td>\n",
       "      <td>77.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.467300</td>\n",
       "      <td>0.363665</td>\n",
       "      <td>1.33</td>\n",
       "      <td>8.97</td>\n",
       "      <td>5.52</td>\n",
       "      <td>79.78</td>\n",
       "      <td>6.34</td>\n",
       "      <td>77.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.558400</td>\n",
       "      <td>0.365436</td>\n",
       "      <td>1.24</td>\n",
       "      <td>10.32</td>\n",
       "      <td>3.95</td>\n",
       "      <td>84.00</td>\n",
       "      <td>5.93</td>\n",
       "      <td>78.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.524900</td>\n",
       "      <td>0.363330</td>\n",
       "      <td>1.31</td>\n",
       "      <td>9.04</td>\n",
       "      <td>6.</td>\n",
       "      <td>77.90</td>\n",
       "      <td>6.31</td>\n",
       "      <td>77.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.493300</td>\n",
       "      <td>0.375537</td>\n",
       "      <td>1.28</td>\n",
       "      <td>17.2</td>\n",
       "      <td>5.97</td>\n",
       "      <td>78.63</td>\n",
       "      <td>6.41</td>\n",
       "      <td>77.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.423800</td>\n",
       "      <td>0.368052</td>\n",
       "      <td>1.36</td>\n",
       "      <td>11.4</td>\n",
       "      <td>5.81</td>\n",
       "      <td>78.99</td>\n",
       "      <td>6.30</td>\n",
       "      <td>77.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.565800</td>\n",
       "      <td>0.362856</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.30</td>\n",
       "      <td>3.95</td>\n",
       "      <td>85.16</td>\n",
       "      <td>6.16</td>\n",
       "      <td>78.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.468300</td>\n",
       "      <td>0.363221</td>\n",
       "      <td>1.30</td>\n",
       "      <td>9.37</td>\n",
       "      <td>5.6</td>\n",
       "      <td>79.22</td>\n",
       "      <td>6.19</td>\n",
       "      <td>77.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.512200</td>\n",
       "      <td>0.366569</td>\n",
       "      <td>1.26</td>\n",
       "      <td>12.52</td>\n",
       "      <td>5.31</td>\n",
       "      <td>79.76</td>\n",
       "      <td>6.03</td>\n",
       "      <td>78.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.567200</td>\n",
       "      <td>0.364427</td>\n",
       "      <td>1.28</td>\n",
       "      <td>8.85</td>\n",
       "      <td>4.92</td>\n",
       "      <td>80.4</td>\n",
       "      <td>5.86</td>\n",
       "      <td>78.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.444900</td>\n",
       "      <td>0.360784</td>\n",
       "      <td>1.32</td>\n",
       "      <td>7.8</td>\n",
       "      <td>5.61</td>\n",
       "      <td>79.59</td>\n",
       "      <td>6.14</td>\n",
       "      <td>78.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.473100</td>\n",
       "      <td>0.362020</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.2</td>\n",
       "      <td>5.54</td>\n",
       "      <td>79.62</td>\n",
       "      <td>6.07</td>\n",
       "      <td>78.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.506700</td>\n",
       "      <td>0.361715</td>\n",
       "      <td>1.26</td>\n",
       "      <td>7.03</td>\n",
       "      <td>4.34</td>\n",
       "      <td>82.34</td>\n",
       "      <td>5.5</td>\n",
       "      <td>79.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.426100</td>\n",
       "      <td>0.361053</td>\n",
       "      <td>1.29</td>\n",
       "      <td>8.06</td>\n",
       "      <td>5.14</td>\n",
       "      <td>80.81</td>\n",
       "      <td>5.91</td>\n",
       "      <td>78.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.438700</td>\n",
       "      <td>0.362888</td>\n",
       "      <td>1.29</td>\n",
       "      <td>8.44</td>\n",
       "      <td>5.03</td>\n",
       "      <td>80.90</td>\n",
       "      <td>5.87</td>\n",
       "      <td>78.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.502900</td>\n",
       "      <td>0.361344</td>\n",
       "      <td>1.3</td>\n",
       "      <td>7.38</td>\n",
       "      <td>5.24</td>\n",
       "      <td>80.35</td>\n",
       "      <td>5.96</td>\n",
       "      <td>78.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.508100</td>\n",
       "      <td>0.360743</td>\n",
       "      <td>1.29</td>\n",
       "      <td>7.14</td>\n",
       "      <td>5.13</td>\n",
       "      <td>80.80</td>\n",
       "      <td>5.92</td>\n",
       "      <td>78.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.506600</td>\n",
       "      <td>0.360870</td>\n",
       "      <td>1.28</td>\n",
       "      <td>7.55</td>\n",
       "      <td>5.11</td>\n",
       "      <td>80.85</td>\n",
       "      <td>5.93</td>\n",
       "      <td>78.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.454700</td>\n",
       "      <td>0.360703</td>\n",
       "      <td>1.28</td>\n",
       "      <td>7.29</td>\n",
       "      <td>5.12</td>\n",
       "      <td>80.79</td>\n",
       "      <td>5.92</td>\n",
       "      <td>78.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "<ipython-input-43-52e946b60fdf>:12: RuntimeWarning: Mean of empty slice.\n",
      "  'full trade accuracy': (soft_profit[abs_trade > .9] > 0).mean() * 100,\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "<ipython-input-43-52e946b60fdf>:12: RuntimeWarning: Mean of empty slice.\n",
      "  'full trade accuracy': (soft_profit[abs_trade > .9] > 0).mean() * 100,\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1500\n",
      "Configuration saved in ./results\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2000\n",
      "Configuration saved in ./results\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2500\n",
      "Configuration saved in ./results\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-3000\n",
      "Configuration saved in ./results\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3270, training_loss=0.5385216050920866, metrics={'train_runtime': 1883.0698, 'train_samples_per_second': 1.737, 'train_steps_per_second': 1.737, 'total_flos': 0.0, 'train_loss': 0.5385216050920866, 'epoch': 1.0})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5e-4 lr (best)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3270\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3270\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3270' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3270/3270 31:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.742800</td>\n",
       "      <td>0.598860</td>\n",
       "      <td>1.4</td>\n",
       "      <td>53.69</td>\n",
       "      <td>0.34</td>\n",
       "      <td>52.07</td>\n",
       "      <td>58.1</td>\n",
       "      <td>52.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.059300</td>\n",
       "      <td>0.629185</td>\n",
       "      <td>1.19</td>\n",
       "      <td>35.43</td>\n",
       "      <td>0.05</td>\n",
       "      <td>50.63</td>\n",
       "      <td>19.08</td>\n",
       "      <td>53.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.618700</td>\n",
       "      <td>0.574223</td>\n",
       "      <td>1.69</td>\n",
       "      <td>58.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>55.6</td>\n",
       "      <td>75.46</td>\n",
       "      <td>51.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.724900</td>\n",
       "      <td>0.498308</td>\n",
       "      <td>1.4</td>\n",
       "      <td>30.9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.</td>\n",
       "      <td>29.13</td>\n",
       "      <td>53.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.475523</td>\n",
       "      <td>1.06</td>\n",
       "      <td>39.59</td>\n",
       "      <td>0.14</td>\n",
       "      <td>46.61</td>\n",
       "      <td>24.92</td>\n",
       "      <td>55.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.504500</td>\n",
       "      <td>0.475192</td>\n",
       "      <td>1.70</td>\n",
       "      <td>38.56</td>\n",
       "      <td>0.06</td>\n",
       "      <td>56.23</td>\n",
       "      <td>36.72</td>\n",
       "      <td>54.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.706500</td>\n",
       "      <td>0.509966</td>\n",
       "      <td>1.87</td>\n",
       "      <td>37.81</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>42.78</td>\n",
       "      <td>52.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.584600</td>\n",
       "      <td>0.460770</td>\n",
       "      <td>1.35</td>\n",
       "      <td>21.82</td>\n",
       "      <td>0.88</td>\n",
       "      <td>50.54</td>\n",
       "      <td>17.30</td>\n",
       "      <td>56.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.572900</td>\n",
       "      <td>0.467708</td>\n",
       "      <td>1.16</td>\n",
       "      <td>24.87</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.</td>\n",
       "      <td>18.21</td>\n",
       "      <td>57.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.735000</td>\n",
       "      <td>0.469905</td>\n",
       "      <td>1.38</td>\n",
       "      <td>25.93</td>\n",
       "      <td>4.13</td>\n",
       "      <td>63.69</td>\n",
       "      <td>17.26</td>\n",
       "      <td>58.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.533800</td>\n",
       "      <td>0.464012</td>\n",
       "      <td>1.27</td>\n",
       "      <td>17.55</td>\n",
       "      <td>0.23</td>\n",
       "      <td>47.86</td>\n",
       "      <td>13.62</td>\n",
       "      <td>59.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.554200</td>\n",
       "      <td>0.478624</td>\n",
       "      <td>1.54</td>\n",
       "      <td>30.78</td>\n",
       "      <td>1.34</td>\n",
       "      <td>70.73</td>\n",
       "      <td>28.17</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.537100</td>\n",
       "      <td>0.394490</td>\n",
       "      <td>1.29</td>\n",
       "      <td>19.18</td>\n",
       "      <td>1.81</td>\n",
       "      <td>78.83</td>\n",
       "      <td>7.21</td>\n",
       "      <td>72.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.398500</td>\n",
       "      <td>0.369955</td>\n",
       "      <td>1.38</td>\n",
       "      <td>11.00</td>\n",
       "      <td>5.94</td>\n",
       "      <td>77.47</td>\n",
       "      <td>6.41</td>\n",
       "      <td>77.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.477000</td>\n",
       "      <td>0.370523</td>\n",
       "      <td>1.38</td>\n",
       "      <td>10.01</td>\n",
       "      <td>5.90</td>\n",
       "      <td>77.97</td>\n",
       "      <td>6.85</td>\n",
       "      <td>74.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.563500</td>\n",
       "      <td>0.369378</td>\n",
       "      <td>1.32</td>\n",
       "      <td>12.82</td>\n",
       "      <td>6.16</td>\n",
       "      <td>77.68</td>\n",
       "      <td>6.72</td>\n",
       "      <td>76.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.526200</td>\n",
       "      <td>0.363844</td>\n",
       "      <td>1.37</td>\n",
       "      <td>9.01</td>\n",
       "      <td>6.63</td>\n",
       "      <td>76.43</td>\n",
       "      <td>6.92</td>\n",
       "      <td>76.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.495900</td>\n",
       "      <td>0.378385</td>\n",
       "      <td>1.29</td>\n",
       "      <td>18.74</td>\n",
       "      <td>6.43</td>\n",
       "      <td>77.18</td>\n",
       "      <td>6.93</td>\n",
       "      <td>76.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.423400</td>\n",
       "      <td>0.372098</td>\n",
       "      <td>1.37</td>\n",
       "      <td>12.98</td>\n",
       "      <td>6.47</td>\n",
       "      <td>76.56</td>\n",
       "      <td>6.83</td>\n",
       "      <td>76.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.564800</td>\n",
       "      <td>0.363691</td>\n",
       "      <td>1.35</td>\n",
       "      <td>8.39</td>\n",
       "      <td>5.49</td>\n",
       "      <td>80.14</td>\n",
       "      <td>6.74</td>\n",
       "      <td>76.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.468700</td>\n",
       "      <td>0.362690</td>\n",
       "      <td>1.34</td>\n",
       "      <td>8.85</td>\n",
       "      <td>6.42</td>\n",
       "      <td>77.14</td>\n",
       "      <td>6.8</td>\n",
       "      <td>76.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.510700</td>\n",
       "      <td>0.367357</td>\n",
       "      <td>1.29</td>\n",
       "      <td>13.33</td>\n",
       "      <td>6.48</td>\n",
       "      <td>76.84</td>\n",
       "      <td>6.77</td>\n",
       "      <td>76.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.566600</td>\n",
       "      <td>0.362199</td>\n",
       "      <td>1.30</td>\n",
       "      <td>7.31</td>\n",
       "      <td>6.12</td>\n",
       "      <td>77.56</td>\n",
       "      <td>6.50</td>\n",
       "      <td>76.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.442600</td>\n",
       "      <td>0.363324</td>\n",
       "      <td>1.33</td>\n",
       "      <td>8.66</td>\n",
       "      <td>6.43</td>\n",
       "      <td>76.54</td>\n",
       "      <td>6.72</td>\n",
       "      <td>76.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.473200</td>\n",
       "      <td>0.362252</td>\n",
       "      <td>1.32</td>\n",
       "      <td>7.73</td>\n",
       "      <td>6.40</td>\n",
       "      <td>76.86</td>\n",
       "      <td>6.63</td>\n",
       "      <td>76.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.510300</td>\n",
       "      <td>0.362037</td>\n",
       "      <td>1.32</td>\n",
       "      <td>7.8</td>\n",
       "      <td>6.32</td>\n",
       "      <td>76.96</td>\n",
       "      <td>6.56</td>\n",
       "      <td>76.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.425900</td>\n",
       "      <td>0.362943</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.38</td>\n",
       "      <td>6.36</td>\n",
       "      <td>76.81</td>\n",
       "      <td>6.59</td>\n",
       "      <td>76.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.431600</td>\n",
       "      <td>0.366106</td>\n",
       "      <td>1.32</td>\n",
       "      <td>9.95</td>\n",
       "      <td>6.27</td>\n",
       "      <td>77.07</td>\n",
       "      <td>6.51</td>\n",
       "      <td>76.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.503600</td>\n",
       "      <td>0.362857</td>\n",
       "      <td>1.32</td>\n",
       "      <td>7.59</td>\n",
       "      <td>6.34</td>\n",
       "      <td>76.9</td>\n",
       "      <td>6.53</td>\n",
       "      <td>76.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.508100</td>\n",
       "      <td>0.362679</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.37</td>\n",
       "      <td>6.27</td>\n",
       "      <td>76.9</td>\n",
       "      <td>6.47</td>\n",
       "      <td>76.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.506500</td>\n",
       "      <td>0.362241</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.31</td>\n",
       "      <td>6.27</td>\n",
       "      <td>76.91</td>\n",
       "      <td>6.48</td>\n",
       "      <td>76.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.456400</td>\n",
       "      <td>0.362123</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.17</td>\n",
       "      <td>6.29</td>\n",
       "      <td>76.93</td>\n",
       "      <td>6.49</td>\n",
       "      <td>76.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "<ipython-input-43-52e946b60fdf>:12: RuntimeWarning: Mean of empty slice.\n",
      "  'full trade accuracy': (soft_profit[abs_trade > .9] > 0).mean() * 100,\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1500\n",
      "Configuration saved in ./results\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2000\n",
      "Configuration saved in ./results\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2500\n",
      "Configuration saved in ./results\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-3000\n",
      "Configuration saved in ./results\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3270, training_loss=0.5552892037487905, metrics={'train_runtime': 1886.12, 'train_samples_per_second': 1.734, 'train_steps_per_second': 1.734, 'total_flos': 0.0, 'train_loss': 0.5552892037487905, 'epoch': 1.0})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1e-3 lr\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3270\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3270\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3270' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3270/3270 31:18, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.740600</td>\n",
       "      <td>0.527701</td>\n",
       "      <td>2.02</td>\n",
       "      <td>38.95</td>\n",
       "      <td>0.09</td>\n",
       "      <td>52.46</td>\n",
       "      <td>32.52</td>\n",
       "      <td>53.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.977000</td>\n",
       "      <td>0.492953</td>\n",
       "      <td>1.67</td>\n",
       "      <td>26.88</td>\n",
       "      <td>0.09</td>\n",
       "      <td>59.97</td>\n",
       "      <td>21.80</td>\n",
       "      <td>56.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.539400</td>\n",
       "      <td>0.543064</td>\n",
       "      <td>1.8</td>\n",
       "      <td>29.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.</td>\n",
       "      <td>27.00</td>\n",
       "      <td>52.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.670100</td>\n",
       "      <td>0.463039</td>\n",
       "      <td>1.55</td>\n",
       "      <td>29.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.</td>\n",
       "      <td>27.</td>\n",
       "      <td>55.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.625400</td>\n",
       "      <td>0.451693</td>\n",
       "      <td>1.31</td>\n",
       "      <td>31.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.</td>\n",
       "      <td>25.62</td>\n",
       "      <td>55.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.480400</td>\n",
       "      <td>0.496943</td>\n",
       "      <td>1.92</td>\n",
       "      <td>42.34</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>41.84</td>\n",
       "      <td>53.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.660500</td>\n",
       "      <td>0.484283</td>\n",
       "      <td>1.5</td>\n",
       "      <td>28.90</td>\n",
       "      <td>0.01</td>\n",
       "      <td>51.19</td>\n",
       "      <td>26.33</td>\n",
       "      <td>56.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.563100</td>\n",
       "      <td>0.441980</td>\n",
       "      <td>1.36</td>\n",
       "      <td>22.58</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>20.</td>\n",
       "      <td>56.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.565500</td>\n",
       "      <td>0.439754</td>\n",
       "      <td>1.29</td>\n",
       "      <td>24.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.</td>\n",
       "      <td>18.22</td>\n",
       "      <td>57.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.710500</td>\n",
       "      <td>0.449362</td>\n",
       "      <td>1.41</td>\n",
       "      <td>22.11</td>\n",
       "      <td>0.04</td>\n",
       "      <td>65.72</td>\n",
       "      <td>17.88</td>\n",
       "      <td>58.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.522800</td>\n",
       "      <td>0.423899</td>\n",
       "      <td>1.29</td>\n",
       "      <td>16.77</td>\n",
       "      <td>0.03</td>\n",
       "      <td>50.35</td>\n",
       "      <td>12.01</td>\n",
       "      <td>62.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.547200</td>\n",
       "      <td>0.392619</td>\n",
       "      <td>1.28</td>\n",
       "      <td>15.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>61.67</td>\n",
       "      <td>7.3</td>\n",
       "      <td>71.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.484300</td>\n",
       "      <td>0.385493</td>\n",
       "      <td>1.29</td>\n",
       "      <td>16.64</td>\n",
       "      <td>3.71</td>\n",
       "      <td>82.07</td>\n",
       "      <td>6.44</td>\n",
       "      <td>74.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.399500</td>\n",
       "      <td>0.373606</td>\n",
       "      <td>1.23</td>\n",
       "      <td>12.00</td>\n",
       "      <td>4.36</td>\n",
       "      <td>79.60</td>\n",
       "      <td>6.06</td>\n",
       "      <td>75.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.466900</td>\n",
       "      <td>0.372316</td>\n",
       "      <td>1.30</td>\n",
       "      <td>12.05</td>\n",
       "      <td>5.36</td>\n",
       "      <td>77.87</td>\n",
       "      <td>6.43</td>\n",
       "      <td>75.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.564900</td>\n",
       "      <td>0.376707</td>\n",
       "      <td>1.24</td>\n",
       "      <td>13.49</td>\n",
       "      <td>5.33</td>\n",
       "      <td>76.04</td>\n",
       "      <td>6.99</td>\n",
       "      <td>71.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.537900</td>\n",
       "      <td>0.364282</td>\n",
       "      <td>1.31</td>\n",
       "      <td>9.27</td>\n",
       "      <td>5.33</td>\n",
       "      <td>78.98</td>\n",
       "      <td>6.42</td>\n",
       "      <td>76.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.492700</td>\n",
       "      <td>0.383330</td>\n",
       "      <td>1.27</td>\n",
       "      <td>17.88</td>\n",
       "      <td>5.28</td>\n",
       "      <td>79.22</td>\n",
       "      <td>7.33</td>\n",
       "      <td>72.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.430200</td>\n",
       "      <td>0.371721</td>\n",
       "      <td>1.38</td>\n",
       "      <td>11.51</td>\n",
       "      <td>5.63</td>\n",
       "      <td>78.57</td>\n",
       "      <td>6.9</td>\n",
       "      <td>75.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.560900</td>\n",
       "      <td>0.366097</td>\n",
       "      <td>1.29</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5.19</td>\n",
       "      <td>79.14</td>\n",
       "      <td>5.95</td>\n",
       "      <td>77.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.468800</td>\n",
       "      <td>0.365137</td>\n",
       "      <td>1.3</td>\n",
       "      <td>10.45</td>\n",
       "      <td>5.48</td>\n",
       "      <td>78.97</td>\n",
       "      <td>6.09</td>\n",
       "      <td>77.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.517500</td>\n",
       "      <td>0.365354</td>\n",
       "      <td>1.28</td>\n",
       "      <td>11.24</td>\n",
       "      <td>5.52</td>\n",
       "      <td>78.82</td>\n",
       "      <td>6.16</td>\n",
       "      <td>77.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.570800</td>\n",
       "      <td>0.366208</td>\n",
       "      <td>1.30</td>\n",
       "      <td>9.64</td>\n",
       "      <td>5.37</td>\n",
       "      <td>79.05</td>\n",
       "      <td>5.96</td>\n",
       "      <td>77.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.447200</td>\n",
       "      <td>0.362986</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.83</td>\n",
       "      <td>5.50</td>\n",
       "      <td>78.6</td>\n",
       "      <td>6.16</td>\n",
       "      <td>76.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.473500</td>\n",
       "      <td>0.364681</td>\n",
       "      <td>1.30</td>\n",
       "      <td>8.52</td>\n",
       "      <td>5.47</td>\n",
       "      <td>78.55</td>\n",
       "      <td>6.07</td>\n",
       "      <td>76.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.506700</td>\n",
       "      <td>0.362462</td>\n",
       "      <td>1.28</td>\n",
       "      <td>7.82</td>\n",
       "      <td>5.17</td>\n",
       "      <td>79.36</td>\n",
       "      <td>5.86</td>\n",
       "      <td>77.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.426600</td>\n",
       "      <td>0.362636</td>\n",
       "      <td>1.28</td>\n",
       "      <td>8.18</td>\n",
       "      <td>5.38</td>\n",
       "      <td>78.80</td>\n",
       "      <td>5.94</td>\n",
       "      <td>77.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.443500</td>\n",
       "      <td>0.364341</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.97</td>\n",
       "      <td>5.48</td>\n",
       "      <td>78.79</td>\n",
       "      <td>6.01</td>\n",
       "      <td>77.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.505200</td>\n",
       "      <td>0.363033</td>\n",
       "      <td>1.3</td>\n",
       "      <td>7.73</td>\n",
       "      <td>5.46</td>\n",
       "      <td>78.87</td>\n",
       "      <td>5.96</td>\n",
       "      <td>77.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.509200</td>\n",
       "      <td>0.362144</td>\n",
       "      <td>1.3</td>\n",
       "      <td>7.55</td>\n",
       "      <td>5.42</td>\n",
       "      <td>79.01</td>\n",
       "      <td>5.96</td>\n",
       "      <td>77.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.507900</td>\n",
       "      <td>0.362074</td>\n",
       "      <td>1.29</td>\n",
       "      <td>7.82</td>\n",
       "      <td>5.37</td>\n",
       "      <td>79.12</td>\n",
       "      <td>5.95</td>\n",
       "      <td>77.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.455800</td>\n",
       "      <td>0.361957</td>\n",
       "      <td>1.29</td>\n",
       "      <td>7.61</td>\n",
       "      <td>5.39</td>\n",
       "      <td>79.03</td>\n",
       "      <td>5.95</td>\n",
       "      <td>77.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "<ipython-input-43-52e946b60fdf>:12: RuntimeWarning: Mean of empty slice.\n",
      "  'full trade accuracy': (soft_profit[abs_trade > .9] > 0).mean() * 100,\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1500\n",
      "Configuration saved in ./results\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2000\n",
      "Configuration saved in ./results\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2500\n",
      "Configuration saved in ./results\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-3000\n",
      "Configuration saved in ./results\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3270, training_loss=0.5430762673007603, metrics={'train_runtime': 1879.3917, 'train_samples_per_second': 1.74, 'train_steps_per_second': 1.74, 'total_flos': 0.0, 'train_loss': 0.5430762673007603, 'epoch': 1.0})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2e-4 lr\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 3270\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3270\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3270' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3270/3270 29:38, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.795100</td>\n",
       "      <td>0.519946</td>\n",
       "      <td>1.84</td>\n",
       "      <td>39.76</td>\n",
       "      <td>2.33</td>\n",
       "      <td>58.80</td>\n",
       "      <td>3.04</td>\n",
       "      <td>32.59</td>\n",
       "      <td>53.53</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.925500</td>\n",
       "      <td>0.442862</td>\n",
       "      <td>1.63</td>\n",
       "      <td>32.52</td>\n",
       "      <td>1.65</td>\n",
       "      <td>91.58</td>\n",
       "      <td>11.87</td>\n",
       "      <td>21.13</td>\n",
       "      <td>54.21</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.427700</td>\n",
       "      <td>0.386134</td>\n",
       "      <td>1.55</td>\n",
       "      <td>28.43</td>\n",
       "      <td>3.78</td>\n",
       "      <td>83.79</td>\n",
       "      <td>6.39</td>\n",
       "      <td>12.35</td>\n",
       "      <td>57.08</td>\n",
       "      <td>1.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.519200</td>\n",
       "      <td>0.339990</td>\n",
       "      <td>1.73</td>\n",
       "      <td>33.33</td>\n",
       "      <td>3.96</td>\n",
       "      <td>77.52</td>\n",
       "      <td>5.39</td>\n",
       "      <td>19.27</td>\n",
       "      <td>64.25</td>\n",
       "      <td>2.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.471500</td>\n",
       "      <td>0.342008</td>\n",
       "      <td>1.55</td>\n",
       "      <td>35.43</td>\n",
       "      <td>3.30</td>\n",
       "      <td>90.87</td>\n",
       "      <td>8.5</td>\n",
       "      <td>24.02</td>\n",
       "      <td>61.75</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.333500</td>\n",
       "      <td>0.302813</td>\n",
       "      <td>1.78</td>\n",
       "      <td>33.35</td>\n",
       "      <td>2.70</td>\n",
       "      <td>94.64</td>\n",
       "      <td>11.23</td>\n",
       "      <td>21.55</td>\n",
       "      <td>68.82</td>\n",
       "      <td>2.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.429500</td>\n",
       "      <td>0.301962</td>\n",
       "      <td>1.89</td>\n",
       "      <td>43.58</td>\n",
       "      <td>5.76</td>\n",
       "      <td>83.92</td>\n",
       "      <td>5.94</td>\n",
       "      <td>34.55</td>\n",
       "      <td>66.01</td>\n",
       "      <td>1.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.386100</td>\n",
       "      <td>0.267107</td>\n",
       "      <td>1.93</td>\n",
       "      <td>32.97</td>\n",
       "      <td>2.4</td>\n",
       "      <td>98.00</td>\n",
       "      <td>13.71</td>\n",
       "      <td>21.87</td>\n",
       "      <td>75.97</td>\n",
       "      <td>3.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.350400</td>\n",
       "      <td>0.242964</td>\n",
       "      <td>1.78</td>\n",
       "      <td>39.49</td>\n",
       "      <td>4.62</td>\n",
       "      <td>93.88</td>\n",
       "      <td>6.49</td>\n",
       "      <td>30.09</td>\n",
       "      <td>72.65</td>\n",
       "      <td>2.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.386200</td>\n",
       "      <td>0.247526</td>\n",
       "      <td>1.71</td>\n",
       "      <td>39.8</td>\n",
       "      <td>4.67</td>\n",
       "      <td>94.88</td>\n",
       "      <td>8.30</td>\n",
       "      <td>30.43</td>\n",
       "      <td>70.33</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.290700</td>\n",
       "      <td>0.222086</td>\n",
       "      <td>2.12</td>\n",
       "      <td>43.64</td>\n",
       "      <td>8.46</td>\n",
       "      <td>90.49</td>\n",
       "      <td>6.61</td>\n",
       "      <td>31.88</td>\n",
       "      <td>73.77</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.253700</td>\n",
       "      <td>0.194933</td>\n",
       "      <td>2.09</td>\n",
       "      <td>39.64</td>\n",
       "      <td>7.38</td>\n",
       "      <td>93.95</td>\n",
       "      <td>6.81</td>\n",
       "      <td>27.30</td>\n",
       "      <td>79.91</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>0.183137</td>\n",
       "      <td>2.19</td>\n",
       "      <td>44.69</td>\n",
       "      <td>10.46</td>\n",
       "      <td>92.9</td>\n",
       "      <td>5.68</td>\n",
       "      <td>31.24</td>\n",
       "      <td>79.12</td>\n",
       "      <td>2.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.189600</td>\n",
       "      <td>0.175074</td>\n",
       "      <td>2.12</td>\n",
       "      <td>41.26</td>\n",
       "      <td>8.69</td>\n",
       "      <td>96.45</td>\n",
       "      <td>6.93</td>\n",
       "      <td>28.13</td>\n",
       "      <td>82.44</td>\n",
       "      <td>2.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.237900</td>\n",
       "      <td>0.183164</td>\n",
       "      <td>2.1</td>\n",
       "      <td>44.68</td>\n",
       "      <td>10.88</td>\n",
       "      <td>94.69</td>\n",
       "      <td>6.01</td>\n",
       "      <td>30.67</td>\n",
       "      <td>77.39</td>\n",
       "      <td>2.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.256600</td>\n",
       "      <td>0.164511</td>\n",
       "      <td>2.28</td>\n",
       "      <td>49.65</td>\n",
       "      <td>13.49</td>\n",
       "      <td>95.03</td>\n",
       "      <td>4.96</td>\n",
       "      <td>35.00</td>\n",
       "      <td>80.38</td>\n",
       "      <td>2.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.201100</td>\n",
       "      <td>0.153465</td>\n",
       "      <td>2.29</td>\n",
       "      <td>46.02</td>\n",
       "      <td>11.88</td>\n",
       "      <td>96.01</td>\n",
       "      <td>5.9</td>\n",
       "      <td>31.47</td>\n",
       "      <td>83.53</td>\n",
       "      <td>2.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.209300</td>\n",
       "      <td>0.148974</td>\n",
       "      <td>2.3</td>\n",
       "      <td>45.94</td>\n",
       "      <td>10.32</td>\n",
       "      <td>96.98</td>\n",
       "      <td>6.06</td>\n",
       "      <td>33.27</td>\n",
       "      <td>85.99</td>\n",
       "      <td>2.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.159100</td>\n",
       "      <td>0.147965</td>\n",
       "      <td>2.36</td>\n",
       "      <td>47.65</td>\n",
       "      <td>13.13</td>\n",
       "      <td>96.72</td>\n",
       "      <td>5.10</td>\n",
       "      <td>32.52</td>\n",
       "      <td>84.20</td>\n",
       "      <td>2.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.201100</td>\n",
       "      <td>0.143522</td>\n",
       "      <td>2.27</td>\n",
       "      <td>46.07</td>\n",
       "      <td>12.47</td>\n",
       "      <td>97.45</td>\n",
       "      <td>5.78</td>\n",
       "      <td>30.75</td>\n",
       "      <td>85.38</td>\n",
       "      <td>2.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.163600</td>\n",
       "      <td>0.135285</td>\n",
       "      <td>2.32</td>\n",
       "      <td>45.33</td>\n",
       "      <td>11.83</td>\n",
       "      <td>98.04</td>\n",
       "      <td>5.84</td>\n",
       "      <td>30.38</td>\n",
       "      <td>87.88</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.179000</td>\n",
       "      <td>0.131303</td>\n",
       "      <td>2.33</td>\n",
       "      <td>47.27</td>\n",
       "      <td>12.23</td>\n",
       "      <td>98.01</td>\n",
       "      <td>6.45</td>\n",
       "      <td>33.01</td>\n",
       "      <td>87.63</td>\n",
       "      <td>2.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.175800</td>\n",
       "      <td>0.130103</td>\n",
       "      <td>2.28</td>\n",
       "      <td>45.74</td>\n",
       "      <td>11.25</td>\n",
       "      <td>97.6</td>\n",
       "      <td>6.09</td>\n",
       "      <td>31.84</td>\n",
       "      <td>89.09</td>\n",
       "      <td>2.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.169500</td>\n",
       "      <td>0.126183</td>\n",
       "      <td>2.32</td>\n",
       "      <td>46.61</td>\n",
       "      <td>12.57</td>\n",
       "      <td>98.25</td>\n",
       "      <td>6.96</td>\n",
       "      <td>31.71</td>\n",
       "      <td>88.82</td>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.150800</td>\n",
       "      <td>0.125547</td>\n",
       "      <td>2.30</td>\n",
       "      <td>45.85</td>\n",
       "      <td>12.49</td>\n",
       "      <td>98.17</td>\n",
       "      <td>6.36</td>\n",
       "      <td>30.47</td>\n",
       "      <td>89.1</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.156000</td>\n",
       "      <td>0.122579</td>\n",
       "      <td>2.32</td>\n",
       "      <td>46.57</td>\n",
       "      <td>12.42</td>\n",
       "      <td>98.30</td>\n",
       "      <td>6.84</td>\n",
       "      <td>31.82</td>\n",
       "      <td>89.56</td>\n",
       "      <td>2.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.153300</td>\n",
       "      <td>0.122105</td>\n",
       "      <td>2.31</td>\n",
       "      <td>46.49</td>\n",
       "      <td>12.67</td>\n",
       "      <td>98.27</td>\n",
       "      <td>6.64</td>\n",
       "      <td>31.37</td>\n",
       "      <td>89.38</td>\n",
       "      <td>2.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.136500</td>\n",
       "      <td>0.120749</td>\n",
       "      <td>2.30</td>\n",
       "      <td>45.90</td>\n",
       "      <td>12.08</td>\n",
       "      <td>98.6</td>\n",
       "      <td>6.96</td>\n",
       "      <td>31.23</td>\n",
       "      <td>90.3</td>\n",
       "      <td>2.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.173700</td>\n",
       "      <td>0.120101</td>\n",
       "      <td>2.32</td>\n",
       "      <td>46.74</td>\n",
       "      <td>12.95</td>\n",
       "      <td>98.5</td>\n",
       "      <td>6.75</td>\n",
       "      <td>31.43</td>\n",
       "      <td>89.82</td>\n",
       "      <td>2.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.158000</td>\n",
       "      <td>0.119680</td>\n",
       "      <td>2.32</td>\n",
       "      <td>46.36</td>\n",
       "      <td>12.53</td>\n",
       "      <td>98.52</td>\n",
       "      <td>6.84</td>\n",
       "      <td>31.33</td>\n",
       "      <td>90.16</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.150300</td>\n",
       "      <td>0.119475</td>\n",
       "      <td>2.33</td>\n",
       "      <td>46.86</td>\n",
       "      <td>13.09</td>\n",
       "      <td>98.50</td>\n",
       "      <td>6.72</td>\n",
       "      <td>31.42</td>\n",
       "      <td>89.82</td>\n",
       "      <td>2.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.140200</td>\n",
       "      <td>0.119341</td>\n",
       "      <td>2.33</td>\n",
       "      <td>46.85</td>\n",
       "      <td>13.06</td>\n",
       "      <td>98.49</td>\n",
       "      <td>6.71</td>\n",
       "      <td>31.43</td>\n",
       "      <td>89.87</td>\n",
       "      <td>2.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1500\n",
      "Configuration saved in ./results\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2000\n",
      "Configuration saved in ./results\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2500\n",
      "Configuration saved in ./results\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-3000\n",
      "Configuration saved in ./results\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3270, training_loss=0.28452607805211244, metrics={'train_runtime': 1779.4527, 'train_samples_per_second': 1.838, 'train_steps_per_second': 1.838, 'total_flos': 0.0, 'train_loss': 0.28452607805211244, 'epoch': 1.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5e-5 (really small) learning rate\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRU experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3270\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3270\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3270' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3270/3270 29:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.758300</td>\n",
       "      <td>0.511011</td>\n",
       "      <td>1.72</td>\n",
       "      <td>37.60</td>\n",
       "      <td>2.08</td>\n",
       "      <td>61.44</td>\n",
       "      <td>3.55</td>\n",
       "      <td>29.25</td>\n",
       "      <td>54.22</td>\n",
       "      <td>1.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.936000</td>\n",
       "      <td>0.450914</td>\n",
       "      <td>1.78</td>\n",
       "      <td>34.24</td>\n",
       "      <td>5.61</td>\n",
       "      <td>74.47</td>\n",
       "      <td>4.59</td>\n",
       "      <td>19.33</td>\n",
       "      <td>51.92</td>\n",
       "      <td>1.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.391700</td>\n",
       "      <td>0.400445</td>\n",
       "      <td>1.44</td>\n",
       "      <td>31.94</td>\n",
       "      <td>2.58</td>\n",
       "      <td>87.24</td>\n",
       "      <td>6.43</td>\n",
       "      <td>19.15</td>\n",
       "      <td>62.35</td>\n",
       "      <td>2.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.481200</td>\n",
       "      <td>0.309347</td>\n",
       "      <td>1.76</td>\n",
       "      <td>36.83</td>\n",
       "      <td>4.74</td>\n",
       "      <td>86.57</td>\n",
       "      <td>7.62</td>\n",
       "      <td>24.70</td>\n",
       "      <td>66.72</td>\n",
       "      <td>1.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.436200</td>\n",
       "      <td>0.313799</td>\n",
       "      <td>1.72</td>\n",
       "      <td>37.91</td>\n",
       "      <td>5.80</td>\n",
       "      <td>80.42</td>\n",
       "      <td>4.82</td>\n",
       "      <td>25.1</td>\n",
       "      <td>66.08</td>\n",
       "      <td>1.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.308700</td>\n",
       "      <td>0.283673</td>\n",
       "      <td>1.77</td>\n",
       "      <td>41.22</td>\n",
       "      <td>6.75</td>\n",
       "      <td>82.12</td>\n",
       "      <td>4.94</td>\n",
       "      <td>29.24</td>\n",
       "      <td>68.34</td>\n",
       "      <td>1.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.385600</td>\n",
       "      <td>0.271688</td>\n",
       "      <td>2.04</td>\n",
       "      <td>45.27</td>\n",
       "      <td>10.61</td>\n",
       "      <td>83.02</td>\n",
       "      <td>4.89</td>\n",
       "      <td>31.36</td>\n",
       "      <td>67.65</td>\n",
       "      <td>1.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.330200</td>\n",
       "      <td>0.243528</td>\n",
       "      <td>2.02</td>\n",
       "      <td>38.33</td>\n",
       "      <td>5.81</td>\n",
       "      <td>94.43</td>\n",
       "      <td>8.94</td>\n",
       "      <td>26.28</td>\n",
       "      <td>74.4</td>\n",
       "      <td>2.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.334200</td>\n",
       "      <td>0.227438</td>\n",
       "      <td>1.99</td>\n",
       "      <td>41.24</td>\n",
       "      <td>5.84</td>\n",
       "      <td>95.10</td>\n",
       "      <td>6.96</td>\n",
       "      <td>31.40</td>\n",
       "      <td>75.00</td>\n",
       "      <td>2.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.320700</td>\n",
       "      <td>0.209981</td>\n",
       "      <td>2.06</td>\n",
       "      <td>44.56</td>\n",
       "      <td>11.33</td>\n",
       "      <td>91.65</td>\n",
       "      <td>5.9</td>\n",
       "      <td>29.89</td>\n",
       "      <td>73.83</td>\n",
       "      <td>2.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.261000</td>\n",
       "      <td>0.174897</td>\n",
       "      <td>2.25</td>\n",
       "      <td>42.56</td>\n",
       "      <td>8.93</td>\n",
       "      <td>95.33</td>\n",
       "      <td>6.85</td>\n",
       "      <td>29.59</td>\n",
       "      <td>82.47</td>\n",
       "      <td>2.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.207200</td>\n",
       "      <td>0.184500</td>\n",
       "      <td>2.45</td>\n",
       "      <td>48.56</td>\n",
       "      <td>16.17</td>\n",
       "      <td>93.47</td>\n",
       "      <td>4.77</td>\n",
       "      <td>30.53</td>\n",
       "      <td>77.70</td>\n",
       "      <td>2.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.214700</td>\n",
       "      <td>0.159796</td>\n",
       "      <td>2.35</td>\n",
       "      <td>47.53</td>\n",
       "      <td>12.6</td>\n",
       "      <td>95.34</td>\n",
       "      <td>6.01</td>\n",
       "      <td>32.82</td>\n",
       "      <td>82.37</td>\n",
       "      <td>2.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.161600</td>\n",
       "      <td>0.150457</td>\n",
       "      <td>2.32</td>\n",
       "      <td>42.28</td>\n",
       "      <td>8.62</td>\n",
       "      <td>98.73</td>\n",
       "      <td>9.56</td>\n",
       "      <td>29.52</td>\n",
       "      <td>87.77</td>\n",
       "      <td>3.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.187900</td>\n",
       "      <td>0.138048</td>\n",
       "      <td>2.35</td>\n",
       "      <td>48.38</td>\n",
       "      <td>13.12</td>\n",
       "      <td>97.37</td>\n",
       "      <td>7.10</td>\n",
       "      <td>33.77</td>\n",
       "      <td>85.83</td>\n",
       "      <td>2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.226000</td>\n",
       "      <td>0.139733</td>\n",
       "      <td>2.42</td>\n",
       "      <td>47.62</td>\n",
       "      <td>13.42</td>\n",
       "      <td>97.52</td>\n",
       "      <td>6.59</td>\n",
       "      <td>32.18</td>\n",
       "      <td>86.5</td>\n",
       "      <td>2.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.159900</td>\n",
       "      <td>0.129388</td>\n",
       "      <td>2.37</td>\n",
       "      <td>45.29</td>\n",
       "      <td>12.93</td>\n",
       "      <td>98.10</td>\n",
       "      <td>6.81</td>\n",
       "      <td>29.69</td>\n",
       "      <td>87.99</td>\n",
       "      <td>2.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.162500</td>\n",
       "      <td>0.122167</td>\n",
       "      <td>2.32</td>\n",
       "      <td>46.42</td>\n",
       "      <td>13.15</td>\n",
       "      <td>98.12</td>\n",
       "      <td>7.34</td>\n",
       "      <td>30.85</td>\n",
       "      <td>89.17</td>\n",
       "      <td>2.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.135900</td>\n",
       "      <td>0.121643</td>\n",
       "      <td>2.39</td>\n",
       "      <td>47.94</td>\n",
       "      <td>14.06</td>\n",
       "      <td>98.04</td>\n",
       "      <td>6.09</td>\n",
       "      <td>31.83</td>\n",
       "      <td>89.21</td>\n",
       "      <td>2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.155900</td>\n",
       "      <td>0.117337</td>\n",
       "      <td>2.35</td>\n",
       "      <td>47.34</td>\n",
       "      <td>14.62</td>\n",
       "      <td>98.54</td>\n",
       "      <td>6.11</td>\n",
       "      <td>30.22</td>\n",
       "      <td>89.70</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.138500</td>\n",
       "      <td>0.114264</td>\n",
       "      <td>2.4</td>\n",
       "      <td>47.92</td>\n",
       "      <td>16.01</td>\n",
       "      <td>98.33</td>\n",
       "      <td>6.33</td>\n",
       "      <td>29.48</td>\n",
       "      <td>89.80</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.139400</td>\n",
       "      <td>0.114385</td>\n",
       "      <td>2.4</td>\n",
       "      <td>48.39</td>\n",
       "      <td>15.59</td>\n",
       "      <td>98.83</td>\n",
       "      <td>6.66</td>\n",
       "      <td>30.88</td>\n",
       "      <td>90.34</td>\n",
       "      <td>2.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.158500</td>\n",
       "      <td>0.108680</td>\n",
       "      <td>2.35</td>\n",
       "      <td>45.39</td>\n",
       "      <td>14.24</td>\n",
       "      <td>99.05</td>\n",
       "      <td>7.32</td>\n",
       "      <td>28.28</td>\n",
       "      <td>91.67</td>\n",
       "      <td>2.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.116300</td>\n",
       "      <td>0.107233</td>\n",
       "      <td>2.39</td>\n",
       "      <td>48.95</td>\n",
       "      <td>16.65</td>\n",
       "      <td>98.84</td>\n",
       "      <td>6.99</td>\n",
       "      <td>30.57</td>\n",
       "      <td>90.79</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>0.107016</td>\n",
       "      <td>2.36</td>\n",
       "      <td>45.34</td>\n",
       "      <td>13.9</td>\n",
       "      <td>99.19</td>\n",
       "      <td>6.55</td>\n",
       "      <td>28.55</td>\n",
       "      <td>92.54</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.125500</td>\n",
       "      <td>0.104738</td>\n",
       "      <td>2.37</td>\n",
       "      <td>47.38</td>\n",
       "      <td>15.10</td>\n",
       "      <td>98.99</td>\n",
       "      <td>7.16</td>\n",
       "      <td>29.95</td>\n",
       "      <td>91.85</td>\n",
       "      <td>2.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.113500</td>\n",
       "      <td>0.103646</td>\n",
       "      <td>2.37</td>\n",
       "      <td>48.02</td>\n",
       "      <td>16.24</td>\n",
       "      <td>99.08</td>\n",
       "      <td>6.87</td>\n",
       "      <td>29.58</td>\n",
       "      <td>91.9</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.111000</td>\n",
       "      <td>0.102776</td>\n",
       "      <td>2.38</td>\n",
       "      <td>48.04</td>\n",
       "      <td>16.09</td>\n",
       "      <td>99.2</td>\n",
       "      <td>6.45</td>\n",
       "      <td>29.76</td>\n",
       "      <td>92.32</td>\n",
       "      <td>2.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.127600</td>\n",
       "      <td>0.101982</td>\n",
       "      <td>2.38</td>\n",
       "      <td>47.90</td>\n",
       "      <td>16.01</td>\n",
       "      <td>99.24</td>\n",
       "      <td>6.96</td>\n",
       "      <td>29.70</td>\n",
       "      <td>92.56</td>\n",
       "      <td>2.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>0.101487</td>\n",
       "      <td>2.37</td>\n",
       "      <td>47.47</td>\n",
       "      <td>15.68</td>\n",
       "      <td>99.28</td>\n",
       "      <td>7.37</td>\n",
       "      <td>29.49</td>\n",
       "      <td>92.79</td>\n",
       "      <td>2.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.117300</td>\n",
       "      <td>0.101191</td>\n",
       "      <td>2.39</td>\n",
       "      <td>48.10</td>\n",
       "      <td>16.09</td>\n",
       "      <td>99.27</td>\n",
       "      <td>7.28</td>\n",
       "      <td>29.9</td>\n",
       "      <td>92.68</td>\n",
       "      <td>2.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.113400</td>\n",
       "      <td>0.101098</td>\n",
       "      <td>2.38</td>\n",
       "      <td>47.95</td>\n",
       "      <td>15.97</td>\n",
       "      <td>99.28</td>\n",
       "      <td>7.29</td>\n",
       "      <td>29.83</td>\n",
       "      <td>92.75</td>\n",
       "      <td>2.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1500\n",
      "Configuration saved in ./results\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2000\n",
      "Configuration saved in ./results\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2500\n",
      "Configuration saved in ./results\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-3000\n",
      "Configuration saved in ./results\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3270, training_loss=0.249264092401627, metrics={'train_runtime': 1777.0102, 'train_samples_per_second': 1.84, 'train_steps_per_second': 1.84, 'total_flos': 0.0, 'train_loss': 0.249264092401627, 'epoch': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sru lr of 1e-4 (6 heads) hidden size 384\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3270\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3270\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3270' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3270/3270 29:22, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.749600</td>\n",
       "      <td>0.504964</td>\n",
       "      <td>1.74</td>\n",
       "      <td>37.87</td>\n",
       "      <td>2.01</td>\n",
       "      <td>62.50</td>\n",
       "      <td>3.88</td>\n",
       "      <td>29.78</td>\n",
       "      <td>54.10</td>\n",
       "      <td>1.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.946700</td>\n",
       "      <td>0.419118</td>\n",
       "      <td>1.74</td>\n",
       "      <td>29.66</td>\n",
       "      <td>2.32</td>\n",
       "      <td>96.17</td>\n",
       "      <td>12.28</td>\n",
       "      <td>15.68</td>\n",
       "      <td>55.41</td>\n",
       "      <td>1.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.370600</td>\n",
       "      <td>0.404617</td>\n",
       "      <td>1.5</td>\n",
       "      <td>35.97</td>\n",
       "      <td>3.06</td>\n",
       "      <td>90.69</td>\n",
       "      <td>6.27</td>\n",
       "      <td>25.53</td>\n",
       "      <td>58.72</td>\n",
       "      <td>1.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.504200</td>\n",
       "      <td>0.334729</td>\n",
       "      <td>1.87</td>\n",
       "      <td>32.85</td>\n",
       "      <td>4.92</td>\n",
       "      <td>80.6</td>\n",
       "      <td>5.81</td>\n",
       "      <td>17.72</td>\n",
       "      <td>65.83</td>\n",
       "      <td>1.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.435100</td>\n",
       "      <td>0.300377</td>\n",
       "      <td>1.57</td>\n",
       "      <td>32.84</td>\n",
       "      <td>3.11</td>\n",
       "      <td>90.46</td>\n",
       "      <td>9.01</td>\n",
       "      <td>19.43</td>\n",
       "      <td>69.63</td>\n",
       "      <td>2.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.325600</td>\n",
       "      <td>0.272262</td>\n",
       "      <td>1.77</td>\n",
       "      <td>38.33</td>\n",
       "      <td>3.9</td>\n",
       "      <td>94.61</td>\n",
       "      <td>10.48</td>\n",
       "      <td>28.19</td>\n",
       "      <td>69.28</td>\n",
       "      <td>2.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.407000</td>\n",
       "      <td>0.283630</td>\n",
       "      <td>2.09</td>\n",
       "      <td>49.07</td>\n",
       "      <td>10.02</td>\n",
       "      <td>83.74</td>\n",
       "      <td>5.37</td>\n",
       "      <td>38.35</td>\n",
       "      <td>65.55</td>\n",
       "      <td>1.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.254286</td>\n",
       "      <td>2.10</td>\n",
       "      <td>39.59</td>\n",
       "      <td>6.25</td>\n",
       "      <td>84.63</td>\n",
       "      <td>5.12</td>\n",
       "      <td>27.66</td>\n",
       "      <td>73.59</td>\n",
       "      <td>2.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.353100</td>\n",
       "      <td>0.240660</td>\n",
       "      <td>1.91</td>\n",
       "      <td>39.95</td>\n",
       "      <td>6.46</td>\n",
       "      <td>93.23</td>\n",
       "      <td>7.41</td>\n",
       "      <td>29.04</td>\n",
       "      <td>74.37</td>\n",
       "      <td>2.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.393600</td>\n",
       "      <td>0.211937</td>\n",
       "      <td>2.19</td>\n",
       "      <td>41.89</td>\n",
       "      <td>7.95</td>\n",
       "      <td>90.82</td>\n",
       "      <td>6.80</td>\n",
       "      <td>29.96</td>\n",
       "      <td>76.90</td>\n",
       "      <td>2.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>0.199988</td>\n",
       "      <td>2.32</td>\n",
       "      <td>40.71</td>\n",
       "      <td>6.72</td>\n",
       "      <td>94.95</td>\n",
       "      <td>7.84</td>\n",
       "      <td>29.52</td>\n",
       "      <td>80.13</td>\n",
       "      <td>2.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.247200</td>\n",
       "      <td>0.192307</td>\n",
       "      <td>2.38</td>\n",
       "      <td>41.36</td>\n",
       "      <td>8.78</td>\n",
       "      <td>95.27</td>\n",
       "      <td>6.83</td>\n",
       "      <td>28.12</td>\n",
       "      <td>80.16</td>\n",
       "      <td>2.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.240600</td>\n",
       "      <td>0.183205</td>\n",
       "      <td>2.39</td>\n",
       "      <td>47.7</td>\n",
       "      <td>13.55</td>\n",
       "      <td>93.65</td>\n",
       "      <td>5.29</td>\n",
       "      <td>32.38</td>\n",
       "      <td>78.07</td>\n",
       "      <td>2.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.185300</td>\n",
       "      <td>0.169526</td>\n",
       "      <td>2.12</td>\n",
       "      <td>43.89</td>\n",
       "      <td>9.43</td>\n",
       "      <td>97.69</td>\n",
       "      <td>6.56</td>\n",
       "      <td>29.98</td>\n",
       "      <td>83.35</td>\n",
       "      <td>2.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.211000</td>\n",
       "      <td>0.172772</td>\n",
       "      <td>2.25</td>\n",
       "      <td>43.48</td>\n",
       "      <td>10.15</td>\n",
       "      <td>96.08</td>\n",
       "      <td>6.16</td>\n",
       "      <td>29.89</td>\n",
       "      <td>82.73</td>\n",
       "      <td>2.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.241100</td>\n",
       "      <td>0.155131</td>\n",
       "      <td>2.40</td>\n",
       "      <td>52.87</td>\n",
       "      <td>18.58</td>\n",
       "      <td>94.42</td>\n",
       "      <td>5.45</td>\n",
       "      <td>33.44</td>\n",
       "      <td>79.89</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.214600</td>\n",
       "      <td>0.144647</td>\n",
       "      <td>2.34</td>\n",
       "      <td>48.06</td>\n",
       "      <td>15.30</td>\n",
       "      <td>96.35</td>\n",
       "      <td>6.02</td>\n",
       "      <td>30.66</td>\n",
       "      <td>83.27</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.183100</td>\n",
       "      <td>0.144314</td>\n",
       "      <td>2.27</td>\n",
       "      <td>45.23</td>\n",
       "      <td>13.22</td>\n",
       "      <td>96.95</td>\n",
       "      <td>5.74</td>\n",
       "      <td>29.06</td>\n",
       "      <td>84.78</td>\n",
       "      <td>2.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.151500</td>\n",
       "      <td>0.134769</td>\n",
       "      <td>2.39</td>\n",
       "      <td>49.54</td>\n",
       "      <td>16.74</td>\n",
       "      <td>96.95</td>\n",
       "      <td>5.99</td>\n",
       "      <td>31.23</td>\n",
       "      <td>84.89</td>\n",
       "      <td>2.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.183000</td>\n",
       "      <td>0.130834</td>\n",
       "      <td>2.32</td>\n",
       "      <td>48.66</td>\n",
       "      <td>17.06</td>\n",
       "      <td>97.15</td>\n",
       "      <td>5.87</td>\n",
       "      <td>29.34</td>\n",
       "      <td>85.83</td>\n",
       "      <td>2.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.148600</td>\n",
       "      <td>0.130160</td>\n",
       "      <td>2.31</td>\n",
       "      <td>47.83</td>\n",
       "      <td>15.93</td>\n",
       "      <td>97.5</td>\n",
       "      <td>6.17</td>\n",
       "      <td>29.36</td>\n",
       "      <td>86.59</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.156300</td>\n",
       "      <td>0.123995</td>\n",
       "      <td>2.33</td>\n",
       "      <td>49.58</td>\n",
       "      <td>17.03</td>\n",
       "      <td>97.90</td>\n",
       "      <td>5.91</td>\n",
       "      <td>30.9</td>\n",
       "      <td>87.14</td>\n",
       "      <td>2.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.158700</td>\n",
       "      <td>0.121040</td>\n",
       "      <td>2.34</td>\n",
       "      <td>46.41</td>\n",
       "      <td>15.37</td>\n",
       "      <td>98.45</td>\n",
       "      <td>5.75</td>\n",
       "      <td>28.42</td>\n",
       "      <td>88.97</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.128800</td>\n",
       "      <td>0.119207</td>\n",
       "      <td>2.35</td>\n",
       "      <td>49.07</td>\n",
       "      <td>17.52</td>\n",
       "      <td>97.86</td>\n",
       "      <td>5.93</td>\n",
       "      <td>29.69</td>\n",
       "      <td>88.14</td>\n",
       "      <td>2.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.140700</td>\n",
       "      <td>0.115714</td>\n",
       "      <td>2.28</td>\n",
       "      <td>46.06</td>\n",
       "      <td>15.</td>\n",
       "      <td>98.39</td>\n",
       "      <td>6.13</td>\n",
       "      <td>28.46</td>\n",
       "      <td>90.21</td>\n",
       "      <td>2.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.135500</td>\n",
       "      <td>0.113593</td>\n",
       "      <td>2.32</td>\n",
       "      <td>47.12</td>\n",
       "      <td>15.51</td>\n",
       "      <td>98.40</td>\n",
       "      <td>5.98</td>\n",
       "      <td>29.24</td>\n",
       "      <td>90.22</td>\n",
       "      <td>2.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.125900</td>\n",
       "      <td>0.111606</td>\n",
       "      <td>2.35</td>\n",
       "      <td>47.31</td>\n",
       "      <td>16.03</td>\n",
       "      <td>98.54</td>\n",
       "      <td>6.21</td>\n",
       "      <td>28.92</td>\n",
       "      <td>90.62</td>\n",
       "      <td>2.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.119300</td>\n",
       "      <td>0.110374</td>\n",
       "      <td>2.33</td>\n",
       "      <td>46.74</td>\n",
       "      <td>15.56</td>\n",
       "      <td>98.77</td>\n",
       "      <td>6.32</td>\n",
       "      <td>28.70</td>\n",
       "      <td>91.22</td>\n",
       "      <td>2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.134300</td>\n",
       "      <td>0.109228</td>\n",
       "      <td>2.35</td>\n",
       "      <td>47.46</td>\n",
       "      <td>16.24</td>\n",
       "      <td>98.80</td>\n",
       "      <td>6.35</td>\n",
       "      <td>28.90</td>\n",
       "      <td>91.09</td>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.132700</td>\n",
       "      <td>0.108662</td>\n",
       "      <td>2.33</td>\n",
       "      <td>46.96</td>\n",
       "      <td>15.68</td>\n",
       "      <td>98.85</td>\n",
       "      <td>6.68</td>\n",
       "      <td>28.8</td>\n",
       "      <td>91.51</td>\n",
       "      <td>2.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.127500</td>\n",
       "      <td>0.108322</td>\n",
       "      <td>2.35</td>\n",
       "      <td>47.78</td>\n",
       "      <td>16.28</td>\n",
       "      <td>98.82</td>\n",
       "      <td>6.51</td>\n",
       "      <td>29.29</td>\n",
       "      <td>91.21</td>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.123400</td>\n",
       "      <td>0.108103</td>\n",
       "      <td>2.34</td>\n",
       "      <td>47.71</td>\n",
       "      <td>16.25</td>\n",
       "      <td>98.81</td>\n",
       "      <td>6.46</td>\n",
       "      <td>29.23</td>\n",
       "      <td>91.24</td>\n",
       "      <td>2.53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1500\n",
      "Configuration saved in ./results\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2000\n",
      "Configuration saved in ./results\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2500\n",
      "Configuration saved in ./results\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-3000\n",
      "Configuration saved in ./results\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3270, training_loss=0.26571082660546724, metrics={'train_runtime': 1763.507, 'train_samples_per_second': 1.854, 'train_steps_per_second': 1.854, 'total_flos': 0.0, 'train_loss': 0.26571082660546724, 'epoch': 1.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sru lr of 1e-4 (no heads)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3270\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3270\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3270' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3270/3270 26:51, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.707000</td>\n",
       "      <td>0.511279</td>\n",
       "      <td>1.23</td>\n",
       "      <td>37.33</td>\n",
       "      <td>4.36</td>\n",
       "      <td>66.84</td>\n",
       "      <td>30.01</td>\n",
       "      <td>56.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.117200</td>\n",
       "      <td>0.573373</td>\n",
       "      <td>1.47</td>\n",
       "      <td>58.55</td>\n",
       "      <td>24.83</td>\n",
       "      <td>54.93</td>\n",
       "      <td>59.16</td>\n",
       "      <td>52.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.452700</td>\n",
       "      <td>0.496984</td>\n",
       "      <td>2.11</td>\n",
       "      <td>49.29</td>\n",
       "      <td>18.54</td>\n",
       "      <td>60.35</td>\n",
       "      <td>45.26</td>\n",
       "      <td>56.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.537700</td>\n",
       "      <td>0.499412</td>\n",
       "      <td>1.34</td>\n",
       "      <td>56.95</td>\n",
       "      <td>26.6</td>\n",
       "      <td>61.27</td>\n",
       "      <td>55.62</td>\n",
       "      <td>58.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.514200</td>\n",
       "      <td>0.305154</td>\n",
       "      <td>1.43</td>\n",
       "      <td>31.76</td>\n",
       "      <td>5.19</td>\n",
       "      <td>80.69</td>\n",
       "      <td>20.34</td>\n",
       "      <td>68.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.388700</td>\n",
       "      <td>0.331800</td>\n",
       "      <td>1.54</td>\n",
       "      <td>27.54</td>\n",
       "      <td>6.15</td>\n",
       "      <td>78.10</td>\n",
       "      <td>13.94</td>\n",
       "      <td>70.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.476200</td>\n",
       "      <td>0.404420</td>\n",
       "      <td>1.88</td>\n",
       "      <td>59.19</td>\n",
       "      <td>18.03</td>\n",
       "      <td>67.74</td>\n",
       "      <td>62.29</td>\n",
       "      <td>61.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.422000</td>\n",
       "      <td>0.292924</td>\n",
       "      <td>1.46</td>\n",
       "      <td>32.75</td>\n",
       "      <td>4.56</td>\n",
       "      <td>84.47</td>\n",
       "      <td>21.5</td>\n",
       "      <td>72.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.407600</td>\n",
       "      <td>0.298788</td>\n",
       "      <td>1.46</td>\n",
       "      <td>35.89</td>\n",
       "      <td>4.53</td>\n",
       "      <td>83.85</td>\n",
       "      <td>26.33</td>\n",
       "      <td>69.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.493400</td>\n",
       "      <td>0.427605</td>\n",
       "      <td>1.49</td>\n",
       "      <td>30.62</td>\n",
       "      <td>8.22</td>\n",
       "      <td>61.58</td>\n",
       "      <td>22.05</td>\n",
       "      <td>63.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.384300</td>\n",
       "      <td>0.304273</td>\n",
       "      <td>1.68</td>\n",
       "      <td>24.04</td>\n",
       "      <td>4.4</td>\n",
       "      <td>85.81</td>\n",
       "      <td>11.81</td>\n",
       "      <td>78.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.423100</td>\n",
       "      <td>0.329487</td>\n",
       "      <td>1.73</td>\n",
       "      <td>29.97</td>\n",
       "      <td>4.94</td>\n",
       "      <td>84.25</td>\n",
       "      <td>18.07</td>\n",
       "      <td>70.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.431600</td>\n",
       "      <td>0.334285</td>\n",
       "      <td>1.81</td>\n",
       "      <td>38.42</td>\n",
       "      <td>7.56</td>\n",
       "      <td>76.45</td>\n",
       "      <td>30.19</td>\n",
       "      <td>67.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.329800</td>\n",
       "      <td>0.377073</td>\n",
       "      <td>1.79</td>\n",
       "      <td>33.08</td>\n",
       "      <td>6.97</td>\n",
       "      <td>75.66</td>\n",
       "      <td>18.40</td>\n",
       "      <td>69.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.487100</td>\n",
       "      <td>0.296720</td>\n",
       "      <td>1.70</td>\n",
       "      <td>32.48</td>\n",
       "      <td>4.30</td>\n",
       "      <td>88.50</td>\n",
       "      <td>24.31</td>\n",
       "      <td>72.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.431400</td>\n",
       "      <td>0.311931</td>\n",
       "      <td>1.82</td>\n",
       "      <td>36.91</td>\n",
       "      <td>8.25</td>\n",
       "      <td>75.86</td>\n",
       "      <td>29.41</td>\n",
       "      <td>69.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.409600</td>\n",
       "      <td>0.291295</td>\n",
       "      <td>2.05</td>\n",
       "      <td>41.74</td>\n",
       "      <td>6.65</td>\n",
       "      <td>81.58</td>\n",
       "      <td>37.66</td>\n",
       "      <td>72.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.397300</td>\n",
       "      <td>0.281329</td>\n",
       "      <td>1.79</td>\n",
       "      <td>43.80</td>\n",
       "      <td>11.03</td>\n",
       "      <td>81.05</td>\n",
       "      <td>39.51</td>\n",
       "      <td>70.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.305800</td>\n",
       "      <td>0.280945</td>\n",
       "      <td>1.49</td>\n",
       "      <td>43.35</td>\n",
       "      <td>8.52</td>\n",
       "      <td>83.74</td>\n",
       "      <td>39.62</td>\n",
       "      <td>68.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.419700</td>\n",
       "      <td>0.249708</td>\n",
       "      <td>1.86</td>\n",
       "      <td>41.09</td>\n",
       "      <td>7.94</td>\n",
       "      <td>88.06</td>\n",
       "      <td>36.8</td>\n",
       "      <td>75.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.326400</td>\n",
       "      <td>0.263779</td>\n",
       "      <td>1.67</td>\n",
       "      <td>35.57</td>\n",
       "      <td>5.43</td>\n",
       "      <td>90.06</td>\n",
       "      <td>30.32</td>\n",
       "      <td>73.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.230042</td>\n",
       "      <td>1.83</td>\n",
       "      <td>39.86</td>\n",
       "      <td>8.59</td>\n",
       "      <td>90.95</td>\n",
       "      <td>35.86</td>\n",
       "      <td>77.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.332700</td>\n",
       "      <td>0.227073</td>\n",
       "      <td>1.80</td>\n",
       "      <td>35.46</td>\n",
       "      <td>6.78</td>\n",
       "      <td>90.66</td>\n",
       "      <td>28.85</td>\n",
       "      <td>79.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.276100</td>\n",
       "      <td>0.226991</td>\n",
       "      <td>1.81</td>\n",
       "      <td>36.12</td>\n",
       "      <td>6.94</td>\n",
       "      <td>91.76</td>\n",
       "      <td>29.43</td>\n",
       "      <td>79.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.305500</td>\n",
       "      <td>0.220739</td>\n",
       "      <td>1.94</td>\n",
       "      <td>36.98</td>\n",
       "      <td>7.24</td>\n",
       "      <td>90.46</td>\n",
       "      <td>30.55</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.293400</td>\n",
       "      <td>0.216680</td>\n",
       "      <td>2.00</td>\n",
       "      <td>35.08</td>\n",
       "      <td>7.18</td>\n",
       "      <td>92.59</td>\n",
       "      <td>28.29</td>\n",
       "      <td>82.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.264900</td>\n",
       "      <td>0.207940</td>\n",
       "      <td>1.97</td>\n",
       "      <td>40.16</td>\n",
       "      <td>8.97</td>\n",
       "      <td>91.68</td>\n",
       "      <td>34.70</td>\n",
       "      <td>80.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.247600</td>\n",
       "      <td>0.205694</td>\n",
       "      <td>2.</td>\n",
       "      <td>40.29</td>\n",
       "      <td>9.59</td>\n",
       "      <td>91.83</td>\n",
       "      <td>35.11</td>\n",
       "      <td>80.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.282800</td>\n",
       "      <td>0.202213</td>\n",
       "      <td>1.99</td>\n",
       "      <td>39.46</td>\n",
       "      <td>8.22</td>\n",
       "      <td>93.58</td>\n",
       "      <td>33.84</td>\n",
       "      <td>82.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.289000</td>\n",
       "      <td>0.198767</td>\n",
       "      <td>2.02</td>\n",
       "      <td>38.99</td>\n",
       "      <td>7.91</td>\n",
       "      <td>93.78</td>\n",
       "      <td>32.84</td>\n",
       "      <td>82.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.289700</td>\n",
       "      <td>0.197976</td>\n",
       "      <td>1.99</td>\n",
       "      <td>39.</td>\n",
       "      <td>8.14</td>\n",
       "      <td>93.46</td>\n",
       "      <td>33.04</td>\n",
       "      <td>82.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.248200</td>\n",
       "      <td>0.196860</td>\n",
       "      <td>2.02</td>\n",
       "      <td>39.34</td>\n",
       "      <td>8.09</td>\n",
       "      <td>93.59</td>\n",
       "      <td>33.49</td>\n",
       "      <td>82.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1500\n",
      "Configuration saved in ./results\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2000\n",
      "Configuration saved in ./results\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2500\n",
      "Configuration saved in ./results\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-3000\n",
      "Configuration saved in ./results\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3270, training_loss=0.4054160027693536, metrics={'train_runtime': 1613.905, 'train_samples_per_second': 2.026, 'train_steps_per_second': 2.026, 'total_flos': 0.0, 'train_loss': 0.4054160027693536, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sru lr of 5e-4\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3270\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3270\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3270' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3270/3270 28:43, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.715600</td>\n",
       "      <td>0.474766</td>\n",
       "      <td>1.47</td>\n",
       "      <td>34.31</td>\n",
       "      <td>2.30</td>\n",
       "      <td>74.83</td>\n",
       "      <td>5.03</td>\n",
       "      <td>23.51</td>\n",
       "      <td>54.96</td>\n",
       "      <td>1.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.002700</td>\n",
       "      <td>0.397932</td>\n",
       "      <td>1.35</td>\n",
       "      <td>36.01</td>\n",
       "      <td>4.60</td>\n",
       "      <td>70.16</td>\n",
       "      <td>4.66</td>\n",
       "      <td>22.50</td>\n",
       "      <td>59.05</td>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.407700</td>\n",
       "      <td>0.576816</td>\n",
       "      <td>1.4</td>\n",
       "      <td>64.61</td>\n",
       "      <td>21.53</td>\n",
       "      <td>63.24</td>\n",
       "      <td>2.31</td>\n",
       "      <td>49.73</td>\n",
       "      <td>51.53</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.607600</td>\n",
       "      <td>0.396596</td>\n",
       "      <td>1.37</td>\n",
       "      <td>41.82</td>\n",
       "      <td>6.82</td>\n",
       "      <td>80.57</td>\n",
       "      <td>6.11</td>\n",
       "      <td>30.14</td>\n",
       "      <td>58.76</td>\n",
       "      <td>1.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.482300</td>\n",
       "      <td>0.416988</td>\n",
       "      <td>1.38</td>\n",
       "      <td>45.03</td>\n",
       "      <td>10.01</td>\n",
       "      <td>74.58</td>\n",
       "      <td>3.96</td>\n",
       "      <td>32.45</td>\n",
       "      <td>58.60</td>\n",
       "      <td>1.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.349200</td>\n",
       "      <td>0.275632</td>\n",
       "      <td>1.74</td>\n",
       "      <td>36.19</td>\n",
       "      <td>6.52</td>\n",
       "      <td>82.49</td>\n",
       "      <td>7.51</td>\n",
       "      <td>21.84</td>\n",
       "      <td>69.87</td>\n",
       "      <td>2.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.450300</td>\n",
       "      <td>0.411370</td>\n",
       "      <td>2.02</td>\n",
       "      <td>44.86</td>\n",
       "      <td>17.14</td>\n",
       "      <td>67.97</td>\n",
       "      <td>2.47</td>\n",
       "      <td>22.69</td>\n",
       "      <td>59.69</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.496200</td>\n",
       "      <td>0.311680</td>\n",
       "      <td>1.64</td>\n",
       "      <td>31.82</td>\n",
       "      <td>4.78</td>\n",
       "      <td>82.2</td>\n",
       "      <td>7.52</td>\n",
       "      <td>15.6</td>\n",
       "      <td>71.5</td>\n",
       "      <td>2.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.438000</td>\n",
       "      <td>0.337457</td>\n",
       "      <td>1.98</td>\n",
       "      <td>30.07</td>\n",
       "      <td>6.52</td>\n",
       "      <td>78.31</td>\n",
       "      <td>4.99</td>\n",
       "      <td>13.66</td>\n",
       "      <td>65.76</td>\n",
       "      <td>1.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.492100</td>\n",
       "      <td>0.321470</td>\n",
       "      <td>1.77</td>\n",
       "      <td>34.01</td>\n",
       "      <td>5.06</td>\n",
       "      <td>76.34</td>\n",
       "      <td>5.85</td>\n",
       "      <td>20.93</td>\n",
       "      <td>67.87</td>\n",
       "      <td>1.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.373900</td>\n",
       "      <td>0.321315</td>\n",
       "      <td>1.89</td>\n",
       "      <td>39.63</td>\n",
       "      <td>8.94</td>\n",
       "      <td>74.94</td>\n",
       "      <td>4.9</td>\n",
       "      <td>24.61</td>\n",
       "      <td>64.79</td>\n",
       "      <td>1.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.385600</td>\n",
       "      <td>0.258230</td>\n",
       "      <td>1.73</td>\n",
       "      <td>26.45</td>\n",
       "      <td>3.93</td>\n",
       "      <td>90.22</td>\n",
       "      <td>10.02</td>\n",
       "      <td>13.05</td>\n",
       "      <td>79.99</td>\n",
       "      <td>2.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.375600</td>\n",
       "      <td>0.295606</td>\n",
       "      <td>1.68</td>\n",
       "      <td>32.51</td>\n",
       "      <td>6.81</td>\n",
       "      <td>77.89</td>\n",
       "      <td>5.63</td>\n",
       "      <td>14.93</td>\n",
       "      <td>71.72</td>\n",
       "      <td>2.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.291200</td>\n",
       "      <td>0.252396</td>\n",
       "      <td>1.83</td>\n",
       "      <td>27.91</td>\n",
       "      <td>4.13</td>\n",
       "      <td>88.13</td>\n",
       "      <td>10.03</td>\n",
       "      <td>13.35</td>\n",
       "      <td>80.30</td>\n",
       "      <td>2.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.384500</td>\n",
       "      <td>0.291286</td>\n",
       "      <td>1.83</td>\n",
       "      <td>30.75</td>\n",
       "      <td>4.09</td>\n",
       "      <td>89.07</td>\n",
       "      <td>10.65</td>\n",
       "      <td>18.75</td>\n",
       "      <td>74.04</td>\n",
       "      <td>2.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.484800</td>\n",
       "      <td>0.254829</td>\n",
       "      <td>1.91</td>\n",
       "      <td>31.72</td>\n",
       "      <td>6.6</td>\n",
       "      <td>82.93</td>\n",
       "      <td>7.14</td>\n",
       "      <td>16.68</td>\n",
       "      <td>75.17</td>\n",
       "      <td>2.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.334600</td>\n",
       "      <td>0.282343</td>\n",
       "      <td>1.76</td>\n",
       "      <td>47.82</td>\n",
       "      <td>16.15</td>\n",
       "      <td>79.71</td>\n",
       "      <td>3.76</td>\n",
       "      <td>29.15</td>\n",
       "      <td>65.90</td>\n",
       "      <td>1.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.332900</td>\n",
       "      <td>0.252752</td>\n",
       "      <td>1.98</td>\n",
       "      <td>40.10</td>\n",
       "      <td>11.04</td>\n",
       "      <td>84.26</td>\n",
       "      <td>5.04</td>\n",
       "      <td>23.81</td>\n",
       "      <td>71.40</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.280300</td>\n",
       "      <td>0.232916</td>\n",
       "      <td>1.91</td>\n",
       "      <td>38.93</td>\n",
       "      <td>8.12</td>\n",
       "      <td>89.01</td>\n",
       "      <td>7.08</td>\n",
       "      <td>24.19</td>\n",
       "      <td>73.45</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.413400</td>\n",
       "      <td>0.236350</td>\n",
       "      <td>1.87</td>\n",
       "      <td>36.97</td>\n",
       "      <td>6.54</td>\n",
       "      <td>90.13</td>\n",
       "      <td>6.86</td>\n",
       "      <td>22.1</td>\n",
       "      <td>75.64</td>\n",
       "      <td>2.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.289700</td>\n",
       "      <td>0.227883</td>\n",
       "      <td>1.93</td>\n",
       "      <td>38.95</td>\n",
       "      <td>8.11</td>\n",
       "      <td>88.17</td>\n",
       "      <td>7.15</td>\n",
       "      <td>25.03</td>\n",
       "      <td>75.96</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.284500</td>\n",
       "      <td>0.217839</td>\n",
       "      <td>1.82</td>\n",
       "      <td>39.1</td>\n",
       "      <td>8.88</td>\n",
       "      <td>90.92</td>\n",
       "      <td>6.41</td>\n",
       "      <td>24.59</td>\n",
       "      <td>75.15</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.295800</td>\n",
       "      <td>0.210158</td>\n",
       "      <td>1.87</td>\n",
       "      <td>39.28</td>\n",
       "      <td>9.89</td>\n",
       "      <td>90.8</td>\n",
       "      <td>6.63</td>\n",
       "      <td>24.23</td>\n",
       "      <td>74.94</td>\n",
       "      <td>2.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.238900</td>\n",
       "      <td>0.208207</td>\n",
       "      <td>1.94</td>\n",
       "      <td>39.81</td>\n",
       "      <td>10.39</td>\n",
       "      <td>91.88</td>\n",
       "      <td>6.69</td>\n",
       "      <td>25.08</td>\n",
       "      <td>74.7</td>\n",
       "      <td>2.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.276600</td>\n",
       "      <td>0.197196</td>\n",
       "      <td>1.80</td>\n",
       "      <td>34.67</td>\n",
       "      <td>7.20</td>\n",
       "      <td>93.94</td>\n",
       "      <td>9.07</td>\n",
       "      <td>21.54</td>\n",
       "      <td>79.72</td>\n",
       "      <td>2.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.257300</td>\n",
       "      <td>0.195312</td>\n",
       "      <td>1.92</td>\n",
       "      <td>39.43</td>\n",
       "      <td>9.75</td>\n",
       "      <td>92.98</td>\n",
       "      <td>7.43</td>\n",
       "      <td>25.26</td>\n",
       "      <td>76.82</td>\n",
       "      <td>2.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.224000</td>\n",
       "      <td>0.191860</td>\n",
       "      <td>1.94</td>\n",
       "      <td>39.55</td>\n",
       "      <td>10.50</td>\n",
       "      <td>92.08</td>\n",
       "      <td>6.90</td>\n",
       "      <td>24.74</td>\n",
       "      <td>77.30</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.214300</td>\n",
       "      <td>0.189584</td>\n",
       "      <td>1.98</td>\n",
       "      <td>40.</td>\n",
       "      <td>10.68</td>\n",
       "      <td>92.80</td>\n",
       "      <td>7.17</td>\n",
       "      <td>25.02</td>\n",
       "      <td>77.15</td>\n",
       "      <td>2.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.251000</td>\n",
       "      <td>0.188702</td>\n",
       "      <td>1.97</td>\n",
       "      <td>38.2</td>\n",
       "      <td>8.91</td>\n",
       "      <td>93.68</td>\n",
       "      <td>7.25</td>\n",
       "      <td>24.31</td>\n",
       "      <td>79.55</td>\n",
       "      <td>2.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.261200</td>\n",
       "      <td>0.185677</td>\n",
       "      <td>1.97</td>\n",
       "      <td>38.06</td>\n",
       "      <td>8.80</td>\n",
       "      <td>94.20</td>\n",
       "      <td>7.69</td>\n",
       "      <td>24.06</td>\n",
       "      <td>79.75</td>\n",
       "      <td>2.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.251500</td>\n",
       "      <td>0.185464</td>\n",
       "      <td>1.98</td>\n",
       "      <td>38.77</td>\n",
       "      <td>9.24</td>\n",
       "      <td>94.05</td>\n",
       "      <td>7.63</td>\n",
       "      <td>24.56</td>\n",
       "      <td>79.52</td>\n",
       "      <td>2.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.219800</td>\n",
       "      <td>0.184963</td>\n",
       "      <td>1.98</td>\n",
       "      <td>38.82</td>\n",
       "      <td>9.32</td>\n",
       "      <td>94.03</td>\n",
       "      <td>7.6</td>\n",
       "      <td>24.57</td>\n",
       "      <td>79.49</td>\n",
       "      <td>2.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1500\n",
      "Configuration saved in ./results\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2000\n",
      "Configuration saved in ./results\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2500\n",
      "Configuration saved in ./results\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-3000\n",
      "Configuration saved in ./results\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3270, training_loss=0.37757016569831686, metrics={'train_runtime': 1725.6916, 'train_samples_per_second': 1.895, 'train_steps_per_second': 1.895, 'total_flos': 0.0, 'train_loss': 0.37757016569831686, 'epoch': 1.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sru lr of 3e-4 (recommended by sru paper)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3270\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3270\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3270' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3270/3270 30:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.782200</td>\n",
       "      <td>0.507575</td>\n",
       "      <td>1.69</td>\n",
       "      <td>38.27</td>\n",
       "      <td>2.82</td>\n",
       "      <td>61.22</td>\n",
       "      <td>3.51</td>\n",
       "      <td>29.37</td>\n",
       "      <td>54.15</td>\n",
       "      <td>1.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.912600</td>\n",
       "      <td>0.412638</td>\n",
       "      <td>1.7</td>\n",
       "      <td>29.22</td>\n",
       "      <td>3.39</td>\n",
       "      <td>87.24</td>\n",
       "      <td>8.66</td>\n",
       "      <td>13.84</td>\n",
       "      <td>55.5</td>\n",
       "      <td>1.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.375100</td>\n",
       "      <td>0.371140</td>\n",
       "      <td>1.52</td>\n",
       "      <td>29.61</td>\n",
       "      <td>3.99</td>\n",
       "      <td>87.84</td>\n",
       "      <td>6.89</td>\n",
       "      <td>14.14</td>\n",
       "      <td>61.09</td>\n",
       "      <td>1.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.519900</td>\n",
       "      <td>0.322033</td>\n",
       "      <td>1.59</td>\n",
       "      <td>30.84</td>\n",
       "      <td>1.79</td>\n",
       "      <td>95.99</td>\n",
       "      <td>12.13</td>\n",
       "      <td>17.96</td>\n",
       "      <td>70.68</td>\n",
       "      <td>2.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.443200</td>\n",
       "      <td>0.309579</td>\n",
       "      <td>1.59</td>\n",
       "      <td>37.17</td>\n",
       "      <td>3.13</td>\n",
       "      <td>92.69</td>\n",
       "      <td>8.45</td>\n",
       "      <td>27.85</td>\n",
       "      <td>65.61</td>\n",
       "      <td>2.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.312800</td>\n",
       "      <td>0.306958</td>\n",
       "      <td>1.75</td>\n",
       "      <td>46.30</td>\n",
       "      <td>6.57</td>\n",
       "      <td>83.6</td>\n",
       "      <td>6.39</td>\n",
       "      <td>38.12</td>\n",
       "      <td>63.18</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.428100</td>\n",
       "      <td>0.289558</td>\n",
       "      <td>1.79</td>\n",
       "      <td>39.36</td>\n",
       "      <td>4.67</td>\n",
       "      <td>87.35</td>\n",
       "      <td>6.59</td>\n",
       "      <td>29.44</td>\n",
       "      <td>70.26</td>\n",
       "      <td>1.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.368500</td>\n",
       "      <td>0.289121</td>\n",
       "      <td>1.82</td>\n",
       "      <td>38.88</td>\n",
       "      <td>6.51</td>\n",
       "      <td>84.92</td>\n",
       "      <td>4.81</td>\n",
       "      <td>27.71</td>\n",
       "      <td>71.16</td>\n",
       "      <td>2.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.339300</td>\n",
       "      <td>0.257594</td>\n",
       "      <td>1.80</td>\n",
       "      <td>42.79</td>\n",
       "      <td>8.24</td>\n",
       "      <td>88.87</td>\n",
       "      <td>4.83</td>\n",
       "      <td>31.80</td>\n",
       "      <td>69.48</td>\n",
       "      <td>2.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.345800</td>\n",
       "      <td>0.247257</td>\n",
       "      <td>1.94</td>\n",
       "      <td>45.57</td>\n",
       "      <td>12.59</td>\n",
       "      <td>85.36</td>\n",
       "      <td>4.14</td>\n",
       "      <td>29.79</td>\n",
       "      <td>70.93</td>\n",
       "      <td>1.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.320200</td>\n",
       "      <td>0.232193</td>\n",
       "      <td>1.72</td>\n",
       "      <td>39.12</td>\n",
       "      <td>6.11</td>\n",
       "      <td>91.38</td>\n",
       "      <td>7.51</td>\n",
       "      <td>28.21</td>\n",
       "      <td>73.07</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.276100</td>\n",
       "      <td>0.220932</td>\n",
       "      <td>2.22</td>\n",
       "      <td>41.87</td>\n",
       "      <td>8.91</td>\n",
       "      <td>94.01</td>\n",
       "      <td>6.09</td>\n",
       "      <td>29.78</td>\n",
       "      <td>76.39</td>\n",
       "      <td>2.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.261800</td>\n",
       "      <td>0.213651</td>\n",
       "      <td>2.03</td>\n",
       "      <td>41.53</td>\n",
       "      <td>9.56</td>\n",
       "      <td>91.8</td>\n",
       "      <td>5.51</td>\n",
       "      <td>28.11</td>\n",
       "      <td>76.32</td>\n",
       "      <td>1.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.201300</td>\n",
       "      <td>0.188296</td>\n",
       "      <td>2.16</td>\n",
       "      <td>38.45</td>\n",
       "      <td>7.43</td>\n",
       "      <td>96.92</td>\n",
       "      <td>6.74</td>\n",
       "      <td>26.48</td>\n",
       "      <td>81.79</td>\n",
       "      <td>2.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.234300</td>\n",
       "      <td>0.170681</td>\n",
       "      <td>2.16</td>\n",
       "      <td>40.38</td>\n",
       "      <td>8.49</td>\n",
       "      <td>95.97</td>\n",
       "      <td>6.10</td>\n",
       "      <td>26.94</td>\n",
       "      <td>84.34</td>\n",
       "      <td>2.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.296700</td>\n",
       "      <td>0.171422</td>\n",
       "      <td>2.25</td>\n",
       "      <td>42.43</td>\n",
       "      <td>8.90</td>\n",
       "      <td>97.62</td>\n",
       "      <td>8.09</td>\n",
       "      <td>29.43</td>\n",
       "      <td>83.15</td>\n",
       "      <td>2.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.216200</td>\n",
       "      <td>0.160589</td>\n",
       "      <td>2.22</td>\n",
       "      <td>43.27</td>\n",
       "      <td>11.13</td>\n",
       "      <td>96.09</td>\n",
       "      <td>5.22</td>\n",
       "      <td>28.11</td>\n",
       "      <td>83.57</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.192200</td>\n",
       "      <td>0.150227</td>\n",
       "      <td>2.27</td>\n",
       "      <td>43.09</td>\n",
       "      <td>11.96</td>\n",
       "      <td>96.62</td>\n",
       "      <td>5.37</td>\n",
       "      <td>27.70</td>\n",
       "      <td>84.63</td>\n",
       "      <td>2.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.159300</td>\n",
       "      <td>0.141130</td>\n",
       "      <td>2.34</td>\n",
       "      <td>46.38</td>\n",
       "      <td>14.7</td>\n",
       "      <td>97.16</td>\n",
       "      <td>6.15</td>\n",
       "      <td>29.04</td>\n",
       "      <td>85.22</td>\n",
       "      <td>2.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.194000</td>\n",
       "      <td>0.138719</td>\n",
       "      <td>2.36</td>\n",
       "      <td>50.20</td>\n",
       "      <td>18.25</td>\n",
       "      <td>96.88</td>\n",
       "      <td>5.73</td>\n",
       "      <td>30.31</td>\n",
       "      <td>83.74</td>\n",
       "      <td>2.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.166900</td>\n",
       "      <td>0.140650</td>\n",
       "      <td>2.32</td>\n",
       "      <td>46.20</td>\n",
       "      <td>14.22</td>\n",
       "      <td>97.11</td>\n",
       "      <td>5.35</td>\n",
       "      <td>29.37</td>\n",
       "      <td>85.78</td>\n",
       "      <td>2.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.168900</td>\n",
       "      <td>0.136455</td>\n",
       "      <td>2.37</td>\n",
       "      <td>49.23</td>\n",
       "      <td>17.07</td>\n",
       "      <td>96.99</td>\n",
       "      <td>5.28</td>\n",
       "      <td>30.37</td>\n",
       "      <td>84.83</td>\n",
       "      <td>2.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.190200</td>\n",
       "      <td>0.126001</td>\n",
       "      <td>2.35</td>\n",
       "      <td>47.54</td>\n",
       "      <td>16.05</td>\n",
       "      <td>97.99</td>\n",
       "      <td>5.65</td>\n",
       "      <td>29.39</td>\n",
       "      <td>87.51</td>\n",
       "      <td>2.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.136800</td>\n",
       "      <td>0.121852</td>\n",
       "      <td>2.35</td>\n",
       "      <td>47.39</td>\n",
       "      <td>15.53</td>\n",
       "      <td>98.37</td>\n",
       "      <td>6.10</td>\n",
       "      <td>29.85</td>\n",
       "      <td>89.2</td>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.148000</td>\n",
       "      <td>0.119641</td>\n",
       "      <td>2.27</td>\n",
       "      <td>44.38</td>\n",
       "      <td>14.09</td>\n",
       "      <td>98.58</td>\n",
       "      <td>5.95</td>\n",
       "      <td>27.45</td>\n",
       "      <td>90.20</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.149400</td>\n",
       "      <td>0.118748</td>\n",
       "      <td>2.32</td>\n",
       "      <td>46.47</td>\n",
       "      <td>15.52</td>\n",
       "      <td>98.25</td>\n",
       "      <td>6.43</td>\n",
       "      <td>28.6</td>\n",
       "      <td>89.34</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.129700</td>\n",
       "      <td>0.116827</td>\n",
       "      <td>2.33</td>\n",
       "      <td>45.94</td>\n",
       "      <td>15.46</td>\n",
       "      <td>98.64</td>\n",
       "      <td>6.37</td>\n",
       "      <td>27.98</td>\n",
       "      <td>90.05</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.124100</td>\n",
       "      <td>0.114864</td>\n",
       "      <td>2.34</td>\n",
       "      <td>47.14</td>\n",
       "      <td>16.38</td>\n",
       "      <td>98.68</td>\n",
       "      <td>6.49</td>\n",
       "      <td>28.6</td>\n",
       "      <td>89.97</td>\n",
       "      <td>2.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.140500</td>\n",
       "      <td>0.114371</td>\n",
       "      <td>2.33</td>\n",
       "      <td>47.12</td>\n",
       "      <td>16.55</td>\n",
       "      <td>98.59</td>\n",
       "      <td>6.16</td>\n",
       "      <td>28.30</td>\n",
       "      <td>89.85</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.142500</td>\n",
       "      <td>0.113230</td>\n",
       "      <td>2.32</td>\n",
       "      <td>46.34</td>\n",
       "      <td>15.85</td>\n",
       "      <td>98.73</td>\n",
       "      <td>6.57</td>\n",
       "      <td>28.05</td>\n",
       "      <td>90.51</td>\n",
       "      <td>2.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.135800</td>\n",
       "      <td>0.112851</td>\n",
       "      <td>2.33</td>\n",
       "      <td>47.25</td>\n",
       "      <td>16.50</td>\n",
       "      <td>98.68</td>\n",
       "      <td>6.31</td>\n",
       "      <td>28.55</td>\n",
       "      <td>90.20</td>\n",
       "      <td>2.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.132300</td>\n",
       "      <td>0.112691</td>\n",
       "      <td>2.33</td>\n",
       "      <td>47.24</td>\n",
       "      <td>16.52</td>\n",
       "      <td>98.68</td>\n",
       "      <td>6.39</td>\n",
       "      <td>28.52</td>\n",
       "      <td>90.21</td>\n",
       "      <td>2.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1500\n",
      "Configuration saved in ./results\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2000\n",
      "Configuration saved in ./results\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2500\n",
      "Configuration saved in ./results\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-3000\n",
      "Configuration saved in ./results\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3270, training_loss=0.2766398222803705, metrics={'train_runtime': 1805.5267, 'train_samples_per_second': 1.811, 'train_steps_per_second': 1.811, 'total_flos': 0.0, 'train_loss': 0.2766398222803705, 'epoch': 1.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sru lr of 1e-4 no embedding layer norm\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## failed idea: have the model go through a timewise curriculum of the data\n",
    "The idea was that if the training didn't respect the timeseries nature of the data, then the model could \"memorize\" parts of the data and use that to predict past data better (which wouldn't be good at test time). Seemingly this isn't an issue as the model performs better on a validation set that does come from the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraderTrainer(Trainer):\n",
    "\n",
    "    def get_train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Returns the training :class:`~torch.utils.data.DataLoader`.\n",
    "\n",
    "        Will use no sampler if :obj:`self.train_dataset` does not implement :obj:`__len__`, a random sampler (adapted\n",
    "        to distributed training if necessary) otherwise.\n",
    "\n",
    "        Subclass and override this method if you want to inject some custom behavior.\n",
    "        \"\"\"\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "        train_sampler = self._get_train_sampler()\n",
    "\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.args.train_batch_size,\n",
    "#             shuffle=False, # TO STOP OVERFITTING\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=self.data_collator,\n",
    "            drop_last=self.args.dataloader_drop_last,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make sure unshuffled split maintains order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = Dataset.from_dict({\"input\": list(range(100))})\n",
    "split = foo.train_test_split(.1, shuffle = False)\n",
    "valid_test = split['test'].train_test_split(.5, shuffle = False)\n",
    "foo = DatasetDict({\n",
    "    'train': split['train'],\n",
    "    'validation': valid_test['train'],\n",
    "    'test': valid_test['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([90, 91, 92, 93, 94], [95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo['validation']['input'], foo['test']['input']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quick timing check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2Trader(config).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.2 ms ± 2.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "fake_data = torch.randn(4, 391, 256)\n",
    "fake_data = fake_data.cuda()\n",
    "model(fake_data)\n",
    "cpu = fake_data.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 9 layers\n"
     ]
    }
   ],
   "source": [
    "model = GPT2Trader(config).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748 ms ± 82.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "fake_data = torch.randn(4, 391, 256)\n",
    "model(fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
