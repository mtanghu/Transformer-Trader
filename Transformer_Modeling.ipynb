{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Model, GPT2Config, PreTrainedModel\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Competitive edge techniques:\n",
    "- Custom loss function based on profit with trade sizing (more of a conceptual innovation)\n",
    "- multiloss to integrate shorter and longer trades (practical innovation to unlock longer times and more data and better gradients)\n",
    "- best in class transformer -- most people don't really understand how transformers even work lol\n",
    "    - hopefully will get more data efficient models\n",
    "- right now model is quite greedy (for my own sanity), but it's totally possible to consider more of a <50% accuracy model but just with higher upside\n",
    "\n",
    "TODO:\n",
    "- compute metrics functions\n",
    "    - still need say a below <30min full trade %, accuracy profitability and over 30+ full trade % and accuracy and profitability\n",
    "    - try 5e-4 lr\n",
    "- Try sru++\n",
    "\n",
    "- longer timeframe? like up to 120min? --\n",
    "- data analysis on when/where the model trades (which time frame and what time period throughout day)\n",
    "    - should I do a graphical analysis?\n",
    "    - what about raw accuracy of big trades?\n",
    "- trickier goal, do usd/jpy and usd/gbp, should they be integrated all at once though? (would be tricky to handle multiple datastreams) may leave this for later after setting up paper trading\n",
    "    - would be fine to just use it as a transfer learning tool though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Trader(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # use levine 2020 layer numbers\n",
    "        n_layer = round((math.log(config.n_embd) - 5.039) / 5.55e-2)\n",
    "        n_layer = max(1, n_layer)\n",
    "        print(f'Using {n_layer} layers')\n",
    "        config.n_layer = n_layer\n",
    "        \n",
    "        config.initializer_range = 1 / math.sqrt(config.n_embd)\n",
    "        \n",
    "        self.embed = nn.Linear(5, config.n_embd, bias = False)\n",
    "        self.norm = nn.LayerNorm(config.n_embd)\n",
    "        self.gpt = GPT2Model(config)\n",
    "        self.trade = nn.Linear(config.n_embd, 120, bias = False)\n",
    "\n",
    "\n",
    "    def forward(self, ohlcv, future):\n",
    "        embed = self.norm(self.embed(ohlcv))\n",
    "        hidden = self.gpt(inputs_embeds = embed).last_hidden_state\n",
    "        \n",
    "        soft_trade = self.trade(hidden)\n",
    "        \n",
    "        # sharpe information\n",
    "        soft_trade = torch.tanh(soft_trade)\n",
    "        soft_profit = soft_trade * future\n",
    "        \n",
    "        # the exp is so that loss is purely positive and minimizes toward 0 (also losses have more loss than profit)\n",
    "        loss_ppl = torch.square(((-soft_profit + future.abs()))).mean()\n",
    "        \n",
    "        # penalty for big trades (to stop trading from happening with no profit)\n",
    "        trade_penalty = soft_trade.abs().mean()\n",
    "        \n",
    "        loss = loss_ppl + .1 * trade_penalty # .1 means that a 100% position must make at least .1 of a std to offset loss\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'profits': soft_profit,\n",
    "            'trades': soft_trade,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds):\n",
    "    soft_profit, soft_trade = preds.predictions\n",
    "    abs_trade = np.abs(soft_trade)\n",
    "    trades = abs_trade.sum()\n",
    "    \n",
    "    day_profits = soft_profit.sum(axis = (1, 2))\n",
    "    \n",
    "    metrics = {\n",
    "        'day sharpe': day_profits.mean() / day_profits.std(),\n",
    "        'trade %': trades * 100 / soft_profit.size,\n",
    "        'full trade %': (abs_trade > .9).mean() * 100,\n",
    "        'full trade accuracy': (soft_profit[abs_trade > .9] > 0).mean() * 100,\n",
    "        'medium trade %': (abs_trade > .5).mean() * 100,\n",
    "        'medium trade accuracy': (soft_profit[abs_trade > .5] > 0).mean() * 100,\n",
    "    }\n",
    "    \n",
    "    # round the metrics\n",
    "    metrics = {k: np.format_float_positional(v, precision = 2) for k, v in metrics.items()}\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "eurusd = Dataset.load_from_disk('data/EURUSD_day.ds')\n",
    "\n",
    "# make splits\n",
    "split = eurusd.train_test_split(.05, shuffle = False)\n",
    "valid_test = split['test'].train_test_split(.5, shuffle = False)\n",
    "eurusd = DatasetDict({\n",
    "    'train': split['train'],\n",
    "    'validation': valid_test['train'],\n",
    "    'test': valid_test['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    logging_strategy = \"steps\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    logging_steps = 100,\n",
    "    eval_steps = 100,\n",
    "    report_to = \"none\",\n",
    "    learning_rate = 2e-4,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    warmup_ratio = .05,\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size = 1,\n",
    "    per_device_eval_batch_size = 1,\n",
    "    max_grad_norm = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config(\n",
    "    n_embd = 384, n_head = 6, vocab_size = 0, n_positions = 2000,\n",
    "    resid_pdrop = .01, embd_pdrop = .01, attn_pdrop = .01, # low dropout since only using 1 epoch training and to make model more robust to data issues (.1 has worse loss, accuracy & t-score)\n",
    "    summary_first_dropout = 0, summary_proj_to_labels = False,\n",
    "    scale_attn_by_inverse_layer_idx = True, use_cache = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 16 layers\n"
     ]
    }
   ],
   "source": [
    "model = GPT2Trader(config).cuda()\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = eurusd['train'],\n",
    "    eval_dataset = eurusd['validation'],\n",
    "    compute_metrics = compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3270\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3270\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3270' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3270/3270 31:18, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.740600</td>\n",
       "      <td>0.527701</td>\n",
       "      <td>2.02</td>\n",
       "      <td>38.95</td>\n",
       "      <td>0.09</td>\n",
       "      <td>52.46</td>\n",
       "      <td>32.52</td>\n",
       "      <td>53.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.977000</td>\n",
       "      <td>0.492953</td>\n",
       "      <td>1.67</td>\n",
       "      <td>26.88</td>\n",
       "      <td>0.09</td>\n",
       "      <td>59.97</td>\n",
       "      <td>21.80</td>\n",
       "      <td>56.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.539400</td>\n",
       "      <td>0.543064</td>\n",
       "      <td>1.8</td>\n",
       "      <td>29.15</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.</td>\n",
       "      <td>27.00</td>\n",
       "      <td>52.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.670100</td>\n",
       "      <td>0.463039</td>\n",
       "      <td>1.55</td>\n",
       "      <td>29.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>20.</td>\n",
       "      <td>27.</td>\n",
       "      <td>55.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.625400</td>\n",
       "      <td>0.451693</td>\n",
       "      <td>1.31</td>\n",
       "      <td>31.58</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.</td>\n",
       "      <td>25.62</td>\n",
       "      <td>55.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.480400</td>\n",
       "      <td>0.496943</td>\n",
       "      <td>1.92</td>\n",
       "      <td>42.34</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>41.84</td>\n",
       "      <td>53.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.660500</td>\n",
       "      <td>0.484283</td>\n",
       "      <td>1.5</td>\n",
       "      <td>28.90</td>\n",
       "      <td>0.01</td>\n",
       "      <td>51.19</td>\n",
       "      <td>26.33</td>\n",
       "      <td>56.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.563100</td>\n",
       "      <td>0.441980</td>\n",
       "      <td>1.36</td>\n",
       "      <td>22.58</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>20.</td>\n",
       "      <td>56.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.565500</td>\n",
       "      <td>0.439754</td>\n",
       "      <td>1.29</td>\n",
       "      <td>24.82</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.</td>\n",
       "      <td>18.22</td>\n",
       "      <td>57.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.710500</td>\n",
       "      <td>0.449362</td>\n",
       "      <td>1.41</td>\n",
       "      <td>22.11</td>\n",
       "      <td>0.04</td>\n",
       "      <td>65.72</td>\n",
       "      <td>17.88</td>\n",
       "      <td>58.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.522800</td>\n",
       "      <td>0.423899</td>\n",
       "      <td>1.29</td>\n",
       "      <td>16.77</td>\n",
       "      <td>0.03</td>\n",
       "      <td>50.35</td>\n",
       "      <td>12.01</td>\n",
       "      <td>62.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.547200</td>\n",
       "      <td>0.392619</td>\n",
       "      <td>1.28</td>\n",
       "      <td>15.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>61.67</td>\n",
       "      <td>7.3</td>\n",
       "      <td>71.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.484300</td>\n",
       "      <td>0.385493</td>\n",
       "      <td>1.29</td>\n",
       "      <td>16.64</td>\n",
       "      <td>3.71</td>\n",
       "      <td>82.07</td>\n",
       "      <td>6.44</td>\n",
       "      <td>74.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.399500</td>\n",
       "      <td>0.373606</td>\n",
       "      <td>1.23</td>\n",
       "      <td>12.00</td>\n",
       "      <td>4.36</td>\n",
       "      <td>79.60</td>\n",
       "      <td>6.06</td>\n",
       "      <td>75.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.466900</td>\n",
       "      <td>0.372316</td>\n",
       "      <td>1.30</td>\n",
       "      <td>12.05</td>\n",
       "      <td>5.36</td>\n",
       "      <td>77.87</td>\n",
       "      <td>6.43</td>\n",
       "      <td>75.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.564900</td>\n",
       "      <td>0.376707</td>\n",
       "      <td>1.24</td>\n",
       "      <td>13.49</td>\n",
       "      <td>5.33</td>\n",
       "      <td>76.04</td>\n",
       "      <td>6.99</td>\n",
       "      <td>71.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.537900</td>\n",
       "      <td>0.364282</td>\n",
       "      <td>1.31</td>\n",
       "      <td>9.27</td>\n",
       "      <td>5.33</td>\n",
       "      <td>78.98</td>\n",
       "      <td>6.42</td>\n",
       "      <td>76.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.492700</td>\n",
       "      <td>0.383330</td>\n",
       "      <td>1.27</td>\n",
       "      <td>17.88</td>\n",
       "      <td>5.28</td>\n",
       "      <td>79.22</td>\n",
       "      <td>7.33</td>\n",
       "      <td>72.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.430200</td>\n",
       "      <td>0.371721</td>\n",
       "      <td>1.38</td>\n",
       "      <td>11.51</td>\n",
       "      <td>5.63</td>\n",
       "      <td>78.57</td>\n",
       "      <td>6.9</td>\n",
       "      <td>75.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.560900</td>\n",
       "      <td>0.366097</td>\n",
       "      <td>1.29</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5.19</td>\n",
       "      <td>79.14</td>\n",
       "      <td>5.95</td>\n",
       "      <td>77.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.468800</td>\n",
       "      <td>0.365137</td>\n",
       "      <td>1.3</td>\n",
       "      <td>10.45</td>\n",
       "      <td>5.48</td>\n",
       "      <td>78.97</td>\n",
       "      <td>6.09</td>\n",
       "      <td>77.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.517500</td>\n",
       "      <td>0.365354</td>\n",
       "      <td>1.28</td>\n",
       "      <td>11.24</td>\n",
       "      <td>5.52</td>\n",
       "      <td>78.82</td>\n",
       "      <td>6.16</td>\n",
       "      <td>77.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.570800</td>\n",
       "      <td>0.366208</td>\n",
       "      <td>1.30</td>\n",
       "      <td>9.64</td>\n",
       "      <td>5.37</td>\n",
       "      <td>79.05</td>\n",
       "      <td>5.96</td>\n",
       "      <td>77.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.447200</td>\n",
       "      <td>0.362986</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.83</td>\n",
       "      <td>5.50</td>\n",
       "      <td>78.6</td>\n",
       "      <td>6.16</td>\n",
       "      <td>76.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.473500</td>\n",
       "      <td>0.364681</td>\n",
       "      <td>1.30</td>\n",
       "      <td>8.52</td>\n",
       "      <td>5.47</td>\n",
       "      <td>78.55</td>\n",
       "      <td>6.07</td>\n",
       "      <td>76.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.506700</td>\n",
       "      <td>0.362462</td>\n",
       "      <td>1.28</td>\n",
       "      <td>7.82</td>\n",
       "      <td>5.17</td>\n",
       "      <td>79.36</td>\n",
       "      <td>5.86</td>\n",
       "      <td>77.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.426600</td>\n",
       "      <td>0.362636</td>\n",
       "      <td>1.28</td>\n",
       "      <td>8.18</td>\n",
       "      <td>5.38</td>\n",
       "      <td>78.80</td>\n",
       "      <td>5.94</td>\n",
       "      <td>77.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.443500</td>\n",
       "      <td>0.364341</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.97</td>\n",
       "      <td>5.48</td>\n",
       "      <td>78.79</td>\n",
       "      <td>6.01</td>\n",
       "      <td>77.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.505200</td>\n",
       "      <td>0.363033</td>\n",
       "      <td>1.3</td>\n",
       "      <td>7.73</td>\n",
       "      <td>5.46</td>\n",
       "      <td>78.87</td>\n",
       "      <td>5.96</td>\n",
       "      <td>77.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.509200</td>\n",
       "      <td>0.362144</td>\n",
       "      <td>1.3</td>\n",
       "      <td>7.55</td>\n",
       "      <td>5.42</td>\n",
       "      <td>79.01</td>\n",
       "      <td>5.96</td>\n",
       "      <td>77.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.507900</td>\n",
       "      <td>0.362074</td>\n",
       "      <td>1.29</td>\n",
       "      <td>7.82</td>\n",
       "      <td>5.37</td>\n",
       "      <td>79.12</td>\n",
       "      <td>5.95</td>\n",
       "      <td>77.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.455800</td>\n",
       "      <td>0.361957</td>\n",
       "      <td>1.29</td>\n",
       "      <td>7.61</td>\n",
       "      <td>5.39</td>\n",
       "      <td>79.03</td>\n",
       "      <td>5.95</td>\n",
       "      <td>77.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "<ipython-input-43-52e946b60fdf>:12: RuntimeWarning: Mean of empty slice.\n",
      "  'full trade accuracy': (soft_profit[abs_trade > .9] > 0).mean() * 100,\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1500\n",
      "Configuration saved in ./results\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2000\n",
      "Configuration saved in ./results\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2500\n",
      "Configuration saved in ./results\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-3000\n",
      "Configuration saved in ./results\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3270, training_loss=0.5430762673007603, metrics={'train_runtime': 1879.3917, 'train_samples_per_second': 1.74, 'train_steps_per_second': 1.74, 'total_flos': 0.0, 'train_loss': 0.5430762673007603, 'epoch': 1.0})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2e-4 lr\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3270\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3270\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3270' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3270/3270 31:22, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.739100</td>\n",
       "      <td>0.576555</td>\n",
       "      <td>1.18</td>\n",
       "      <td>55.53</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>68.10</td>\n",
       "      <td>52.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.038700</td>\n",
       "      <td>0.508294</td>\n",
       "      <td>1.57</td>\n",
       "      <td>25.15</td>\n",
       "      <td>0.04</td>\n",
       "      <td>53.85</td>\n",
       "      <td>19.25</td>\n",
       "      <td>56.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.546400</td>\n",
       "      <td>0.482877</td>\n",
       "      <td>1.60</td>\n",
       "      <td>33.05</td>\n",
       "      <td>0.02</td>\n",
       "      <td>43.69</td>\n",
       "      <td>32.39</td>\n",
       "      <td>53.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.691100</td>\n",
       "      <td>0.527884</td>\n",
       "      <td>1.36</td>\n",
       "      <td>42.9</td>\n",
       "      <td>1.1</td>\n",
       "      <td>61.84</td>\n",
       "      <td>34.89</td>\n",
       "      <td>54.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.639200</td>\n",
       "      <td>0.453005</td>\n",
       "      <td>1.28</td>\n",
       "      <td>27.61</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>19.7</td>\n",
       "      <td>57.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.492500</td>\n",
       "      <td>0.488201</td>\n",
       "      <td>1.82</td>\n",
       "      <td>38.68</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>34.53</td>\n",
       "      <td>54.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.654900</td>\n",
       "      <td>0.472404</td>\n",
       "      <td>1.43</td>\n",
       "      <td>36.18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.99</td>\n",
       "      <td>31.99</td>\n",
       "      <td>54.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.555600</td>\n",
       "      <td>0.428319</td>\n",
       "      <td>1.41</td>\n",
       "      <td>21.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>42.86</td>\n",
       "      <td>17.26</td>\n",
       "      <td>58.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.554300</td>\n",
       "      <td>0.445764</td>\n",
       "      <td>1.26</td>\n",
       "      <td>25.83</td>\n",
       "      <td>0.16</td>\n",
       "      <td>52.64</td>\n",
       "      <td>15.78</td>\n",
       "      <td>59.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.662100</td>\n",
       "      <td>0.395642</td>\n",
       "      <td>1.37</td>\n",
       "      <td>19.72</td>\n",
       "      <td>4.11</td>\n",
       "      <td>77.08</td>\n",
       "      <td>8.08</td>\n",
       "      <td>68.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.471200</td>\n",
       "      <td>0.408307</td>\n",
       "      <td>1.36</td>\n",
       "      <td>24.43</td>\n",
       "      <td>2.35</td>\n",
       "      <td>83.77</td>\n",
       "      <td>6.88</td>\n",
       "      <td>74.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.479100</td>\n",
       "      <td>0.375257</td>\n",
       "      <td>1.23</td>\n",
       "      <td>16.63</td>\n",
       "      <td>5.12</td>\n",
       "      <td>79.26</td>\n",
       "      <td>6.15</td>\n",
       "      <td>77.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.465000</td>\n",
       "      <td>0.391933</td>\n",
       "      <td>1.18</td>\n",
       "      <td>25.64</td>\n",
       "      <td>6.06</td>\n",
       "      <td>78.17</td>\n",
       "      <td>6.44</td>\n",
       "      <td>77.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.391800</td>\n",
       "      <td>0.364388</td>\n",
       "      <td>1.32</td>\n",
       "      <td>10.43</td>\n",
       "      <td>6.19</td>\n",
       "      <td>77.89</td>\n",
       "      <td>6.54</td>\n",
       "      <td>77.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.467300</td>\n",
       "      <td>0.363665</td>\n",
       "      <td>1.33</td>\n",
       "      <td>8.97</td>\n",
       "      <td>5.52</td>\n",
       "      <td>79.78</td>\n",
       "      <td>6.34</td>\n",
       "      <td>77.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.558400</td>\n",
       "      <td>0.365436</td>\n",
       "      <td>1.24</td>\n",
       "      <td>10.32</td>\n",
       "      <td>3.95</td>\n",
       "      <td>84.00</td>\n",
       "      <td>5.93</td>\n",
       "      <td>78.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.524900</td>\n",
       "      <td>0.363330</td>\n",
       "      <td>1.31</td>\n",
       "      <td>9.04</td>\n",
       "      <td>6.</td>\n",
       "      <td>77.90</td>\n",
       "      <td>6.31</td>\n",
       "      <td>77.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.493300</td>\n",
       "      <td>0.375537</td>\n",
       "      <td>1.28</td>\n",
       "      <td>17.2</td>\n",
       "      <td>5.97</td>\n",
       "      <td>78.63</td>\n",
       "      <td>6.41</td>\n",
       "      <td>77.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.423800</td>\n",
       "      <td>0.368052</td>\n",
       "      <td>1.36</td>\n",
       "      <td>11.4</td>\n",
       "      <td>5.81</td>\n",
       "      <td>78.99</td>\n",
       "      <td>6.30</td>\n",
       "      <td>77.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.565800</td>\n",
       "      <td>0.362856</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.30</td>\n",
       "      <td>3.95</td>\n",
       "      <td>85.16</td>\n",
       "      <td>6.16</td>\n",
       "      <td>78.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.468300</td>\n",
       "      <td>0.363221</td>\n",
       "      <td>1.30</td>\n",
       "      <td>9.37</td>\n",
       "      <td>5.6</td>\n",
       "      <td>79.22</td>\n",
       "      <td>6.19</td>\n",
       "      <td>77.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.512200</td>\n",
       "      <td>0.366569</td>\n",
       "      <td>1.26</td>\n",
       "      <td>12.52</td>\n",
       "      <td>5.31</td>\n",
       "      <td>79.76</td>\n",
       "      <td>6.03</td>\n",
       "      <td>78.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.567200</td>\n",
       "      <td>0.364427</td>\n",
       "      <td>1.28</td>\n",
       "      <td>8.85</td>\n",
       "      <td>4.92</td>\n",
       "      <td>80.4</td>\n",
       "      <td>5.86</td>\n",
       "      <td>78.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.444900</td>\n",
       "      <td>0.360784</td>\n",
       "      <td>1.32</td>\n",
       "      <td>7.8</td>\n",
       "      <td>5.61</td>\n",
       "      <td>79.59</td>\n",
       "      <td>6.14</td>\n",
       "      <td>78.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.473100</td>\n",
       "      <td>0.362020</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.2</td>\n",
       "      <td>5.54</td>\n",
       "      <td>79.62</td>\n",
       "      <td>6.07</td>\n",
       "      <td>78.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.506700</td>\n",
       "      <td>0.361715</td>\n",
       "      <td>1.26</td>\n",
       "      <td>7.03</td>\n",
       "      <td>4.34</td>\n",
       "      <td>82.34</td>\n",
       "      <td>5.5</td>\n",
       "      <td>79.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.426100</td>\n",
       "      <td>0.361053</td>\n",
       "      <td>1.29</td>\n",
       "      <td>8.06</td>\n",
       "      <td>5.14</td>\n",
       "      <td>80.81</td>\n",
       "      <td>5.91</td>\n",
       "      <td>78.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.438700</td>\n",
       "      <td>0.362888</td>\n",
       "      <td>1.29</td>\n",
       "      <td>8.44</td>\n",
       "      <td>5.03</td>\n",
       "      <td>80.90</td>\n",
       "      <td>5.87</td>\n",
       "      <td>78.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.502900</td>\n",
       "      <td>0.361344</td>\n",
       "      <td>1.3</td>\n",
       "      <td>7.38</td>\n",
       "      <td>5.24</td>\n",
       "      <td>80.35</td>\n",
       "      <td>5.96</td>\n",
       "      <td>78.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.508100</td>\n",
       "      <td>0.360743</td>\n",
       "      <td>1.29</td>\n",
       "      <td>7.14</td>\n",
       "      <td>5.13</td>\n",
       "      <td>80.80</td>\n",
       "      <td>5.92</td>\n",
       "      <td>78.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.506600</td>\n",
       "      <td>0.360870</td>\n",
       "      <td>1.28</td>\n",
       "      <td>7.55</td>\n",
       "      <td>5.11</td>\n",
       "      <td>80.85</td>\n",
       "      <td>5.93</td>\n",
       "      <td>78.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.454700</td>\n",
       "      <td>0.360703</td>\n",
       "      <td>1.28</td>\n",
       "      <td>7.29</td>\n",
       "      <td>5.12</td>\n",
       "      <td>80.79</td>\n",
       "      <td>5.92</td>\n",
       "      <td>78.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "<ipython-input-43-52e946b60fdf>:12: RuntimeWarning: Mean of empty slice.\n",
      "  'full trade accuracy': (soft_profit[abs_trade > .9] > 0).mean() * 100,\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "<ipython-input-43-52e946b60fdf>:12: RuntimeWarning: Mean of empty slice.\n",
      "  'full trade accuracy': (soft_profit[abs_trade > .9] > 0).mean() * 100,\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1500\n",
      "Configuration saved in ./results\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2000\n",
      "Configuration saved in ./results\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2500\n",
      "Configuration saved in ./results\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-3000\n",
      "Configuration saved in ./results\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3270, training_loss=0.5385216050920866, metrics={'train_runtime': 1883.0698, 'train_samples_per_second': 1.737, 'train_steps_per_second': 1.737, 'total_flos': 0.0, 'train_loss': 0.5385216050920866, 'epoch': 1.0})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5e-4 lr\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3270\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3270\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3270' max='3270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3270/3270 31:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.742800</td>\n",
       "      <td>0.598860</td>\n",
       "      <td>1.4</td>\n",
       "      <td>53.69</td>\n",
       "      <td>0.34</td>\n",
       "      <td>52.07</td>\n",
       "      <td>58.1</td>\n",
       "      <td>52.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.059300</td>\n",
       "      <td>0.629185</td>\n",
       "      <td>1.19</td>\n",
       "      <td>35.43</td>\n",
       "      <td>0.05</td>\n",
       "      <td>50.63</td>\n",
       "      <td>19.08</td>\n",
       "      <td>53.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.618700</td>\n",
       "      <td>0.574223</td>\n",
       "      <td>1.69</td>\n",
       "      <td>58.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>55.6</td>\n",
       "      <td>75.46</td>\n",
       "      <td>51.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.724900</td>\n",
       "      <td>0.498308</td>\n",
       "      <td>1.4</td>\n",
       "      <td>30.9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.</td>\n",
       "      <td>29.13</td>\n",
       "      <td>53.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.643000</td>\n",
       "      <td>0.475523</td>\n",
       "      <td>1.06</td>\n",
       "      <td>39.59</td>\n",
       "      <td>0.14</td>\n",
       "      <td>46.61</td>\n",
       "      <td>24.92</td>\n",
       "      <td>55.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.504500</td>\n",
       "      <td>0.475192</td>\n",
       "      <td>1.70</td>\n",
       "      <td>38.56</td>\n",
       "      <td>0.06</td>\n",
       "      <td>56.23</td>\n",
       "      <td>36.72</td>\n",
       "      <td>54.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.706500</td>\n",
       "      <td>0.509966</td>\n",
       "      <td>1.87</td>\n",
       "      <td>37.81</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>42.78</td>\n",
       "      <td>52.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.584600</td>\n",
       "      <td>0.460770</td>\n",
       "      <td>1.35</td>\n",
       "      <td>21.82</td>\n",
       "      <td>0.88</td>\n",
       "      <td>50.54</td>\n",
       "      <td>17.30</td>\n",
       "      <td>56.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.572900</td>\n",
       "      <td>0.467708</td>\n",
       "      <td>1.16</td>\n",
       "      <td>24.87</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.</td>\n",
       "      <td>18.21</td>\n",
       "      <td>57.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.735000</td>\n",
       "      <td>0.469905</td>\n",
       "      <td>1.38</td>\n",
       "      <td>25.93</td>\n",
       "      <td>4.13</td>\n",
       "      <td>63.69</td>\n",
       "      <td>17.26</td>\n",
       "      <td>58.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.533800</td>\n",
       "      <td>0.464012</td>\n",
       "      <td>1.27</td>\n",
       "      <td>17.55</td>\n",
       "      <td>0.23</td>\n",
       "      <td>47.86</td>\n",
       "      <td>13.62</td>\n",
       "      <td>59.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.554200</td>\n",
       "      <td>0.478624</td>\n",
       "      <td>1.54</td>\n",
       "      <td>30.78</td>\n",
       "      <td>1.34</td>\n",
       "      <td>70.73</td>\n",
       "      <td>28.17</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.537100</td>\n",
       "      <td>0.394490</td>\n",
       "      <td>1.29</td>\n",
       "      <td>19.18</td>\n",
       "      <td>1.81</td>\n",
       "      <td>78.83</td>\n",
       "      <td>7.21</td>\n",
       "      <td>72.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.398500</td>\n",
       "      <td>0.369955</td>\n",
       "      <td>1.38</td>\n",
       "      <td>11.00</td>\n",
       "      <td>5.94</td>\n",
       "      <td>77.47</td>\n",
       "      <td>6.41</td>\n",
       "      <td>77.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.477000</td>\n",
       "      <td>0.370523</td>\n",
       "      <td>1.38</td>\n",
       "      <td>10.01</td>\n",
       "      <td>5.90</td>\n",
       "      <td>77.97</td>\n",
       "      <td>6.85</td>\n",
       "      <td>74.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.563500</td>\n",
       "      <td>0.369378</td>\n",
       "      <td>1.32</td>\n",
       "      <td>12.82</td>\n",
       "      <td>6.16</td>\n",
       "      <td>77.68</td>\n",
       "      <td>6.72</td>\n",
       "      <td>76.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.526200</td>\n",
       "      <td>0.363844</td>\n",
       "      <td>1.37</td>\n",
       "      <td>9.01</td>\n",
       "      <td>6.63</td>\n",
       "      <td>76.43</td>\n",
       "      <td>6.92</td>\n",
       "      <td>76.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.495900</td>\n",
       "      <td>0.378385</td>\n",
       "      <td>1.29</td>\n",
       "      <td>18.74</td>\n",
       "      <td>6.43</td>\n",
       "      <td>77.18</td>\n",
       "      <td>6.93</td>\n",
       "      <td>76.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.423400</td>\n",
       "      <td>0.372098</td>\n",
       "      <td>1.37</td>\n",
       "      <td>12.98</td>\n",
       "      <td>6.47</td>\n",
       "      <td>76.56</td>\n",
       "      <td>6.83</td>\n",
       "      <td>76.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.564800</td>\n",
       "      <td>0.363691</td>\n",
       "      <td>1.35</td>\n",
       "      <td>8.39</td>\n",
       "      <td>5.49</td>\n",
       "      <td>80.14</td>\n",
       "      <td>6.74</td>\n",
       "      <td>76.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.468700</td>\n",
       "      <td>0.362690</td>\n",
       "      <td>1.34</td>\n",
       "      <td>8.85</td>\n",
       "      <td>6.42</td>\n",
       "      <td>77.14</td>\n",
       "      <td>6.8</td>\n",
       "      <td>76.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.510700</td>\n",
       "      <td>0.367357</td>\n",
       "      <td>1.29</td>\n",
       "      <td>13.33</td>\n",
       "      <td>6.48</td>\n",
       "      <td>76.84</td>\n",
       "      <td>6.77</td>\n",
       "      <td>76.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.566600</td>\n",
       "      <td>0.362199</td>\n",
       "      <td>1.30</td>\n",
       "      <td>7.31</td>\n",
       "      <td>6.12</td>\n",
       "      <td>77.56</td>\n",
       "      <td>6.50</td>\n",
       "      <td>76.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.442600</td>\n",
       "      <td>0.363324</td>\n",
       "      <td>1.33</td>\n",
       "      <td>8.66</td>\n",
       "      <td>6.43</td>\n",
       "      <td>76.54</td>\n",
       "      <td>6.72</td>\n",
       "      <td>76.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.473200</td>\n",
       "      <td>0.362252</td>\n",
       "      <td>1.32</td>\n",
       "      <td>7.73</td>\n",
       "      <td>6.40</td>\n",
       "      <td>76.86</td>\n",
       "      <td>6.63</td>\n",
       "      <td>76.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.510300</td>\n",
       "      <td>0.362037</td>\n",
       "      <td>1.32</td>\n",
       "      <td>7.8</td>\n",
       "      <td>6.32</td>\n",
       "      <td>76.96</td>\n",
       "      <td>6.56</td>\n",
       "      <td>76.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.425900</td>\n",
       "      <td>0.362943</td>\n",
       "      <td>1.31</td>\n",
       "      <td>8.38</td>\n",
       "      <td>6.36</td>\n",
       "      <td>76.81</td>\n",
       "      <td>6.59</td>\n",
       "      <td>76.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.431600</td>\n",
       "      <td>0.366106</td>\n",
       "      <td>1.32</td>\n",
       "      <td>9.95</td>\n",
       "      <td>6.27</td>\n",
       "      <td>77.07</td>\n",
       "      <td>6.51</td>\n",
       "      <td>76.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.503600</td>\n",
       "      <td>0.362857</td>\n",
       "      <td>1.32</td>\n",
       "      <td>7.59</td>\n",
       "      <td>6.34</td>\n",
       "      <td>76.9</td>\n",
       "      <td>6.53</td>\n",
       "      <td>76.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.508100</td>\n",
       "      <td>0.362679</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.37</td>\n",
       "      <td>6.27</td>\n",
       "      <td>76.9</td>\n",
       "      <td>6.47</td>\n",
       "      <td>76.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.506500</td>\n",
       "      <td>0.362241</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.31</td>\n",
       "      <td>6.27</td>\n",
       "      <td>76.91</td>\n",
       "      <td>6.48</td>\n",
       "      <td>76.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.456400</td>\n",
       "      <td>0.362123</td>\n",
       "      <td>1.31</td>\n",
       "      <td>7.17</td>\n",
       "      <td>6.29</td>\n",
       "      <td>76.93</td>\n",
       "      <td>6.49</td>\n",
       "      <td>76.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-500\n",
      "Configuration saved in ./results\\checkpoint-500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "<ipython-input-43-52e946b60fdf>:12: RuntimeWarning: Mean of empty slice.\n",
      "  'full trade accuracy': (soft_profit[abs_trade > .9] > 0).mean() * 100,\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1000\n",
      "Configuration saved in ./results\\checkpoint-1000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-1500\n",
      "Configuration saved in ./results\\checkpoint-1500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2000\n",
      "Configuration saved in ./results\\checkpoint-2000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-2500\n",
      "Configuration saved in ./results\\checkpoint-2500\\config.json\n",
      "Model weights saved in ./results\\checkpoint-2500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./results\\checkpoint-3000\n",
      "Configuration saved in ./results\\checkpoint-3000\\config.json\n",
      "Model weights saved in ./results\\checkpoint-3000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 86\n",
      "  Batch size = 1\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3270, training_loss=0.5552892037487905, metrics={'train_runtime': 1886.12, 'train_samples_per_second': 1.734, 'train_steps_per_second': 1.734, 'total_flos': 0.0, 'train_loss': 0.5552892037487905, 'epoch': 1.0})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1e-3 lr\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.evaluate(eurusd['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## failed idea: have the model go through a timewise curriculum of the data\n",
    "The idea was that if the training didn't respect the timeseries nature of the data, then the model could \"memorize\" parts of the data and use that to predict past data better (which wouldn't be good at test time). Seemingly this isn't an issue as the model performs better on a validation set that does come from the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraderTrainer(Trainer):\n",
    "\n",
    "    def get_train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Returns the training :class:`~torch.utils.data.DataLoader`.\n",
    "\n",
    "        Will use no sampler if :obj:`self.train_dataset` does not implement :obj:`__len__`, a random sampler (adapted\n",
    "        to distributed training if necessary) otherwise.\n",
    "\n",
    "        Subclass and override this method if you want to inject some custom behavior.\n",
    "        \"\"\"\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "        train_sampler = self._get_train_sampler()\n",
    "\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.args.train_batch_size,\n",
    "#             shuffle=False, # TO STOP OVERFITTING\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=self.data_collator,\n",
    "            drop_last=self.args.dataloader_drop_last,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make sure unshuffled split maintains order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = Dataset.from_dict({\"input\": list(range(100))})\n",
    "split = foo.train_test_split(.1, shuffle = False)\n",
    "valid_test = split['test'].train_test_split(.5, shuffle = False)\n",
    "foo = DatasetDict({\n",
    "    'train': split['train'],\n",
    "    'validation': valid_test['train'],\n",
    "    'test': valid_test['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([90, 91, 92, 93, 94], [95, 96, 97, 98, 99])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo['validation']['input'], foo['test']['input']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quick timing check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2Trader(config).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.2 ms ± 2.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "fake_data = torch.randn(4, 391, 256)\n",
    "fake_data = fake_data.cuda()\n",
    "model(fake_data)\n",
    "cpu = fake_data.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 9 layers\n"
     ]
    }
   ],
   "source": [
    "model = GPT2Trader(config).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "748 ms ± 82.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "fake_data = torch.randn(4, 391, 256)\n",
    "model(fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
