{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import PreTrainedModel\n",
    "from trader_models import SGConvConfig, SGConvTrader\n",
    "import datasets\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds):\n",
    "    soft_profit, soft_trade = preds.predictions\n",
    "    abs_trade = np.abs(soft_trade)\n",
    "    abs_trade = abs_trade.astype('float64') # half precision will cause the sum to overflow on next line\n",
    "    trades = abs_trade.sum()\n",
    "    \n",
    "    day_profits = soft_profit.sum(axis = (1, 2))\n",
    "    \n",
    "    metrics = {\n",
    "        'day profit': day_profits.mean(),\n",
    "        'day sharpe': day_profits.mean() / day_profits.std(),\n",
    "        'trade %': trades * 100 / soft_profit.size,\n",
    "        \n",
    "        'full trade %': (abs_trade >= .7).mean() * 100,\n",
    "        'full trade accuracy': (soft_profit[abs_trade >= .7] > 0).mean() * 100,\n",
    "        'full trade g/l': soft_profit[(abs_trade >= .7) & (soft_profit > 0)].mean()\n",
    "                          / -soft_profit[(abs_trade >= .7) & (soft_profit < 0)].mean(),\n",
    "        \n",
    "        'medium trade %': ((abs_trade < .7) & (abs_trade >= .4)).mean() * 100,\n",
    "        'medium trade accuracy': (soft_profit[(abs_trade < .7) & (abs_trade >= .4)] > 0).mean() * 100,\n",
    "        'medium trade g/l': soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit > 0)].mean()\n",
    "                            / -soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit < 0)].mean(),       \n",
    "        \n",
    "        'small trade %': ((abs_trade < .4) & (abs_trade >= .2)).mean() * 100,\n",
    "        'small trade accuracy': (soft_profit[(abs_trade < .4) & (abs_trade >= .2)] > 0).mean() * 100,\n",
    "        'small trade g/l': soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit > 0)].mean()\n",
    "                            / -soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit < 0)].mean(),        \n",
    "    }\n",
    "    \n",
    "    # round the metrics\n",
    "    metrics = {k: np.format_float_positional(v, precision = 4) for k, v in metrics.items()}\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = Dataset.load_from_disk('data/fx_days')\n",
    "\n",
    "# make splits\n",
    "split = fx.train_test_split(.003, shuffle = False)\n",
    "# split = fx.train_test_split(.0015, shuffle = False)\n",
    "valid_test = split['test'].train_test_split(.3, shuffle = False)\n",
    "fx = DatasetDict({\n",
    "    'train': split['train'],\n",
    "    'validation': valid_test['train'],\n",
    "    'test': valid_test['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ohlcv', 'labels', 'future'],\n",
       "        num_rows: 35213\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ohlcv', 'labels', 'future'],\n",
       "        num_rows: 74\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ohlcv', 'labels', 'future'],\n",
       "        num_rows: 32\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./results\",\n",
    "    logging_strategy = \"steps\",\n",
    "    evaluation_strategy = \"steps\",\n",
    "    logging_steps = 200,\n",
    "    eval_steps = 200,\n",
    "    save_steps = 10000,\n",
    "    report_to = \"none\",\n",
    "    learning_rate = 2e-4,\n",
    "    weight_decay = .01,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    warmup_ratio = .05,\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size = 4,\n",
    "    per_device_eval_batch_size = 4,\n",
    "    max_grad_norm = 1,\n",
    "#     fp16 = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 13 layers\n"
     ]
    }
   ],
   "source": [
    "config = SGConvConfig(\n",
    "    n_embd = 320, n_head = 1, hidden_dropout_prob = 0\n",
    ")\n",
    "\n",
    "model = SGConvTrader(config)\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = fx['train'],\n",
    "    eval_dataset = fx['validation'],\n",
    "    compute_metrics = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interesting Experiment on loss function\n",
    "\n",
    "This model uses a particularly interesting loss function which is to maxmize mean gain / mean loss (i.e. a profitability ratio) though losses are weighted 2x. This has some desireable properties like that it is 1. volitility agnostic (doesn't favor low or high volitility) and 2. closely matches trying to create the most profit for the least risk.\n",
    "\n",
    "As we can see it is able to acheve a massive 21% trades at 54% accuracy (which is completely unseen before) very early in training. Unfortunetly it suffers from an issue where instead of trying to turn all losses in to profits, it'll end up just trying to get losses as close to 0 as possible. I hypothesize that if a loss function can have the same original properties as this one without introducing odd inductive biases, it could easily get 10% of trades above 66% when fully trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8804' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8804/8804 47:51, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.931200</td>\n",
       "      <td>1.926929</td>\n",
       "      <td>0.0520</td>\n",
       "      <td>0.1246</td>\n",
       "      <td>25.6956</td>\n",
       "      <td>2.7233</td>\n",
       "      <td>52.2015</td>\n",
       "      <td>1.0196</td>\n",
       "      <td>17.8804</td>\n",
       "      <td>50.6467</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>34.5888</td>\n",
       "      <td>49.9466</td>\n",
       "      <td>1.0133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.879100</td>\n",
       "      <td>1.890123</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>36.6114</td>\n",
       "      <td>14.7138</td>\n",
       "      <td>49.3165</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>24.4749</td>\n",
       "      <td>50.4806</td>\n",
       "      <td>0.9753</td>\n",
       "      <td>26.7429</td>\n",
       "      <td>50.943</td>\n",
       "      <td>1.0198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.730700</td>\n",
       "      <td>1.659309</td>\n",
       "      <td>0.2349</td>\n",
       "      <td>0.4443</td>\n",
       "      <td>19.8502</td>\n",
       "      <td>10.8074</td>\n",
       "      <td>53.6272</td>\n",
       "      <td>1.2617</td>\n",
       "      <td>5.6224</td>\n",
       "      <td>51.4753</td>\n",
       "      <td>1.0386</td>\n",
       "      <td>8.5016</td>\n",
       "      <td>49.8908</td>\n",
       "      <td>0.9994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.617100</td>\n",
       "      <td>1.521395</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.3548</td>\n",
       "      <td>29.7141</td>\n",
       "      <td>21.3043</td>\n",
       "      <td>54.0425</td>\n",
       "      <td>1.1447</td>\n",
       "      <td>7.1844</td>\n",
       "      <td>50.0501</td>\n",
       "      <td>0.9862</td>\n",
       "      <td>8.6662</td>\n",
       "      <td>48.6163</td>\n",
       "      <td>0.9462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.511700</td>\n",
       "      <td>1.304314</td>\n",
       "      <td>0.2701</td>\n",
       "      <td>0.3904</td>\n",
       "      <td>13.0267</td>\n",
       "      <td>7.9798</td>\n",
       "      <td>57.4919</td>\n",
       "      <td>1.1768</td>\n",
       "      <td>3.535</td>\n",
       "      <td>54.7106</td>\n",
       "      <td>0.9572</td>\n",
       "      <td>4.3138</td>\n",
       "      <td>52.5513</td>\n",
       "      <td>0.9480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.463900</td>\n",
       "      <td>1.327118</td>\n",
       "      <td>0.1898</td>\n",
       "      <td>0.3623</td>\n",
       "      <td>12.4044</td>\n",
       "      <td>7.5620</td>\n",
       "      <td>55.7713</td>\n",
       "      <td>1.1685</td>\n",
       "      <td>3.6159</td>\n",
       "      <td>50.3374</td>\n",
       "      <td>0.9847</td>\n",
       "      <td>3.8146</td>\n",
       "      <td>50.6424</td>\n",
       "      <td>0.9883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.436200</td>\n",
       "      <td>1.395933</td>\n",
       "      <td>0.2121</td>\n",
       "      <td>0.3324</td>\n",
       "      <td>16.9442</td>\n",
       "      <td>12.2026</td>\n",
       "      <td>52.8267</td>\n",
       "      <td>1.1691</td>\n",
       "      <td>3.7334</td>\n",
       "      <td>50.5348</td>\n",
       "      <td>1.0332</td>\n",
       "      <td>3.7505</td>\n",
       "      <td>51.4165</td>\n",
       "      <td>1.0371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.447000</td>\n",
       "      <td>1.452734</td>\n",
       "      <td>0.0248</td>\n",
       "      <td>0.1016</td>\n",
       "      <td>6.8664</td>\n",
       "      <td>3.6950</td>\n",
       "      <td>50.7097</td>\n",
       "      <td>0.9809</td>\n",
       "      <td>1.7609</td>\n",
       "      <td>47.3176</td>\n",
       "      <td>1.043</td>\n",
       "      <td>2.2978</td>\n",
       "      <td>50.5468</td>\n",
       "      <td>0.9641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.450800</td>\n",
       "      <td>1.370238</td>\n",
       "      <td>0.0894</td>\n",
       "      <td>0.4092</td>\n",
       "      <td>7.3327</td>\n",
       "      <td>3.8766</td>\n",
       "      <td>55.9229</td>\n",
       "      <td>1.2147</td>\n",
       "      <td>2.2341</td>\n",
       "      <td>50.0467</td>\n",
       "      <td>0.9963</td>\n",
       "      <td>2.3319</td>\n",
       "      <td>47.7419</td>\n",
       "      <td>0.9980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.445800</td>\n",
       "      <td>1.336127</td>\n",
       "      <td>0.0579</td>\n",
       "      <td>0.2298</td>\n",
       "      <td>8.4095</td>\n",
       "      <td>5.1978</td>\n",
       "      <td>52.2819</td>\n",
       "      <td>1.0553</td>\n",
       "      <td>2.3649</td>\n",
       "      <td>48.5185</td>\n",
       "      <td>1.0457</td>\n",
       "      <td>2.4887</td>\n",
       "      <td>49.7779</td>\n",
       "      <td>1.0324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.415200</td>\n",
       "      <td>1.352638</td>\n",
       "      <td>0.0726</td>\n",
       "      <td>0.3712</td>\n",
       "      <td>5.816</td>\n",
       "      <td>2.5357</td>\n",
       "      <td>55.5720</td>\n",
       "      <td>1.0658</td>\n",
       "      <td>1.6337</td>\n",
       "      <td>52.2466</td>\n",
       "      <td>1.1118</td>\n",
       "      <td>2.1843</td>\n",
       "      <td>51.4082</td>\n",
       "      <td>0.9955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.342700</td>\n",
       "      <td>1.200780</td>\n",
       "      <td>0.0925</td>\n",
       "      <td>0.3331</td>\n",
       "      <td>4.8855</td>\n",
       "      <td>2.7568</td>\n",
       "      <td>54.4763</td>\n",
       "      <td>1.1976</td>\n",
       "      <td>1.1767</td>\n",
       "      <td>54.5680</td>\n",
       "      <td>1.1414</td>\n",
       "      <td>1.3916</td>\n",
       "      <td>54.788</td>\n",
       "      <td>1.0179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.370000</td>\n",
       "      <td>1.219476</td>\n",
       "      <td>0.0688</td>\n",
       "      <td>0.3968</td>\n",
       "      <td>5.9247</td>\n",
       "      <td>3.6351</td>\n",
       "      <td>55.1948</td>\n",
       "      <td>1.1295</td>\n",
       "      <td>1.5216</td>\n",
       "      <td>48.5644</td>\n",
       "      <td>1.1125</td>\n",
       "      <td>1.5376</td>\n",
       "      <td>48.0334</td>\n",
       "      <td>1.0249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.237800</td>\n",
       "      <td>1.159579</td>\n",
       "      <td>0.0738</td>\n",
       "      <td>0.4001</td>\n",
       "      <td>4.8047</td>\n",
       "      <td>2.8940</td>\n",
       "      <td>55.9935</td>\n",
       "      <td>1.1022</td>\n",
       "      <td>1.3665</td>\n",
       "      <td>51.6826</td>\n",
       "      <td>1.1025</td>\n",
       "      <td>1.4443</td>\n",
       "      <td>52.1118</td>\n",
       "      <td>1.1061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.297900</td>\n",
       "      <td>1.139385</td>\n",
       "      <td>0.0991</td>\n",
       "      <td>0.3230</td>\n",
       "      <td>6.5791</td>\n",
       "      <td>4.3529</td>\n",
       "      <td>55.2963</td>\n",
       "      <td>1.0592</td>\n",
       "      <td>1.62</td>\n",
       "      <td>52.3429</td>\n",
       "      <td>1.0479</td>\n",
       "      <td>1.5794</td>\n",
       "      <td>52.3866</td>\n",
       "      <td>1.0423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.287200</td>\n",
       "      <td>1.335727</td>\n",
       "      <td>0.0709</td>\n",
       "      <td>0.3233</td>\n",
       "      <td>4.0388</td>\n",
       "      <td>2.3406</td>\n",
       "      <td>56.6312</td>\n",
       "      <td>1.1037</td>\n",
       "      <td>1.0197</td>\n",
       "      <td>53.9830</td>\n",
       "      <td>1.0887</td>\n",
       "      <td>1.0283</td>\n",
       "      <td>51.4703</td>\n",
       "      <td>1.0314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.249300</td>\n",
       "      <td>1.235851</td>\n",
       "      <td>0.0725</td>\n",
       "      <td>0.3103</td>\n",
       "      <td>4.0439</td>\n",
       "      <td>2.4652</td>\n",
       "      <td>55.2026</td>\n",
       "      <td>1.0884</td>\n",
       "      <td>0.9247</td>\n",
       "      <td>52.5936</td>\n",
       "      <td>1.1116</td>\n",
       "      <td>1.0079</td>\n",
       "      <td>53.8175</td>\n",
       "      <td>0.9785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.220500</td>\n",
       "      <td>1.096093</td>\n",
       "      <td>0.0936</td>\n",
       "      <td>0.3425</td>\n",
       "      <td>3.7708</td>\n",
       "      <td>2.2582</td>\n",
       "      <td>57.7735</td>\n",
       "      <td>1.1058</td>\n",
       "      <td>1.0151</td>\n",
       "      <td>56.1068</td>\n",
       "      <td>1.0563</td>\n",
       "      <td>1.0514</td>\n",
       "      <td>54.1704</td>\n",
       "      <td>0.9703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.166800</td>\n",
       "      <td>1.136321</td>\n",
       "      <td>0.0804</td>\n",
       "      <td>0.3249</td>\n",
       "      <td>3.1727</td>\n",
       "      <td>2.0013</td>\n",
       "      <td>57.0989</td>\n",
       "      <td>1.1708</td>\n",
       "      <td>0.6951</td>\n",
       "      <td>55.2505</td>\n",
       "      <td>1.0191</td>\n",
       "      <td>0.7111</td>\n",
       "      <td>55.6891</td>\n",
       "      <td>1.0215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.194200</td>\n",
       "      <td>1.113035</td>\n",
       "      <td>0.0708</td>\n",
       "      <td>0.3173</td>\n",
       "      <td>3.4846</td>\n",
       "      <td>2.1195</td>\n",
       "      <td>56.4176</td>\n",
       "      <td>1.0901</td>\n",
       "      <td>0.7662</td>\n",
       "      <td>55.0898</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.8428</td>\n",
       "      <td>54.3115</td>\n",
       "      <td>1.0202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.170000</td>\n",
       "      <td>1.076820</td>\n",
       "      <td>0.0798</td>\n",
       "      <td>0.2794</td>\n",
       "      <td>3.2451</td>\n",
       "      <td>2.0127</td>\n",
       "      <td>56.872</td>\n",
       "      <td>1.156</td>\n",
       "      <td>0.7443</td>\n",
       "      <td>56.2202</td>\n",
       "      <td>1.0957</td>\n",
       "      <td>0.8525</td>\n",
       "      <td>54.4521</td>\n",
       "      <td>1.0148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.227700</td>\n",
       "      <td>1.076502</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.3515</td>\n",
       "      <td>3.9661</td>\n",
       "      <td>2.7987</td>\n",
       "      <td>56.1827</td>\n",
       "      <td>1.0537</td>\n",
       "      <td>0.8282</td>\n",
       "      <td>54.6015</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>0.9004</td>\n",
       "      <td>55.3098</td>\n",
       "      <td>0.9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.157500</td>\n",
       "      <td>1.086418</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.3043</td>\n",
       "      <td>3.0815</td>\n",
       "      <td>1.9904</td>\n",
       "      <td>57.0905</td>\n",
       "      <td>1.102</td>\n",
       "      <td>0.7321</td>\n",
       "      <td>56.2740</td>\n",
       "      <td>1.0260</td>\n",
       "      <td>0.7740</td>\n",
       "      <td>57.2006</td>\n",
       "      <td>0.9484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.129900</td>\n",
       "      <td>1.175059</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.31</td>\n",
       "      <td>3.8998</td>\n",
       "      <td>2.6406</td>\n",
       "      <td>55.9193</td>\n",
       "      <td>1.1300</td>\n",
       "      <td>0.8350</td>\n",
       "      <td>58.0295</td>\n",
       "      <td>1.0157</td>\n",
       "      <td>0.939</td>\n",
       "      <td>56.5464</td>\n",
       "      <td>0.9695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.094400</td>\n",
       "      <td>1.056204</td>\n",
       "      <td>0.0984</td>\n",
       "      <td>0.3315</td>\n",
       "      <td>4.3094</td>\n",
       "      <td>3.0239</td>\n",
       "      <td>55.4138</td>\n",
       "      <td>1.1768</td>\n",
       "      <td>0.9064</td>\n",
       "      <td>56.8733</td>\n",
       "      <td>0.9636</td>\n",
       "      <td>0.9874</td>\n",
       "      <td>55.4699</td>\n",
       "      <td>0.9969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.147700</td>\n",
       "      <td>1.082088</td>\n",
       "      <td>0.0722</td>\n",
       "      <td>0.3364</td>\n",
       "      <td>3.1499</td>\n",
       "      <td>1.9221</td>\n",
       "      <td>55.1915</td>\n",
       "      <td>1.0766</td>\n",
       "      <td>0.8430</td>\n",
       "      <td>58.2560</td>\n",
       "      <td>1.0225</td>\n",
       "      <td>0.8714</td>\n",
       "      <td>56.4437</td>\n",
       "      <td>0.9687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.074200</td>\n",
       "      <td>0.979969</td>\n",
       "      <td>0.0764</td>\n",
       "      <td>0.3159</td>\n",
       "      <td>3.2872</td>\n",
       "      <td>2.0487</td>\n",
       "      <td>57.0236</td>\n",
       "      <td>1.0799</td>\n",
       "      <td>0.9006</td>\n",
       "      <td>54.4749</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.8945</td>\n",
       "      <td>56.7199</td>\n",
       "      <td>0.9888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.070200</td>\n",
       "      <td>0.955285</td>\n",
       "      <td>0.0774</td>\n",
       "      <td>0.3083</td>\n",
       "      <td>3.3315</td>\n",
       "      <td>2.2965</td>\n",
       "      <td>54.7857</td>\n",
       "      <td>1.1672</td>\n",
       "      <td>0.7380</td>\n",
       "      <td>56.3436</td>\n",
       "      <td>1.1084</td>\n",
       "      <td>0.8541</td>\n",
       "      <td>57.3434</td>\n",
       "      <td>1.0174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.075100</td>\n",
       "      <td>0.947143</td>\n",
       "      <td>0.0809</td>\n",
       "      <td>0.3396</td>\n",
       "      <td>3.6636</td>\n",
       "      <td>2.4477</td>\n",
       "      <td>56.1941</td>\n",
       "      <td>1.1201</td>\n",
       "      <td>0.9814</td>\n",
       "      <td>55.4292</td>\n",
       "      <td>1.0696</td>\n",
       "      <td>1.0617</td>\n",
       "      <td>55.3133</td>\n",
       "      <td>0.9994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.041200</td>\n",
       "      <td>1.000587</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.3571</td>\n",
       "      <td>3.0008</td>\n",
       "      <td>2.0660</td>\n",
       "      <td>57.6360</td>\n",
       "      <td>1.0938</td>\n",
       "      <td>0.7883</td>\n",
       "      <td>53.0026</td>\n",
       "      <td>1.1141</td>\n",
       "      <td>0.7012</td>\n",
       "      <td>54.2156</td>\n",
       "      <td>1.0739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.095000</td>\n",
       "      <td>0.893193</td>\n",
       "      <td>0.0716</td>\n",
       "      <td>0.355</td>\n",
       "      <td>3.2742</td>\n",
       "      <td>2.2932</td>\n",
       "      <td>56.0769</td>\n",
       "      <td>1.0988</td>\n",
       "      <td>0.8670</td>\n",
       "      <td>52.4354</td>\n",
       "      <td>1.1718</td>\n",
       "      <td>0.8421</td>\n",
       "      <td>55.6835</td>\n",
       "      <td>1.0666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.010900</td>\n",
       "      <td>0.930325</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.3261</td>\n",
       "      <td>3.2839</td>\n",
       "      <td>2.2764</td>\n",
       "      <td>56.5546</td>\n",
       "      <td>1.1126</td>\n",
       "      <td>0.8837</td>\n",
       "      <td>52.5074</td>\n",
       "      <td>1.1339</td>\n",
       "      <td>0.8546</td>\n",
       "      <td>54.9414</td>\n",
       "      <td>1.0495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.079000</td>\n",
       "      <td>0.931358</td>\n",
       "      <td>0.0807</td>\n",
       "      <td>0.333</td>\n",
       "      <td>3.4121</td>\n",
       "      <td>2.3717</td>\n",
       "      <td>56.5110</td>\n",
       "      <td>1.1328</td>\n",
       "      <td>0.8591</td>\n",
       "      <td>52.737</td>\n",
       "      <td>1.1249</td>\n",
       "      <td>0.8033</td>\n",
       "      <td>52.9725</td>\n",
       "      <td>1.0394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.989200</td>\n",
       "      <td>0.898873</td>\n",
       "      <td>0.0682</td>\n",
       "      <td>0.3231</td>\n",
       "      <td>2.8162</td>\n",
       "      <td>1.9032</td>\n",
       "      <td>57.1554</td>\n",
       "      <td>1.0647</td>\n",
       "      <td>0.8186</td>\n",
       "      <td>55.1904</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.7513</td>\n",
       "      <td>54.2956</td>\n",
       "      <td>1.0308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.986300</td>\n",
       "      <td>0.903288</td>\n",
       "      <td>0.0603</td>\n",
       "      <td>0.3167</td>\n",
       "      <td>2.5086</td>\n",
       "      <td>1.7328</td>\n",
       "      <td>57.1489</td>\n",
       "      <td>1.0813</td>\n",
       "      <td>0.6720</td>\n",
       "      <td>54.9108</td>\n",
       "      <td>1.1370</td>\n",
       "      <td>0.6627</td>\n",
       "      <td>54.1693</td>\n",
       "      <td>1.0707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.982000</td>\n",
       "      <td>0.894768</td>\n",
       "      <td>0.0637</td>\n",
       "      <td>0.3170</td>\n",
       "      <td>2.5486</td>\n",
       "      <td>1.7658</td>\n",
       "      <td>56.9472</td>\n",
       "      <td>1.0951</td>\n",
       "      <td>0.7056</td>\n",
       "      <td>55.7707</td>\n",
       "      <td>1.0969</td>\n",
       "      <td>0.6858</td>\n",
       "      <td>54.3713</td>\n",
       "      <td>1.0484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.932700</td>\n",
       "      <td>0.892433</td>\n",
       "      <td>0.0536</td>\n",
       "      <td>0.2837</td>\n",
       "      <td>2.2373</td>\n",
       "      <td>1.5210</td>\n",
       "      <td>57.5101</td>\n",
       "      <td>1.073</td>\n",
       "      <td>0.6418</td>\n",
       "      <td>54.801</td>\n",
       "      <td>1.0831</td>\n",
       "      <td>0.6413</td>\n",
       "      <td>53.4634</td>\n",
       "      <td>1.0809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.981000</td>\n",
       "      <td>0.851276</td>\n",
       "      <td>0.0601</td>\n",
       "      <td>0.2844</td>\n",
       "      <td>2.3614</td>\n",
       "      <td>1.5924</td>\n",
       "      <td>57.2289</td>\n",
       "      <td>1.0824</td>\n",
       "      <td>0.6650</td>\n",
       "      <td>55.4249</td>\n",
       "      <td>1.1133</td>\n",
       "      <td>0.6884</td>\n",
       "      <td>54.6501</td>\n",
       "      <td>1.0413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.028200</td>\n",
       "      <td>0.982832</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>1.7968</td>\n",
       "      <td>1.1758</td>\n",
       "      <td>56.4296</td>\n",
       "      <td>1.1047</td>\n",
       "      <td>0.5027</td>\n",
       "      <td>58.2452</td>\n",
       "      <td>1.0186</td>\n",
       "      <td>0.5571</td>\n",
       "      <td>55.8301</td>\n",
       "      <td>1.0133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.900300</td>\n",
       "      <td>0.892286</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>0.2670</td>\n",
       "      <td>1.9299</td>\n",
       "      <td>1.2815</td>\n",
       "      <td>56.7046</td>\n",
       "      <td>1.0938</td>\n",
       "      <td>0.5426</td>\n",
       "      <td>57.3021</td>\n",
       "      <td>1.0917</td>\n",
       "      <td>0.5889</td>\n",
       "      <td>56.0375</td>\n",
       "      <td>0.9904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.914800</td>\n",
       "      <td>0.902403</td>\n",
       "      <td>0.0468</td>\n",
       "      <td>0.2644</td>\n",
       "      <td>1.9057</td>\n",
       "      <td>1.2647</td>\n",
       "      <td>56.7400</td>\n",
       "      <td>1.0893</td>\n",
       "      <td>0.5367</td>\n",
       "      <td>57.4898</td>\n",
       "      <td>1.0561</td>\n",
       "      <td>0.5908</td>\n",
       "      <td>55.6654</td>\n",
       "      <td>1.0102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.961200</td>\n",
       "      <td>0.894621</td>\n",
       "      <td>0.0469</td>\n",
       "      <td>0.2636</td>\n",
       "      <td>1.9044</td>\n",
       "      <td>1.2625</td>\n",
       "      <td>56.8302</td>\n",
       "      <td>1.0900</td>\n",
       "      <td>0.5394</td>\n",
       "      <td>57.3748</td>\n",
       "      <td>1.0438</td>\n",
       "      <td>0.5914</td>\n",
       "      <td>55.2362</td>\n",
       "      <td>1.0216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.924400</td>\n",
       "      <td>0.878306</td>\n",
       "      <td>0.0475</td>\n",
       "      <td>0.2660</td>\n",
       "      <td>1.9368</td>\n",
       "      <td>1.2858</td>\n",
       "      <td>56.8080</td>\n",
       "      <td>1.0865</td>\n",
       "      <td>0.5535</td>\n",
       "      <td>57.2909</td>\n",
       "      <td>1.0606</td>\n",
       "      <td>0.5996</td>\n",
       "      <td>55.3565</td>\n",
       "      <td>1.0139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.850300</td>\n",
       "      <td>0.882431</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.2661</td>\n",
       "      <td>1.9405</td>\n",
       "      <td>1.2904</td>\n",
       "      <td>56.8162</td>\n",
       "      <td>1.0861</td>\n",
       "      <td>0.5553</td>\n",
       "      <td>57.2099</td>\n",
       "      <td>1.0647</td>\n",
       "      <td>0.5999</td>\n",
       "      <td>55.4493</td>\n",
       "      <td>1.0105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8804, training_loss=1.2161343785536824, metrics={'train_runtime': 2875.508, 'train_samples_per_second': 12.246, 'train_steps_per_second': 3.062, 'total_flos': 0.0, 'train_loss': 1.2161343785536824, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 2e-4, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# raw standardized trade loss, losses 2x (summed per day per timeframe)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8804' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8804/8804 56:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.968200</td>\n",
       "      <td>0.947759</td>\n",
       "      <td>0.0501</td>\n",
       "      <td>0.1156</td>\n",
       "      <td>26.0528</td>\n",
       "      <td>4.3777</td>\n",
       "      <td>49.6784</td>\n",
       "      <td>1.053</td>\n",
       "      <td>16.7949</td>\n",
       "      <td>50.0875</td>\n",
       "      <td>1.0033</td>\n",
       "      <td>32.2314</td>\n",
       "      <td>49.3976</td>\n",
       "      <td>1.0166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.934200</td>\n",
       "      <td>0.952670</td>\n",
       "      <td>0.0651</td>\n",
       "      <td>0.0901</td>\n",
       "      <td>36.5554</td>\n",
       "      <td>15.2807</td>\n",
       "      <td>48.8911</td>\n",
       "      <td>0.9586</td>\n",
       "      <td>23.9898</td>\n",
       "      <td>50.2851</td>\n",
       "      <td>0.9727</td>\n",
       "      <td>25.6584</td>\n",
       "      <td>51.1110</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.864400</td>\n",
       "      <td>0.819264</td>\n",
       "      <td>0.2321</td>\n",
       "      <td>0.4685</td>\n",
       "      <td>18.7678</td>\n",
       "      <td>10.0807</td>\n",
       "      <td>53.6958</td>\n",
       "      <td>1.2813</td>\n",
       "      <td>5.1913</td>\n",
       "      <td>52.5418</td>\n",
       "      <td>1.0114</td>\n",
       "      <td>7.7154</td>\n",
       "      <td>50.3473</td>\n",
       "      <td>0.9773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.801500</td>\n",
       "      <td>0.721963</td>\n",
       "      <td>0.2942</td>\n",
       "      <td>0.3882</td>\n",
       "      <td>21.9927</td>\n",
       "      <td>13.7199</td>\n",
       "      <td>55.043</td>\n",
       "      <td>1.1637</td>\n",
       "      <td>5.8127</td>\n",
       "      <td>52.6208</td>\n",
       "      <td>1.0421</td>\n",
       "      <td>8.3941</td>\n",
       "      <td>48.9236</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.756800</td>\n",
       "      <td>0.654370</td>\n",
       "      <td>0.3002</td>\n",
       "      <td>0.381</td>\n",
       "      <td>15.4652</td>\n",
       "      <td>10.2951</td>\n",
       "      <td>56.5114</td>\n",
       "      <td>1.1758</td>\n",
       "      <td>3.6271</td>\n",
       "      <td>52.6779</td>\n",
       "      <td>0.9861</td>\n",
       "      <td>4.8654</td>\n",
       "      <td>52.1935</td>\n",
       "      <td>0.9190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.730900</td>\n",
       "      <td>0.681203</td>\n",
       "      <td>0.1976</td>\n",
       "      <td>0.3889</td>\n",
       "      <td>13.0798</td>\n",
       "      <td>7.2069</td>\n",
       "      <td>56.0282</td>\n",
       "      <td>1.2104</td>\n",
       "      <td>3.8606</td>\n",
       "      <td>50.8008</td>\n",
       "      <td>1.04</td>\n",
       "      <td>4.702</td>\n",
       "      <td>51.0600</td>\n",
       "      <td>0.9603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.723200</td>\n",
       "      <td>0.646497</td>\n",
       "      <td>0.1886</td>\n",
       "      <td>0.3773</td>\n",
       "      <td>12.8167</td>\n",
       "      <td>8.6397</td>\n",
       "      <td>54.2265</td>\n",
       "      <td>1.2455</td>\n",
       "      <td>3.1383</td>\n",
       "      <td>49.4418</td>\n",
       "      <td>0.9981</td>\n",
       "      <td>3.6688</td>\n",
       "      <td>51.0473</td>\n",
       "      <td>0.9778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.723800</td>\n",
       "      <td>0.827965</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>7.4842</td>\n",
       "      <td>4.1659</td>\n",
       "      <td>48.9525</td>\n",
       "      <td>0.9522</td>\n",
       "      <td>1.569</td>\n",
       "      <td>47.1988</td>\n",
       "      <td>0.8918</td>\n",
       "      <td>1.9683</td>\n",
       "      <td>50.6754</td>\n",
       "      <td>0.9726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.744000</td>\n",
       "      <td>0.756372</td>\n",
       "      <td>0.0556</td>\n",
       "      <td>0.3281</td>\n",
       "      <td>4.6308</td>\n",
       "      <td>1.8173</td>\n",
       "      <td>55.5454</td>\n",
       "      <td>1.1778</td>\n",
       "      <td>1.0443</td>\n",
       "      <td>52.5112</td>\n",
       "      <td>1.0336</td>\n",
       "      <td>1.7374</td>\n",
       "      <td>50.8822</td>\n",
       "      <td>1.0006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.729500</td>\n",
       "      <td>0.665394</td>\n",
       "      <td>0.0686</td>\n",
       "      <td>0.2741</td>\n",
       "      <td>6.6985</td>\n",
       "      <td>3.5642</td>\n",
       "      <td>53.2385</td>\n",
       "      <td>1.1463</td>\n",
       "      <td>2.1139</td>\n",
       "      <td>48.9913</td>\n",
       "      <td>1.0414</td>\n",
       "      <td>2.304</td>\n",
       "      <td>50.8056</td>\n",
       "      <td>1.0239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.707700</td>\n",
       "      <td>0.678212</td>\n",
       "      <td>0.0697</td>\n",
       "      <td>0.3106</td>\n",
       "      <td>5.3774</td>\n",
       "      <td>2.6025</td>\n",
       "      <td>53.7001</td>\n",
       "      <td>1.0997</td>\n",
       "      <td>1.2271</td>\n",
       "      <td>55.1326</td>\n",
       "      <td>1.1317</td>\n",
       "      <td>1.7560</td>\n",
       "      <td>53.2035</td>\n",
       "      <td>1.0070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.679200</td>\n",
       "      <td>0.627939</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.3094</td>\n",
       "      <td>4.6297</td>\n",
       "      <td>2.5400</td>\n",
       "      <td>52.9885</td>\n",
       "      <td>1.1971</td>\n",
       "      <td>0.9393</td>\n",
       "      <td>54.8401</td>\n",
       "      <td>1.1403</td>\n",
       "      <td>1.2888</td>\n",
       "      <td>56.2945</td>\n",
       "      <td>0.9816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.675300</td>\n",
       "      <td>0.607567</td>\n",
       "      <td>0.0622</td>\n",
       "      <td>0.3417</td>\n",
       "      <td>4.5995</td>\n",
       "      <td>2.7082</td>\n",
       "      <td>56.1352</td>\n",
       "      <td>1.0537</td>\n",
       "      <td>1.2448</td>\n",
       "      <td>52.1528</td>\n",
       "      <td>1.0809</td>\n",
       "      <td>1.3754</td>\n",
       "      <td>51.1333</td>\n",
       "      <td>1.0616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.622500</td>\n",
       "      <td>0.565472</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.3464</td>\n",
       "      <td>6.2793</td>\n",
       "      <td>4.0828</td>\n",
       "      <td>54.4974</td>\n",
       "      <td>1.1306</td>\n",
       "      <td>1.7426</td>\n",
       "      <td>50.4548</td>\n",
       "      <td>1.1043</td>\n",
       "      <td>1.7087</td>\n",
       "      <td>49.4111</td>\n",
       "      <td>1.0618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.655900</td>\n",
       "      <td>0.603646</td>\n",
       "      <td>0.0973</td>\n",
       "      <td>0.2933</td>\n",
       "      <td>5.9108</td>\n",
       "      <td>3.3816</td>\n",
       "      <td>55.6967</td>\n",
       "      <td>1.0515</td>\n",
       "      <td>1.4303</td>\n",
       "      <td>55.5442</td>\n",
       "      <td>1.0866</td>\n",
       "      <td>1.7302</td>\n",
       "      <td>51.7266</td>\n",
       "      <td>0.9928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.656900</td>\n",
       "      <td>0.633585</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.3150</td>\n",
       "      <td>4.372</td>\n",
       "      <td>2.5774</td>\n",
       "      <td>56.2100</td>\n",
       "      <td>1.1339</td>\n",
       "      <td>1.1275</td>\n",
       "      <td>52.7328</td>\n",
       "      <td>1.1259</td>\n",
       "      <td>1.2584</td>\n",
       "      <td>52.1833</td>\n",
       "      <td>1.0296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.627100</td>\n",
       "      <td>0.607446</td>\n",
       "      <td>0.0747</td>\n",
       "      <td>0.2734</td>\n",
       "      <td>4.5424</td>\n",
       "      <td>2.7826</td>\n",
       "      <td>53.8072</td>\n",
       "      <td>1.0840</td>\n",
       "      <td>0.9902</td>\n",
       "      <td>54.8126</td>\n",
       "      <td>1.0447</td>\n",
       "      <td>1.1918</td>\n",
       "      <td>55.5993</td>\n",
       "      <td>0.9506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.608300</td>\n",
       "      <td>0.579915</td>\n",
       "      <td>0.0838</td>\n",
       "      <td>0.3113</td>\n",
       "      <td>3.62</td>\n",
       "      <td>2.1576</td>\n",
       "      <td>56.4614</td>\n",
       "      <td>1.1088</td>\n",
       "      <td>0.9713</td>\n",
       "      <td>56.715</td>\n",
       "      <td>1.0486</td>\n",
       "      <td>1.0272</td>\n",
       "      <td>54.898</td>\n",
       "      <td>1.0041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.588800</td>\n",
       "      <td>0.588311</td>\n",
       "      <td>0.0803</td>\n",
       "      <td>0.3142</td>\n",
       "      <td>3.1463</td>\n",
       "      <td>1.7511</td>\n",
       "      <td>57.7706</td>\n",
       "      <td>1.1495</td>\n",
       "      <td>0.7628</td>\n",
       "      <td>56.1372</td>\n",
       "      <td>1.0006</td>\n",
       "      <td>0.8007</td>\n",
       "      <td>56.2183</td>\n",
       "      <td>1.0095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.599900</td>\n",
       "      <td>0.556559</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.2907</td>\n",
       "      <td>3.8359</td>\n",
       "      <td>2.2237</td>\n",
       "      <td>56.9915</td>\n",
       "      <td>1.1079</td>\n",
       "      <td>1.0085</td>\n",
       "      <td>53.4222</td>\n",
       "      <td>1.0322</td>\n",
       "      <td>1.0767</td>\n",
       "      <td>53.5348</td>\n",
       "      <td>1.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.592200</td>\n",
       "      <td>0.541420</td>\n",
       "      <td>0.0830</td>\n",
       "      <td>0.2834</td>\n",
       "      <td>3.8011</td>\n",
       "      <td>2.1063</td>\n",
       "      <td>56.1436</td>\n",
       "      <td>1.1554</td>\n",
       "      <td>1.0582</td>\n",
       "      <td>56.981</td>\n",
       "      <td>1.0207</td>\n",
       "      <td>1.2236</td>\n",
       "      <td>55.3813</td>\n",
       "      <td>0.9508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.610400</td>\n",
       "      <td>0.532325</td>\n",
       "      <td>0.0663</td>\n",
       "      <td>0.3068</td>\n",
       "      <td>3.5736</td>\n",
       "      <td>2.3195</td>\n",
       "      <td>54.9876</td>\n",
       "      <td>1.0433</td>\n",
       "      <td>0.9405</td>\n",
       "      <td>57.3171</td>\n",
       "      <td>1.0045</td>\n",
       "      <td>0.9180</td>\n",
       "      <td>56.2812</td>\n",
       "      <td>0.9738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.578800</td>\n",
       "      <td>0.553581</td>\n",
       "      <td>0.0702</td>\n",
       "      <td>0.2658</td>\n",
       "      <td>3.4486</td>\n",
       "      <td>2.2094</td>\n",
       "      <td>55.93</td>\n",
       "      <td>1.0674</td>\n",
       "      <td>0.7973</td>\n",
       "      <td>57.5726</td>\n",
       "      <td>1.0071</td>\n",
       "      <td>0.9162</td>\n",
       "      <td>56.6974</td>\n",
       "      <td>0.9546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.557400</td>\n",
       "      <td>0.591628</td>\n",
       "      <td>0.0703</td>\n",
       "      <td>0.3106</td>\n",
       "      <td>3.0741</td>\n",
       "      <td>1.9326</td>\n",
       "      <td>56.712</td>\n",
       "      <td>1.041</td>\n",
       "      <td>0.7534</td>\n",
       "      <td>54.7682</td>\n",
       "      <td>1.1109</td>\n",
       "      <td>0.8380</td>\n",
       "      <td>57.0238</td>\n",
       "      <td>1.0449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.545800</td>\n",
       "      <td>0.530392</td>\n",
       "      <td>0.1008</td>\n",
       "      <td>0.3064</td>\n",
       "      <td>4.1968</td>\n",
       "      <td>2.8633</td>\n",
       "      <td>56.3146</td>\n",
       "      <td>1.1509</td>\n",
       "      <td>0.978</td>\n",
       "      <td>55.6456</td>\n",
       "      <td>1.039</td>\n",
       "      <td>1.0589</td>\n",
       "      <td>54.4756</td>\n",
       "      <td>0.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.560800</td>\n",
       "      <td>0.520814</td>\n",
       "      <td>0.0653</td>\n",
       "      <td>0.2778</td>\n",
       "      <td>3.4296</td>\n",
       "      <td>2.275</td>\n",
       "      <td>55.908</td>\n",
       "      <td>1.0215</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>55.9948</td>\n",
       "      <td>0.9951</td>\n",
       "      <td>0.8698</td>\n",
       "      <td>55.4663</td>\n",
       "      <td>0.9988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.538100</td>\n",
       "      <td>0.495379</td>\n",
       "      <td>0.0754</td>\n",
       "      <td>0.3047</td>\n",
       "      <td>3.6481</td>\n",
       "      <td>2.3696</td>\n",
       "      <td>56.1892</td>\n",
       "      <td>1.0529</td>\n",
       "      <td>0.9665</td>\n",
       "      <td>55.8097</td>\n",
       "      <td>1.035</td>\n",
       "      <td>0.9628</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>0.9509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.530500</td>\n",
       "      <td>0.469783</td>\n",
       "      <td>0.0724</td>\n",
       "      <td>0.2811</td>\n",
       "      <td>3.6804</td>\n",
       "      <td>2.4617</td>\n",
       "      <td>55.3602</td>\n",
       "      <td>1.0971</td>\n",
       "      <td>0.9133</td>\n",
       "      <td>55.0748</td>\n",
       "      <td>1.1412</td>\n",
       "      <td>1.0575</td>\n",
       "      <td>53.3228</td>\n",
       "      <td>0.9826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.546800</td>\n",
       "      <td>0.449411</td>\n",
       "      <td>0.0674</td>\n",
       "      <td>0.2789</td>\n",
       "      <td>3.4277</td>\n",
       "      <td>2.1822</td>\n",
       "      <td>56.9859</td>\n",
       "      <td>1.0444</td>\n",
       "      <td>1.0037</td>\n",
       "      <td>54.8722</td>\n",
       "      <td>1.0780</td>\n",
       "      <td>1.0707</td>\n",
       "      <td>54.6455</td>\n",
       "      <td>0.9439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.517000</td>\n",
       "      <td>0.535509</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.2865</td>\n",
       "      <td>2.6469</td>\n",
       "      <td>1.8228</td>\n",
       "      <td>57.3651</td>\n",
       "      <td>1.0661</td>\n",
       "      <td>0.6325</td>\n",
       "      <td>56.5447</td>\n",
       "      <td>1.0476</td>\n",
       "      <td>0.6376</td>\n",
       "      <td>56.5658</td>\n",
       "      <td>0.9626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>0.493034</td>\n",
       "      <td>0.0605</td>\n",
       "      <td>0.2681</td>\n",
       "      <td>2.6747</td>\n",
       "      <td>1.8747</td>\n",
       "      <td>56.0598</td>\n",
       "      <td>1.0851</td>\n",
       "      <td>0.6001</td>\n",
       "      <td>55.2911</td>\n",
       "      <td>1.1870</td>\n",
       "      <td>0.68</td>\n",
       "      <td>55.651</td>\n",
       "      <td>0.9768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.504400</td>\n",
       "      <td>0.503415</td>\n",
       "      <td>0.0672</td>\n",
       "      <td>0.2608</td>\n",
       "      <td>2.9545</td>\n",
       "      <td>2.0098</td>\n",
       "      <td>56.1556</td>\n",
       "      <td>1.0800</td>\n",
       "      <td>0.7494</td>\n",
       "      <td>53.0541</td>\n",
       "      <td>1.1035</td>\n",
       "      <td>0.8038</td>\n",
       "      <td>56.1811</td>\n",
       "      <td>0.9901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.533900</td>\n",
       "      <td>0.487482</td>\n",
       "      <td>0.0616</td>\n",
       "      <td>0.2724</td>\n",
       "      <td>2.7517</td>\n",
       "      <td>1.8279</td>\n",
       "      <td>56.6572</td>\n",
       "      <td>1.0539</td>\n",
       "      <td>0.6804</td>\n",
       "      <td>54.9119</td>\n",
       "      <td>1.1513</td>\n",
       "      <td>0.7430</td>\n",
       "      <td>55.4308</td>\n",
       "      <td>1.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.491000</td>\n",
       "      <td>0.432375</td>\n",
       "      <td>0.0635</td>\n",
       "      <td>0.2799</td>\n",
       "      <td>2.6765</td>\n",
       "      <td>1.7535</td>\n",
       "      <td>57.5727</td>\n",
       "      <td>1.0452</td>\n",
       "      <td>0.7223</td>\n",
       "      <td>54.5258</td>\n",
       "      <td>1.1364</td>\n",
       "      <td>0.8014</td>\n",
       "      <td>55.5425</td>\n",
       "      <td>0.9916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.490200</td>\n",
       "      <td>0.446865</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>2.5069</td>\n",
       "      <td>1.7368</td>\n",
       "      <td>56.2466</td>\n",
       "      <td>1.0529</td>\n",
       "      <td>0.6278</td>\n",
       "      <td>56.3694</td>\n",
       "      <td>1.1962</td>\n",
       "      <td>0.681</td>\n",
       "      <td>53.5293</td>\n",
       "      <td>1.0384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.485300</td>\n",
       "      <td>0.473347</td>\n",
       "      <td>0.0588</td>\n",
       "      <td>0.2626</td>\n",
       "      <td>2.4987</td>\n",
       "      <td>1.7676</td>\n",
       "      <td>56.5007</td>\n",
       "      <td>1.0576</td>\n",
       "      <td>0.5986</td>\n",
       "      <td>54.0498</td>\n",
       "      <td>1.2377</td>\n",
       "      <td>0.6499</td>\n",
       "      <td>54.2275</td>\n",
       "      <td>0.9755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.464400</td>\n",
       "      <td>0.482099</td>\n",
       "      <td>0.0503</td>\n",
       "      <td>0.2451</td>\n",
       "      <td>2.1411</td>\n",
       "      <td>1.4963</td>\n",
       "      <td>56.6899</td>\n",
       "      <td>1.0535</td>\n",
       "      <td>0.5169</td>\n",
       "      <td>54.2263</td>\n",
       "      <td>1.1296</td>\n",
       "      <td>0.5491</td>\n",
       "      <td>54.7854</td>\n",
       "      <td>1.127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.487400</td>\n",
       "      <td>0.441659</td>\n",
       "      <td>0.0595</td>\n",
       "      <td>0.2549</td>\n",
       "      <td>2.3953</td>\n",
       "      <td>1.6510</td>\n",
       "      <td>57.0923</td>\n",
       "      <td>1.0626</td>\n",
       "      <td>0.6083</td>\n",
       "      <td>55.7251</td>\n",
       "      <td>1.1867</td>\n",
       "      <td>0.6578</td>\n",
       "      <td>54.3192</td>\n",
       "      <td>0.9773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.506300</td>\n",
       "      <td>0.496211</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.2275</td>\n",
       "      <td>1.8019</td>\n",
       "      <td>1.1944</td>\n",
       "      <td>56.7176</td>\n",
       "      <td>1.0566</td>\n",
       "      <td>0.4666</td>\n",
       "      <td>55.4637</td>\n",
       "      <td>1.0015</td>\n",
       "      <td>0.5406</td>\n",
       "      <td>55.1591</td>\n",
       "      <td>1.1193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.456200</td>\n",
       "      <td>0.452095</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.2369</td>\n",
       "      <td>2.0022</td>\n",
       "      <td>1.3349</td>\n",
       "      <td>56.8271</td>\n",
       "      <td>1.0525</td>\n",
       "      <td>0.5326</td>\n",
       "      <td>55.6774</td>\n",
       "      <td>1.0606</td>\n",
       "      <td>0.5947</td>\n",
       "      <td>55.69</td>\n",
       "      <td>1.0283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.457900</td>\n",
       "      <td>0.450200</td>\n",
       "      <td>0.0454</td>\n",
       "      <td>0.2329</td>\n",
       "      <td>1.9323</td>\n",
       "      <td>1.2824</td>\n",
       "      <td>57.0128</td>\n",
       "      <td>1.0462</td>\n",
       "      <td>0.5107</td>\n",
       "      <td>55.6758</td>\n",
       "      <td>1.0016</td>\n",
       "      <td>0.5797</td>\n",
       "      <td>55.5576</td>\n",
       "      <td>1.1072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.469200</td>\n",
       "      <td>0.445731</td>\n",
       "      <td>0.0447</td>\n",
       "      <td>0.231</td>\n",
       "      <td>1.9028</td>\n",
       "      <td>1.2604</td>\n",
       "      <td>57.0152</td>\n",
       "      <td>1.0463</td>\n",
       "      <td>0.5059</td>\n",
       "      <td>55.8120</td>\n",
       "      <td>1.0053</td>\n",
       "      <td>0.5816</td>\n",
       "      <td>55.2349</td>\n",
       "      <td>1.1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.460800</td>\n",
       "      <td>0.438243</td>\n",
       "      <td>0.0452</td>\n",
       "      <td>0.2325</td>\n",
       "      <td>1.9267</td>\n",
       "      <td>1.2755</td>\n",
       "      <td>57.1977</td>\n",
       "      <td>1.0418</td>\n",
       "      <td>0.5144</td>\n",
       "      <td>55.6659</td>\n",
       "      <td>1.0049</td>\n",
       "      <td>0.5881</td>\n",
       "      <td>55.0709</td>\n",
       "      <td>1.1181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.419500</td>\n",
       "      <td>0.439947</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.2326</td>\n",
       "      <td>1.9282</td>\n",
       "      <td>1.2782</td>\n",
       "      <td>57.2116</td>\n",
       "      <td>1.0436</td>\n",
       "      <td>0.5156</td>\n",
       "      <td>55.6926</td>\n",
       "      <td>0.9995</td>\n",
       "      <td>0.5891</td>\n",
       "      <td>55.1858</td>\n",
       "      <td>1.1143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8804, training_loss=0.6076698233354422, metrics={'train_runtime': 3418.1337, 'train_samples_per_second': 10.302, 'train_steps_per_second': 2.576, 'total_flos': 0.0, 'train_loss': 0.6076698233354422, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 2e-4, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# raw standardized trade loss (summed per day per timeframe)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8/8 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.7025059461593628,\n",
       " 'eval_day profit': '0.0661',\n",
       " 'eval_day sharpe': '0.5309',\n",
       " 'eval_trade %': '8.2851',\n",
       " 'eval_full trade %': '1.3026',\n",
       " 'eval_full trade accuracy': '9.4595',\n",
       " 'eval_full trade g/l': '2.8037',\n",
       " 'eval_medium trade %': '1.1709',\n",
       " 'eval_medium trade accuracy': '68.9662',\n",
       " 'eval_medium trade g/l': '1.4940',\n",
       " 'eval_small trade %': '3.493',\n",
       " 'eval_small trade accuracy': '61.0659',\n",
       " 'eval_small trade g/l': '1.0435',\n",
       " 'eval_runtime': 2.1841,\n",
       " 'eval_samples_per_second': 14.651,\n",
       " 'eval_steps_per_second': 3.663,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(fx['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sqconv_raw.model'\n",
    "trainer.save_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quick prediction test to ensure model isn't cheating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 13 layers\n"
     ]
    }
   ],
   "source": [
    "config = SGConvConfig(\n",
    "    n_embd = 320, n_head = 1, hidden_dropout_prob = 0\n",
    ")\n",
    "model = SGConvTrader.from_pretrained(model_name, config = config, ignore_mismatched_sizes=True).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([-0.0257, -0.0132, -0.0076, -0.0249,  0.0175, -0.0234, -0.0048,  0.0026,\n",
      "        -0.0191])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([ 3.5118e-02,  2.6283e-02, -2.4536e-02,  6.2625e-02, -2.8070e-04,\n",
      "        -2.2268e-01, -2.5588e-01, -3.8412e-01, -4.5883e-01])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([ 0.0920, -0.0433, -0.0012,  0.0656,  0.0100, -0.0220,  0.0235,  0.0310,\n",
      "         0.0280])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "tensor([ 0.0243,  0.0728,  0.0025,  0.0273, -0.0455, -0.1555, -0.1298, -0.0893,\n",
      "        -0.1602])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "tensor([-0.0907, -0.1053, -0.3088, -0.2540, -0.3148, -0.7390, -0.7282, -0.4338,\n",
      "        -0.2508])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "tensor([-0.0098,  0.0283, -0.0867, -0.1492,  0.3091, -0.0040,  0.1885,  0.2064,\n",
      "         0.2083])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA/4ElEQVR4nO3de3zcVZn48c+Te3Nv7pe2SdqmadJCL5RyLRQppSIWxVXBy6Losqx30RVRf79Vf8uuyuquuwosgsoulYtclDstCMitlAK9t+m9TZo0mTRNkzT3mef3x8yk02QmM8kkmWT6vF+vvDrz/Z7vN+cLSZ455zznHFFVjDHGGH9iIl0BY4wxE5cFCWOMMQFZkDDGGBOQBQljjDEBWZAwxhgTUFykKzCacnJytLS0NNLVMMaYSeXdd99tUtVcf+eiKkiUlpaycePGSFfDGGMmFRE5FOicdTcZY4wJyIKEMcaYgCxIGGOMCciChDHGmIAsSBhjjAnIgoQxxpiALEgYY4wJyIKEGVXbjpzgzb1Nka6GMWaUWJAwo0ZVueWRTXz1wfexfUqMiQ4WJMyoeb+mhd0N7Rw72cM+x8lIV8cYMwrCChIicoeI7BKRLSLyhIhkBih3UES2isgmEdnoc/xhz7FNnjKbPMdLRaTT59zd4dTTjI+HN9QQFyMAbDjQPGRZVeXoia7xqJYxJgzhtiTWAfNV9WxgN3DbEGUvU9WFqrrEe0BVP+k5thB4DHjcp/w+7zlVvTnMepox1t7dx1Nb6rh2cTE5qQm8c3DoIPHM1nou+ulfqGnuGKcaGmNGIqwgoaprVbXP83Y9MG0k9xERAT4BPBhOfUzkPL25jo4eJ9ctncHSsqygLYnntx3F6VK217WOUw2NMSMxmmMSNwLPBTinwFoReVdEbvJzfhnQoKp7fI6Vicj7IvKqiCwL9E1F5CYR2SgiGx0Ox8hrb8Ly4Ds1zMlPZdH0TJaWZnGkpZPa4/5bCX1OF6/tcWdA7W1sG89qGmOGKWiQEJEXRWSbn69rfMp8H+gD1gS4zUWquhj4IPBlEblkwPnrOb0VUQ/MUNVFwC3AH0Qk3d+NVfUeVV2iqktyc/0uh27G2M76VjbXtPDJc2cgIpxblgUQsMtpc20LJzp7Adjd0D5u9TTGDF/Q/SRUdcVQ50XkBuBq4HINkPeoqnWefxtF5AlgKfBXz/VxwLXAOT7lu4Fuz+t3RWQfMAewzSImoIffqSEhNoaPLioGYG5BOmlJcWw40MxHFw3ugXyl2kGMwILpmexptCBhzEQWbnbTKuBWYLWq+u1bEJEUEUnzvgZWAtt8iqwAdqlqrc81uSIS63k9EygH9odTVzM2unqdPPH+Ea6cX0BWSgIAsTHCuaVZvB1gXOKVageLZ0xlSclU9jnacbpsToUxE1W4YxK/AtKAdb6pqiJSJCLPesrkA6+LyGZgA/CMqj7vc4/rGDxgfQmwxXPNo8DNqjr0SKiJiBe2H+VEZy/XnTv9tONLy7LY7zhJU3v3accdbd1sPXKC5RW5lOel0dPnsgwnYyawsLYvVdXZAY7XAVd5Xu8HFgxxj8/5OfYY7pRYM8E9/E4N07OmcMHM7NOOn1vqGZc40MwHzyrsP/7X3e7kguUVefQ6XQDsaWynNCdlnGpsjBkOm3FtRuzQsZO8ue8Yn1wynRjPJDqvs4ozSIqPYcOAwetXdzvISU2kqjCd2XmpAOyxDCdjJiwLEmbEHn6nhhiBvzln+qBzCXExLJ4x9bT5Ek6X8tc9Di6dk0tMjJCWFE9hRhJ7LMPJmAnLgoQZsbU7Gri4PJeCjCS/588tzWJHfSutXe501821LbR09HJpxalU5dl5qdaSMGYCsyBhRsTlUg4f66CyIC1gmfPKslCFdw8dB06lvl5SntNfZk5+Gnsb23FZhpMxE5IFCTMijW3d9DhdTMtKDlhm0YypxMVIf5fTq9WNLJyeSWZyQn+Z8rxUunpdHGnpHPM6G2OGz4KEGZHDnrTVGUMEiSkJsZw1LYMNB5o51t7NliMnWF6Rd1qZ8nwbvDZmIrMgYUbEO7dh+tQpQ5ZbWpbFltoW1u5oQBWWV5y+dMrsXHd3lS3PYczEZEHCjMjh5g5EoDhIkDivLItep/Lrl/eSk5rA/KKM085nJMeTl5ZoGU7GTFAWJM4QW2tP8P0nttLnmcAWrprjHRSkJ5EYFztkuXNKshCB2uOdXFKeO2g+BXgHr627yZiJyIIE7vz9xrYuunqdka7KmHnsvVrWvH2Yx96rDV44BDXNHUyfGng8witjSjxzC9wL+F5a4X+VXncabLvti23MBGRBAthU08LS219i/f5jka7KmNlR797c55cv7hmVYFjT3Mn0IQatfZ0/M4vYGOGScv9Bojw/lY4eJ3W2nakxE05YazdFi2zP6qXNJ3siXJOxoarsrG+lIj+N6oY2/vD2YW68uGzE9+vqddLQ1sX0rKHHI7y+9oFyPji/kKkpCX7Pl+e5B6/3NLRRnBnaPY2JpL/samC/4+Sg4xeX5/S3nKOFBQkgO9X9x+tYe3QGiSMtnbR19fGZVSU8t7WeO1/ZyyfPnU5K4sj+9x9p6UR16PRXX1NTEljq2YjIn3LvGk4N7YNSZI2ZaI6e6OKL92/E3/zP88qyePjvLxj/So0hCxJAamIcCXExNJ3sDl54EtpZ7x4UripMY15ROtfe+Sa/f/MgX77M7yK+QfWnv4YYJIKZmpJATmqizZUwk8Kj79bgUnju68uY5pPdd/szO3lmaz2qisjgBI3JysYkABEhOyUhpJZEV6+zf4nryWJnfSsiUFGQzuIZU1lRmcfdr+7jREfviO5XE8JEuuEq9wxeGzORuVzKwxtruGBmNpWF6aQlxfd/zS/OoK2rL+pWD7Ag4ZGdmhDSmMS1d77JHS9Uj0ONRs/O+lZKspJJ9XQvfWtlBW1dfdzz2r4R3a/meCcJcTHkpiaOWh3L81PZ22AZTmZie2v/MWqaO7lu6eCVjysL3WMR3pZ7tLAg4ZGdksix9qG7m1SVPY1tvD3JsqB21rf2/wCD+4f5wwuK+O3rB3G0Db+Lraa5g2lTp/id8zBS5flptHX30dAanV1+Jjo89E4NGVPiuXJewaBzcwvSEHH/vkWTcPe4vkNEdonIFhF5QkQyA5TLFJFHPWV3isgFnuNZIrJORPZ4/p3qc81tIrJXRKpF5Mpw6hmK7JQEmoJ0N53o7KXXqVQ3tE2afZnbu/s4eKzjtCAB8M0V5fQ4Xdz5yt5h3/Nwc8eodjXBqcHr3Q3R9SnMRI/mkz28sO0oH11UTFL84EmkKYlxlGanWJAYYB0wX1XPBnYDtwUo90vgeVWdi3sr052e498FXlLVcuAlz3tEpAr33tfzgFXAnSIy9NTeMIXS3eT91N3V6+JA0+D0t4mo+qj7B3ZgkJiZm8rfLJ7GmvWHh73HdKgT6YajP8PJxiXMBPXE+0focbr45LmDu5q8KgvT+uckRYuwgoSqrlXVPs/b9cC0gWVEJB24BLjPc02PqrZ4Tl8D3O95fT/wEZ/jD6lqt6oeAPYCS8OpazDZqYl09jrp6OkLWMbh0x01WT4t7PD0j1YWDt734RtXlBMfK/zgT9tCHgs40dFLa1ffqLckslMTyUpJsOU5zISkqjz8zmEWTM8c9IHLV2VBOoeOddDeHfjvyGQzmmMSNwLP+Tk+E3AAvxOR90XkXhHx7nqfr6r1AJ5/vUnyxUCNzz1qPccGEZGbRGSjiGx0OBwjrnxWSvC5Er7995MlSOysbyU9Kc7vJLXCjCl8a2UFr+528MzW+pDuV3Pcm/46+pPeZuel2kJ/ZkJ6v6aF3Q3tXD9EKwJOtdi9LfhoEDRIiMiLIrLNz9c1PmW+D/QBa/zcIg5YDNylqouAk3i6lYb6tn6O+f2oq6r3qOoSVV2Sm+t/2YdQ5Hgn1A3R5eQdsyjMSJo0TUrvoHWgvO0bLizlrOIMfvTUDk50Bk+J9e4jMW2Uu5sA5uSfvoaTqrKppoVfv7yX2uPD6xIzZjQ9tOEwyQmxXL2gaMhylUXuILEjijKcgk6mU9UVQ50XkRuAq4HL1X+fRS1Qq6pve94/yqkg0SAihapaLyKFQKPPNb4hexpQF6yu4chOcadzDpXh5GjrJj5WOH9mNm/uaxrL6owKp0upPtrGJ5YE/vQTGyP867VnsfpXr/Oz53dx+0fPGvKe/XMkskc/SJTnpXGi8zDPbTvKhgPNvLD9KPWe9Zw6evr4xyvnjvr3NCaYtq5entpcz+oFRf1p5IEUZSSRnhQ3aXoaQhFudtMq4FZgtar6/ainqkeBGhGp8By6HNjhef0kcIPn9Q3An32OXyciiSJSBpQDG8KpazChdjflpCZSVZhOQ2v3hF/r6dCxk3T0OKkaog8VYH5xBp+/qIw1bx/u3486kJrjHWRMiSc9KX40qwqcGrz+0pr3eOidw5xVnMEvPrGAaVOncPCYtSRMZDy9pZ7OXief9DM3YiARobIwPaqCRLjLcvwKSATWeboz1qvqzSJSBNyrqld5yn0VWCMiCcB+4POe4z8BHhGRLwCHgY8DqOp2EXkEdzDpA76sqmO6jnd2SN1N3eSmJfpMmmnlotk5Y1mtsOzsH7QOvuDYLVfM4bmt9Xzv8a08/bWLiY/1//nhcHPnqA9ae51blsUtV8xhTn4ql8zJJTnB/eP55OY6Dk6SbDITfR56p4Y5+aksmp4ZUvmqonQe2lCD06XEjuJcokgJK0ioqt/Ff1S1DrjK5/0mYImfcsdwtyz83eN24PZw6jccyQlxJCfEBu1uKshI6s8UmvhBopXYGOnfR3ooKYlx/Oia+fzd/2zkN6/t50vL/a/rVNvcwVw/mVKjIT42hq9dXj7oeGl2ChsPHo+6NXHMxPfuoeNsrmnh/15dFfLPXmVhOp29Tg4dO8nM3OC/exOdzbj2kZWSELwlkZpIdmoieWmJE37wemd9K7NyU/xO/PHniqp8Vs0r4Jcv7vE7d8LlUmqPd476HIlgSrOTae/uCzrZ0ZjRpKrc8cIuclIT/C7DEUhVlC3PYUHCR3ZqYsAg4XIpx072kJvmHuCuLExnR93EDxKhdDX5+qfVVfS5lDVvHx50rqGtix6na9RWfw1VSY47Y/rQMetyMuPnjb3HWL+/mS9fNru/6zMUs/NSiY2RqBmXsCDhIyclIWB30/GOHpwu7U+VrSpKZ5+jnZ6+ibkibEtHD3UnuoYdJAozpnDx7Bye2lw3aIJdTbN7dcvxDhKl2e4gMVlmuZvJT1W5Y201RRlJfOq8GcO6Nik+llm50bM8hwUJH1lDLBfunW2dm5YEuFsSvU5l7wRdRmI4g9YDrV5QxJGWTt47fHqm0+ExWCI8FNOmTiE2RjhkGU5mlHT2ONlc0xLw/LodDWyuaeHrK8pJjBv+ikCVhekTvjs6VBYkfGSnJtJ8ssfvEhXe2db9LQmfweuJyPsD6m85jmBWzssnMS6GJzedPjWlprkDESjKTBqVOoYqPjbGkwZrLQkTvhMdvVz3m/Vc8+s3+K+X9gz6fXe5lJ+v3U1ZTgofWzxopaGQVBamU3+ii5aOyT+OZkHCR3ZKAj1OF21+1l1p6m9JuMckSrNTSIyLmbBBYmd9KzmpieSlDf8PelpSPB+Ym8czW+vp89lgqaa5g8L0pBF9sgpXaXaKBQkTtmPt3Vz/m/XsrGvl4tk5/Hzdbn72QvVpgeKpLXVUN7TxzSvmEBcgFTwYbws+GloTFiR8DLXXdX9LwhMk4mJjqChIY+c4rtEynA153IPWI09VXb2giKb2Ht7y2Tuj5ngH08a5q8mrNDuZQ00dtimRGbGG1i4+ec969jna+c0NS/ifG5fyqfNmcNcr+/jRUztwuZRep4tfrNvN3II0rj6rcMTfK5oynCxI+Mj27LTW7Gev66b2HhLjYkjzmZZfWeDOcBqvP1zX/PoNfvninqDlep0u9jS0B51pPZTL5uaRlhh3WpfT4TFYIjxUJdkptHX3DZmibEwgtcc7+MR/v0V9Syf337iUS+fkEhMj3P6R+Xzh4jJ+/+ZBbnt8Kw+/U8OhYx18e2VFWJtq5aYlkpOaOGF7GobDgoSPbM/SHP7y8R1t7tnWvhNqqorSOd7ROy67qfU6XWypPcGdr+ylobVryLL7HSfpcbpGNGjtlRQfy8p5BTy//SjdfU66ep00tHaP+6C1V5mlwZoR2tvYzifufovjJ3v43y+ex/kzs/vPiQg/+FAlX/3AbB7eWMP//fM2Fs3I5PLKvCHuGJrKwjQLEtFmqO6mpnb3uk2+fJfnGGveOnX3ufivvwzdmtjZP2g98iABsHphEW1dfbxS7ejf3H0slggPRYlnQcEDTZbhZELT69l58UP/+RpdfS7+8Hfns3jG1EHlRIRvrazgH6+sIC42hltXzR2Vmf1VhensaWin1zkx0+RDZUHCh3eRP3/dTd6WhC/v8hTjMTjlHROZkZXMQxtqODxEOuimmhYSYmOYmZsSsEwoLpqVTXZKAk9urotY+qvXtKnJnjRYa0mY4LbUtrD6V2/ws+erWV6Ry7NfW8b84owhr/nyZbPZ+sOVp7U0wlFZmE6P08U+x8RMkw+VBQkfiXGxpCXGBexuGtiSSE+KZ9rUKePSkvBmV33vqkpiY4T/eGm333LvHmrmgfWHuKIqP+AifaGKi43hqrMKeWlnA9VH3QNw4z2RzishLobiTFsN1gyto6ePf356Bx/59Rsca+/m7s8s5r8/u4SCjNCy/EYzc288exrGkgWJAbJTB6/f1Od00dzRM6glAeM3acbbkphfnM4NF5byxPtH2NNweuZEY1sX//DAexRPncK/XDv0vhChWr2wiK5eF//71iES4mLITR3832C8lOak2GqwZkjf/uNm7n39ANctncG6Wy5l1fyRZyiFa2ZuCgmxMZM+w8mCxADuCXWndze5J9gRMEgcbDpJZ8+YrmTeP+M7JzWRmy+dRUpCHL9Yd6o10et08ZU/vE9rVy93f+YcMqaMzn4P58yYSlFGEkdaOpk+dUpYGR/hKs1O5uCxk5YGa/xyuZTX9jRx3bnT+ZePnjVqvwMjFR8bw5yCVGtJRBt/S3P0L8nhGdj2VVWYhkuhumFsPy042rpJS4ojKT6WrJQEvrisjOe2HWVLbQsAP3luFxsONPOTa88Oe8DaV0yM8GHPlo2R6mryKslOoa2rb8Jv9mQi4+Cxk7R19fkdnI6UhdMz2XjwOCc6gm8NPFFZkBggJzVh0JiEt6vHX0uiqtA9GDbWnxYcnmXKvb5wcRlTk+P5t7W7eWpzHfe9foDPXVjKRxYVj/r39gaJSA1ae5XluL+/jUsYfzZ7PjCdPX3oAerx9KmlJXT2Olmz4VCkqzJiFiQGyE5J5HhHDy7XqS4Nb9AYOHAN7sXnUhPHfk/bprbu/tne4F4640vLZ/PX3Q6+9cfNnFMyle9dVTkm33teUTpfuWw2145wHZvRUuJZDdbGJYw/m2tOkJwQS3ne2GyKNRJVRelcPDuH+988OGFXjA4m3D2u7xCRXSKyRUSeEJHMAOUyReRRT9mdInLBUNeLSKmIdIrIJs/X3eHUcziyUhJwupQTnaeah6cW9xscJGJihLkFaWO+t8TAlgTAZy8oIT89kfSkeO789GIS4sYm5osI376ygoUhbt84VqZPTSZGbEKd8W9zbQvzizIm3JahX1hWRkNrN09vqQteeAIK96/KOmC+qp4N7AZuC1Dul8DzqjoXWADsDOH6faq60PN1c5j1DNmpva5PDV472rpJToglJdH/xiOVhensOtp2WutjtPmbp5EUH8ujN1/IU1+9iPz08V2ZNRIS4mIonmppsGawXqeL7XWtnD1t4nQ1eS2fk0t5Xir3vnZgUiZdhBUkVHWtqnqXTF0PDOqPEJF04BLgPs81ParaEur1483bWvAdvG5qH/wH2ldlYTrt3X3UHu8ckzp19Tpp6+rrX6bc1/SsZAozIjMLOhJsNVjjT/XRNnr6XCyIcGvXHxHhCxeXsaO+lbf2HQt+wQQzmv0TNwLP+Tk+E3AAvxOR90XkXhHxNxV44PVlnvKvisiyQN9URG4SkY0istHhcIT1AHBq1rXvXAlH2+CuHl+VYzzzeuAy5Wey0uwUDjRZGqw5nXfQesG0zIjWI5CPLComOyWBe18/EOmqDFvQICEiL4rINj9f1/iU+T7QB6zxc4s4YDFwl6ouAk4C3x3wPQZeXw/M8JS/BfiDp0UyiKreo6pLVHVJbm5u0AcO5tT6Tae6m/yt2+RrbkE6MTJ2GU7egXMLEu41nNq6+jg+iVMKzejbXNPC1OT4iK0tFkxSfCyfvaCEv+xqZG/j5JpcFzRIqOoKVZ3v5+vPACJyA3A18Gn1//GuFqhV1bc97x/FHTQIdL2qdqvqMc/rd4F9wJyRP2bospL9tCSCdDdNSYilNGfs9rQdauD8TONdDda6nIyvLbUnOHta5qgszDdWPnt+CQlxMdz3+sFIV2VYws1uWgXcCqxWVb+jiap6FKgRkQrPocuBHUNdLyK5IhLreT0TKAf2h1PXUMXFxpCZHN8/JtHT56KlozfoH+jKwvQx24DIuptOsTRYM1BHTx+7G9om5HiEr+zURD62uJjH36s9raeiq9fJizsauPOVvf0fCCcS/+k6ofsVkAis80Tw9ap6s4gUAfeq6lWecl8F1ohIAu4/9p8f6nrcA90/FpE+wAncrKrNYdY1ZNkpCf3ZTd5/g/2BripM55kt9bR29ZKeNLrLAXh/cLJTLEhMz5pCjNiEOnPKtiOtuBQWTMDMpoG+cHEZD26o4Z7X9jO/KIPntx/l5V2NdHiW9fnvV/fz/Q9V8vFzpk2YVlFYQUJVZwc4Xgdc5fN+E7BkGNc/BjwWTt3CkZ2a2D8OMNRsa1/ewetd9W0sLcsa1fo42rrJTI4fs3kQk0liXCxFmVNsroTpt7mmBYCzJ+igta/ZeWlcVpHLf7/q7hjJSU3gmoXFrJpfQF5aIv/3z9v4zqNb+POmI/zLR8/qbzlHUrgtiaiUnZLAnkb3GvBN/QvrDU4/9eW7LHCgIOF0KYebO/r71UMVbOD8TFOabavBmlM217ZQnDll0nTH/uDqKs4qPsKyObksnjH1tMl/D990AX/YcJifPreLlf/+V76xYg5/e0FJwDla48E+mvqRnZrQ32cYakuiID2JqcnxQw5eP7D+ECv//VWOD3OBumApuGea0pxk624y/dyD1hO/q8lrVm4qt6ys4NzSrEGzw2NihM+cX8K6Wy7l0jm5/PT5XSz6f+v44v0b+ePGmmH/7RgN1pLwIyslkeMdvfQ5XUOu2+RLRNyD10MEibU7jtLrVGqPdzI1ZeiWia+m9u5J0ZQeL6XZKZzo7OX4yZ5h/Xc00af5ZA+Hmzv41HkzIl2VUVWQkcQ9f7uEDQeaeXZrPWu3H+XFnQ3ExgjnlWVRnDk41XfB9Ew+c37JqNfFgoQf3q6l4x29py3RHUxlYToPrD9En9NF3IBd4U529/HOgeMA1J/o5KxhfPLxtyvemaw0+1QarAWJM5t3qfzJ1JIYjqVlWSwty+KfPlzF1iMneH7bUf6yq9Fvd2sof6NGwoKEH94somMnu/0urBdIZWE63X0uDh47yewBK1G+ue8YPZ4N0Rtau0KuS0dPHyd7nJOmv3U8lPYvGX6SRRNo7wAz/jbXnEAEzgqyf/VkJyKcPS2Ts6dl8p1Vc8f1e9uYhB/9S3O097g/xYf4B/rU8hyDZ1S+uruRlIRY4mKE+hOhB4mmNm93l31i9pqelYwIHGyycYkz3ZbaFmblppI2ymnn5hQLEn7kpJ6add3kZ/XVQGbnpRIXI4PGJVSVV6odXDg7h/z0JI4OoyXhaHeXtZbEKYlxsczISmb7GC/PbiY2VWVzbUvUdjVNFBYk/MjuXwl2eN1NiXGxzM4bvKftPsdJao93srwil4KMJI4OoyXhaAtt4PxMc+mcXF7f6xjzvcXNxFV3ooum9p6I73MS7SxI+JE5JZ4YgbqWTtq6+ob1Kb7KT4bTK9WNgPsPW0H6MIOEJxU3z1oSp7lyXgFdvS5e2xP+yr9mcppMk+gmMwsSfsTECFkpCVQ3uCfUDWc8oLIwnYbW7tPWZnl1t4PZealMm5rsbkm0doW81HVTWzcip8ZJjNvSsizSk+JYu6Mh0lUxEbK5toX4WOkfCzRjw4JEANkpiezytAiG05I4NfPaPXjd0dPH2/ubWT7HvYx5QXoSHT1OWrv6At7Dl6O9m6zkhEEptWe6+NgYLq/M56WdDfQ5J+fewSY8m2taqCxMJzFubFI/jZv95QkgKyWBxhEs0e39VOPtcnrLk/q6vCIPcE+SAULucvK3balxW1mVz/GOXt45eDzSVTHjrLvPyZbaEzYeMQ4sSASQ7dPFNJw/0tmpieSlJfYHiVeqHSQnxHJumTufv9AbJELMcLJ1mwK7ZE4uCXExrN1xNNJVMeNs48HjdPQ4WVYe/kZjZmgWJALw/cM83CW6q4rS2VHf6k593d3IhbOy+5vE+enelkRo+2FbSyKwlMQ4ls3OYe32BtvO9AzzSnUjCbExXDgrO9JViXoWJALwDhSPZInuysJ09jnaqW5oo6a5k0s9XU1wKkiEMqFOVT0tCRu0DmTlvHyOtHSO2f7iZmJ6pdrBuWVTI7o66pnCgkQA3u6mkay+WlmYTq9Tue8196bn3kFrgIS4GHJSE0NamqO9u4+uXpe1JIawojKfGIG12y3L6UxxpKWTPY3tLJ+TF7ywCZsFiQC8XUwjGQ+o8gxe/2nTEWblpjA9K/m08wUZiSG1JGxv6+CyUxNZUpJlqbBnEO+8o+UVNh4xHsLd4/oOEdklIltE5AkRyQxQLlNEHvWU3SkiF3iO/1BEjojIJs/XVT7X3CYie0WkWkSuDKeeI9HfkhjBp/jS7BQS42LodSqX+vm0U5A+JaTsJu8y5daSGNrKefnsrG+lptnWchrK+v3HuOWRTZM+ZfjVagfFmVOYnZca6aqcEcJtSawD5qvq2cBu4LYA5X4JPK+qc4EFwE6fc/+uqgs9X88CiEgVcB0wD1gF3Cki45oMnZ0y8iARFxtDRYG7NeHv005hRmjrN4W64dGZ7oqqfABe2G5ZTkP548ZaHn/vCI+/dyTSVRlSa1dvwHM9fS7e2NvEpRW5E2YP6GgXVpBQ1bWq6p0Vth6YNrCMiKQDlwD3ea7pUdWWILe+BnhIVbtV9QCwF1gaTl2HKzctkbgY8bu5RyjOKs4gJSHW71amBRlJtHT00tU79LpDp7ZOtSAxlJLsFOYWpFmXUxDevRd++dIeuvsm5ppXm2paWPTjdbwY4P/lxkPNnOxxnjbOZ8bWaI5J3Ag85+f4TMAB/E5E3heRe0XEd5Pnr3i6q34rIt7NAYqBGp8ytZ5jg4jITSKyUUQ2Ohyjt45PWlI8T3zpIq5fOrIdr761soLHvnSh341ACtJDm1DnaOsmNkaYmmzZTcGsrMpn48Hm05ZDMae0d/ex19HOeWVZHGnp5MG3D0e6Sn49+m4NTpfysxd24XQNTmt+tdpBfKxw4eycCNTuzBQ0SIjIiyKyzc/XNT5lvg/0AWv83CIOWAzcpaqLgJPAdz3n7gJmAQuBeuDn3lv6uY/fRHhVvUdVl6jqktzc0f10cda0DKYkjKyXKyslgbkF6X7PeSfUBRu8drR1k5WSMGgfXDPYynkFuBRe2tUY6apMSFtrT6AKNy+fxfkzs/jVy3vp6AltaZjx0ut08ezWoxSkJ7G7oZ0nNw/uFnul2sGSkixSLfV13AQNEqq6QlXn+/n6M4CI3ABcDXxa/c9oqgVqVfVtz/tHcQcNVLVBVZ2q6gJ+w6kupVpgus89pgF1I3nAiSi/f9b10BPqmoaxTPmZbl5ROsWZUywVNoDNnq6mBdMy+ccrK2hq7+F3bxyMaJ0GemNvE80ne/jh6nlUFabz7+v20OszyF5/opPqhjbLahpn4WY3rQJuBVarqt/UElU9CtSISIXn0OXADs/1hT5FPwps87x+ErhORBJFpAwoBzaEU9eJ5FR309BdI4720HfFO9OJCBfMymZTja3j5M+W2hamZ00hKyWBc0qyuHxuHv/96j5OdAYeJB5vT26uIy0pjsvm5vLtK+dwuLmDRzae6nV+tdrdnby8wuZHjKdwxyR+BaQB6zwprHcDiEiRiDzrU+6rwBoR2YK7a+lfPMd/JiJbPccvA74JoKrbgUdwB5PngS+r6sQcaRuBlMQ40pPigi7N0dRmLYnhqCxMp6m9h8a20PfrOFNsrjnBAp99F761soLWrj5+89f9kauUj65eJ2u3N7BqXgGJcbFcVpHHOSVT+c+X9vQneLxS7aAwI4k5+Zb6Op7CzW6ararTfVJYb/Ycr1PVq3zKbfKMG5ytqh9R1eOe459V1bM8x1erar3PNber6ixVrVBVfwPik1pBRtKQYxLuJTl6LP11GE6twDt4j/EzWVN7N0daOk8LElVF6Vx9diG/feNAfxZdJL28q5H27j5WLywC3C3Df7yygobWbh5Yf4hepzv1dbmlvo47m3EdIQUZU4ZcmqO1s48ep8vWbRqGqv69PGwdJ1/e1NcFA5bVvuWKOXT3ubjz5X3jX6kBntxcR05qAhfMPLVg3/kzs1lWnsOvX97LX3c7aOvu8zs51YwtSxGIkIL0U5sa+eNodwcQa0mELjM5gcKMJAsSA2yqOUGMwPzi07PtZuam8jeLp/HA+kN8/qLSQcvHBKKq7Kxv4/ntR1m3o6F/0qevysI0frh6HrNyg3cNtXX18tKuRq4/d/qgzbW+vbKCa379Brc+toW4GOGi2bbq63izIBEhBRlTcLR30+t0Ee9n1znvhkc2JjE8/vYYP9NtqW1hTn4ayQmDf92/cUU5T2+p4wd/2sbvP3/ukF052+tO8OdNdTy/7SiHmzsQgXNLs1g0I/O0ci6X8uzWej74H6/xtctnc9Mls4ZcSXnt9gZ6+lz9XU2+FkzPZGVVPmt3NHD+zCzSkuJDf3AzKixIREhhRhKq7rkQRX5mddu6TSNTWZjOK7sddPU6/U5kPNOoKptrWvqXLhmoMGMK31pZwY+f3sEzW+u5+uzBf6jBvcPiZ+57mxiBi2bn8A/LZ7GiMj/gz+ctK+fwo6d28G9rd/PU5np+8rGzWDRjqt+yT26uY9rUKSwOcP5bKyt4aVcjV1QVhPDEZrTZmESEFATZV8JWgB2ZysJ0nC5lT0N7pKsyIdQe7+R4Ry9n+wxaD3TDhaWcVZzBj57a4Tcltv5EJ1998D1KspPZ8L0V/P7zS7l+6YwhP8DkpSXx608t5t6/XUJrVy/X3vUmP3xyO+3dp0/gO9bezet7m/jwgqKArZiKgjRe+fZybrigJLSHNqPKgkSEePe6DjR43dTeTXyskDHFmtfDMXCP8TPdppoWgCH3go6NEf712rM41t7Nz57fddq5nj4XX1rzHp09Tu757DlMTRleIsWKqnzWfvMSPnt+Cfe/dZCVv3iVv+w6NeHx2W1HcbqU1Qv8t2C8pmclDxqvMOPD/qtHSLClORxt3WSnJBJjS3IMS0l2ClPiY22nOo8ttS0kxJ1alTiQ+cUZfP6iMta8fZh3D52akPjPz+zg/cMt/OxvFjA7b+h7BJKWFM+Pr5nPozdfSEpiHDf+fiNfffB9HG3dPLWpjvK8VOYGqZ+JHAsSEZIxJZ7EuJiAE+psb+uRiY0R5hamWUvCY3PtCeYVpftNjhjolivmUJSRxPce30qv08Xj79XyP28d4u+WlfGhswuDXh/MOSVTeeZry/jmijm8sO0oK37xKhsONrN6iK4mE3kWJCJERDz7SvifyNTUbkFipCo9GU7+lxI7czhdyrYjp8+0HkpKYhw/vmY+1Q1tfO/xrdz2+FbOn5nFravmjlqdEuJi+PqKcp79+sWU56WSEBvjN6vJTBwWJCKoICNpyJaETaQbmcrCdFq7+qgLYfe/aLa3sZ2OHicLpmeEfM2KqnxWzSvgj+/WMjU5gf+6fvGYjAXMzkvjkb+/gPXfu5yS7JTgF5iIsRTYCCpIT+Ldw4MXpHO5lGMnbUmOkfLuMb6zrnXEm0ZFg82eQeuhMpv8+eHqefS5XHzlA+Vj+jMYEyNkDXMg3Iw/a0lEUEHGFBpOdOMasLlK3YlOnC61iXQjVOHZx+NMH7zeXNtCWlIcZcP8pF6QkcS9N5w7ZEaUOXNYkIigwowkepwumjt6Tjv+0IYaRGxJ5JFKTYyjJDv5jB+83lzbwtnTMixDzoTFgkQE5fvZxrSjp48H3j7EFZX5lOZYX+1InenLc3T1OtlV3xbyoLUxgViQiCDvXAnfIPHYe0do6ejl7y6ZGalqRYXKwnQONXdwsntibdE5XnbUt9Ln0mGPRxgzkAWJCOqfUOeZde1yKb99/QALpmWwpMT/OjYmNJWF6ajCrqNn5t4SWzyD1sPJbDLGHwsSEZSdmkhsjNDgaUm8uLOBA00n+eKymTa5KExn8vIc7x8+zv1vHSI/PbF/jTBjRsqCRATFxgj5aYn9S3Pc+/oBijOn8MH5ttpluIozp5CeFOc3SJzo7OX5bUcjUKuxdbK7jx89tZ1r73qTrl4nv/jEQvuwYcIW1jwJEbkD+DDQA+wDPq+qLX7KZQL3AvMBBW5U1bdE5GGgwlMsE2hR1YUiUgrsBKo959Z7t0aNNgUZSRxt7WRLbQsbDjTzgw9V2kJmo0BEmFuYPigNts/p4ktr3uWNvcd4+qsXM784OrpjXt7VyA/+tI26E5387fklfPvKCtt7wYyKcCfTrQNuU9U+EfkpcBtwq59yvwSeV9W/EZEEIBlAVT/pLSAiPwdO+FyzT1UXhlm/Ca8gI4nqo23c+9oBUhPj+OS50yNdpahRVZjOIxtrcLm0Pw305+t288beYwCs3dEQFUHin5/ewb2vH6A8L5VHb76Ac0qyIl0lE0XC+siqqmtV1Zs+sh6YNrCMiKQDlwD3ea7pGdjaEHeb+BPAg+HUZzIqSJ9CTXMnz2yt57pzp9unv1FUVZhOR4+TQ80dADy/7Sh3vbKPT503g6VlWazdPvm7nA4dO8l9bxzgY4un8fTXLrYAYUbdaPZr3Ag85+f4TMAB/E5E3heRe0Vk4ASAZUCDqu7xOVbmKf+qiCwL9E1F5CYR2SgiGx0OR9gPMd4KMhLpcboA+NxFpZGtTJSpLHTPvN5Z38o+Rzvf/uNmFkzP5J8+XMXKqnx2HW3j0LGTEa5leH77+gHiYoTvrKogMc524jOjL2iQEJEXRWSbn69rfMp8H+gD1vi5RRywGLhLVRcBJ4HvDihzPae3IuqBGZ7ytwB/8LRIBlHVe1R1iaouyc3NDfY4E05BhnttoQ/OL2Da1NA2ojehKc9PJTZG2HjwODf/77skxMVw16cXkxgXy5Xz3MkB63Y0BLnLxHWio5dHNtayekFx/8RMY0Zb0DEJVV0x1HkRuQG4Grhc/a/NXAvUqurbnveP4hMkRCQOuBY4x+d7dgPdntfvisg+YA6wMVh9J5uzijPIS0vkH5bPinRVok5SfCwzc1L43ZsHEOCBL5zXv5/49KxkKgvTWbu9gS8um5wTF9dsOERnr5MvXFwW6aqYKBZWd5OIrMI9UL1aVTv8lVHVo0CNiHizmC4HdvgUWQHsUtVan/vmikis5/VMoBzYH05dJ6qynBQ2fH8F84om/wDqROSdVPedVXO5cHbOaedWVuWz8VAzTe3+9/SYyHr6XNz/5kEunp1DVZHfRrYxoyLcMYlfAWnAOhHZJCJ3A4hIkYg861Puq8AaEdkCLAT+xefcdQwesL4E2CIim3G3PG5W1eYw62rOQJ+7qJRvr5zD3/tZ5mTlvHxcCi/tnHxdTk9vqaOhtZsvLLNWhBlbEk27dy1ZskQ3boy6HikzRlSVi3/6MnML0rjvc+dGujohU1U+9J+v0+t0sfabl9iEORM2EXlXVZf4O2eztswZS0RYOS+f1/Y2TaqFAN/ad4wd9a18cVmZBQgz5ixImDPalfMK6Olz8dfdkyd9+t7XD5CTmsA1C4sjXRVzBrAgYc5oS0qmMjU5nrWTJBV2b2Mbf9nVyGfPLyUp3uZFmLFnQcKc0eJiY7i8Mp+XdjbQ65nUOJHd9/pBEuNi+Mz5MyJdFXOGsCBhzngrq/Jp7erj7f0TO4Hu6IkuHnuvlmsXTyPb9j8348SChDnjLSvPJSk+hrU7JvZaTv/5lz2oKl+yiZdmHFmQMGe8KQmxXFKey9rtDUzUlPBDx07yyDs1XL90BtOzbPkWM34sSBiDO8vpaGsX2+sm5k52//HiHuJiha9cNjvSVTFnGAsSxgDnz8oG4H3P3tATye6GNv606Qg3XFhKni3kZ8aZBQljgKKMJKYmx7Ot9kTwwuPs52urSU2I4+ZLbCzCjD8LEsbgnn09vziDbXUTK0hsrmnhBc9KtVNTEiJdHXMGsiBhjMe8ogx2N7TR3eeMdFX6/dvaarJSEmwhPxMxFiSM8ZhfnE6vU9nT0B7pqgCwfv8xXtvTxJeWzyI1Mdzt6I0ZGQsSxnjM9+zpse3I2HU5dfU6eedg8El7qsq/vVBNfnoinzm/ZMzqY0wwFiSM8SjJTiYtKW5MxyV+vraaj9/9Fne8sCvgnAyXS/k/f97GxkPH+caKObZGk4koCxLGeIgI84rS2XZkbOZKdPc5eey9I6QnxfHrl/fx46d3DAoUfU4X//joFh5Yf5ibL53FdedOH5O6GBMqCxLG+JhflMHO+lb6xmCxv3U7Gmg+2cN/Xr+Iz11Yyu/eOMj3ntiGy+UOFL1OF19/eBOPvVfLt66Yw62rKmy/CBNx4e5xfYeI7BKRLSLyhIhk+ilT4dna1PvVKiLf8JzLEpF1IrLH8+9Un+tuE5G9IlItIleGU09jQjW/OIPuPhd7HaM/eP3wOzUUZ05hWXku//ThKr60fBYPbjjMt/64mY6ePv7hgXd5Zks9P/hQJV+9vNwChJkQwm1JrAPmq+rZwG7gtoEFVLVaVReq6kLgHKADeMJz+rvAS6paDrzkeY+IVOHe+3oesAq4U0SsY9aMufnF6QCj3uVU09zBa3ua+PiSacTGCCLCd1bN5dsr5/DE+0e4+Kcv8+LORv7fR+bzxWWD9+M2JlLCChKqulZVvfs+rgemBbnkcmCfqh7yvL8GuN/z+n7gIz7HH1LVblU9AOwFloZTV2NCUZaTypT42FHPcPrjxhpE4BNLTh9j+MoHyvk/V1fR2ePk3z6+gM9aJpOZYEYz+fpG4OEgZa4DHvR5n6+q9QCqWi8ieZ7jxbiDjlet59ggInITcBPAjBm2EYsJT2yMUFWUzvZRzHDqc7p4ZGMtl87JpShzyqDzX7i4jBsuKCEu1oYIzcQT9KdSRF4UkW1+vq7xKfN9oA9YM8R9EoDVwB9DqJe/zli/+YKqeo+qLlHVJbm5uSHc2pihzS9KZ3tda/+Acrj+usfB0dauITOVLECYiSpoS0JVVwx1XkRuAK4GLtehF+P/IPCeqvpuJtwgIoWeVkQh0Og5Xgv4/kZNA+qC1dWY0TC/OIP73zrEgWMnmZWbGvb9HtpQQ05qAh+Ymz8KtTNmfIWb3bQKuBVYraodQYpfz+ldTQBPAjd4Xt8A/Nnn+HUikigiZUA5sCGcuhoTqvnFozfzurG1i5d2NfKxxdNIiLPWgpl8wv2p/RWQBqzzpLfeDSAiRSLyrLeQiCQDVwCPD7j+J8AVIrLHc/4nAKq6HXgE2AE8D3xZVSfOqmsmqs3OSyUhLmZUNiB69L1anC7lkzYpzkxSYQ1cq6rfbbJUtQ64yud9B5Dtp9wx3BlP/u5xO3B7OPUzZiTiY2OoLEhja5h7S6gqD79Tw9KyLGaOQreVMZFg7V9j/Jjn2VsinD2v1+9v5tCxDltaw0xqFiSM8WN+UQZtXX3UNHeO+B7Pbq0nJSGWD84vHMWaGTO+LEgY40f/zOsw5kvUtXRSkp3ClARbLMBMXhYkjPFjTn4acTESVoZTY1s3uWmJo1grY8afBQlj/EiKj2VOfhrbwshwcrR1k2dBwkxyFiSMCWB+cTrbj4xs8NrlUprarSVhJj8LEsYEML84g2Mnezja2jXsa4939NDnUgsSZtKzIGFMAPM8e16PZL6Eo70bgLy0pFGtkzHjzYKEMQHMLUgDYHdD27CvdbS5g4S1JMxkZ0HCmABSEuPIS0vk4LFgy5INZkHCRAsLEsYMoTQnhUPHTg77ukYLEiZKWJAwZgil2ckjbkkkJ8SSmjia+3oZM/4sSBgzhJLsFBxt3bR39wUv7MNhE+lMlLAgYcwQynJSAIbd5dTY1kVuqgUJM/lZkDBmCCXZyQAcbBpel5OjrZu8dAsSZvKzIGHMEEqz3S2Jg8NsSTjauq0lYaKCBQljhpCSGEduWiIHm0IPEl29Tlq7+mxMwkSFcPe4vkNEdonIFhF5QkQy/ZSp8Gxt6v1qFZFvDHW9iJSKSKfPNXeHU09jwlGWncKhYWQ42RwJE03CbUmsA+ar6tnAbuC2gQVUtVpVF6rqQuAcoAN4IoTr93mvU9Wbw6ynMSNWkp08rO4mW5LDRJOwgoSqrlVVb27gemBakEsux/3H/9AIrzdm3JXmpNDY1s3JENNgrSVhoslojkncCDwXpMx1wIMhXl8mIu+LyKsisizQDUXkJhHZKCIbHQ7H8GpsTAi8g9ehdjnZbGsTTYIGCRF5UUS2+fm6xqfM94E+YM0Q90kAVgN/9HNu4PX1wAxVXQTcAvxBRNL93VdV71HVJaq6JDc3N9jjGDNs/WmwIXY5Odq6EYHslISxrJYx4yLomgGqumKo8yJyA3A1cLkOvTvLB4H3VLUh2PWq2g10e16/KyL7gDnAxmD1NWa0leYMLw3W0dZNdkoCcbGWPGgmv7AWlhGRVcCtwKWqGqwtfj0DupoCXS8iuUCzqjpFZCZQDuwPp67GjFRqYhw5qaGnwTrausixORImSoT7UedXQBqwzjdVVUSKRORZbyERSQauAB4P5XrgEmCLiGwGHgVuVtXmMOtqzIiV5YS+0J+t22SiSVgtCVWdHeB4HXCVz/sOIHsY1z8GPBZO3YwZTSXZKby2J7TECEdbN7Pz0sa4RsaMD+s0NSYEZTkpNLR209EzdBqsquJot5aEiR4WJIwJgTfDKVgabEtHL71OtSBhooYFCWNC0L/QX5DB61OzrS1ImOhgQcKYEJyaKzF0S8JmW5toY0HCmBCkJcWTk5oQtCXR2NYFWJAw0cOChDEhKs1OCTqhzloSJtpYkDAmRCUhLBnuaOsmKT6GtMSwssuNmTAsSBgTorKcZI62dtHZ4wxYptEzkU5ExrFmxowdCxLGhKjEuxpsc+AuJ9u21EQbCxLGhKgsJ3garKOt2zYbMlHFgoQxIZoRQhqszbY20caChDEhSk+KJzslcBpsd5+Tlo5eCxImqliQMGYYSnMCp8E2tfcANtvaRBcLEsYMQ0l2csA0WJsjYaKRBQljhqEsO4X6E/7TYBtbbba1iT4WJIwZhhJPhtPh5sGtCe/ifhYkTDSxIGHMMJR55koc8DN47e1usq1LTTQJK0iIyB0isktEtojIEyKS6adMhWdrUu9Xq4h8w3PuhyJyxOfcVT7X3SYie0WkWkSuDKeexoyWkhzvvhKDg0RjWzdZKQnEx9pnLxM9wv1pXgfMV9Wzgd3AbQMLqGq1qi5U1YXAOUAH8IRPkX/3nlfVZwFEpAq4DpgHrALuFJHYMOtqTNjSk+IpzEhi/f5jg87ZbGsTjcIKEqq6VlW9+zmuB6YFueRyYJ+qHgpS7hrgIVXtVtUDwF5gaTh1NWa0fPLc6bxc7WCfo/204462bvLSLUiY6DKa7eIbgeeClLkOeHDAsa94uqt+KyJTPceKgRqfMrWeY4OIyE0islFENjocoW1Ub0w4PnN+CQlxMdz3+oHTjltLwkSjoEFCRF4UkW1+vq7xKfN9oA9YM8R9EoDVwB99Dt8FzAIWAvXAz73F/dxC/d1XVe9R1SWquiQ3NzfY4xgTtpzURD62uJjH3q2l+aR7Ap2quoOEZTaZKBM0SKjqClWd7+frzwAicgNwNfBpVfX7h9zjg8B7qtrgc+8GVXWqqgv4Dae6lGqB6T7XTgPqhvdoxoydL1xcRnefiwfWu3tOWzv76HG6LEiYqBNudtMq4FZgtaoOvRsLXM+AriYRKfR5+1Fgm+f1k8B1IpIoImVAObAhnLoaM5pm56VxWUUu//PWQbp6nTjabSKdiU7hjkn8CkgD1nlSWO8GEJEiEXnWW0hEkoErgMcHXP8zEdkqIluAy4BvAqjqduARYAfwPPBlVQ2804sxEfDFZTNpau/hyU11NLbaRDoTncLaY1FVZwc4Xgdc5fO+A8j2U+6zQ9z7duD2cOpnzFi6cFY2lYXp3Pv6fr603P2rYIv7mWhjs36MGSER4YsXl7G7oZ3H3qsFINc2HDJRxoKEMWH48IIi8tISeW1PEwlxMaQnhdU4N2bCsSBhTBgS4mK44cJSAHJTExHxl71tzORlQcKYMH36vBlMiY+1QWsTlaxtbEyYMpMT+NdrzyIp3j5zmehjQcKYUfCRRX5XjTFm0rOPPsYYYwKyIGGMMSYgCxLGGGMCsiBhjDEmIAsSxhhjArIgYYwxJiALEsYYYwKyIGGMMSYgGXozuclFRBzAoTBukQM0jVJ1Ii2angWi63mi6Vkgup4nmp4FQn+eElX1u/9zVAWJcInIRlVdEul6jIZoehaIrueJpmeB6HqeaHoWGJ3nse4mY4wxAVmQMMYYE5AFidPdE+kKjKJoehaIrueJpmeB6HqeaHoWGIXnsTEJY4wxAVlLwhhjTEAWJIwxxgRkQQIQkVUiUi0ie0Xku5Guz3CJyG9FpFFEtvkcyxKRdSKyx/Pv1EjWMVQiMl1EXhaRnSKyXUS+7jk+WZ8nSUQ2iMhmz/P8yHN8Uj4PgIjEisj7IvK05/1kfpaDIrJVRDaJyEbPsUn5PCKSKSKPisguz+/PBaPxLGd8kBCRWODXwAeBKuB6EamKbK2G7ffAqgHHvgu8pKrlwEue95NBH/AtVa0Ezge+7Pn/MVmfpxv4gKouABYCq0TkfCbv8wB8Hdjp834yPwvAZaq60Gc+wWR9nl8Cz6vqXGAB7v9H4T+Lqp7RX8AFwAs+728Dbot0vUbwHKXANp/31UCh53UhUB3pOo7wuf4MXBENzwMkA+8B503W5wGmef7YfAB42nNsUj6Lp74HgZwBxybd8wDpwAE8yUij+SxnfEsCKAZqfN7Xeo5NdvmqWg/g+TcvwvUZNhEpBRYBbzOJn8fTPbMJaATWqepkfp7/AL4DuHyOTdZnAVBgrYi8KyI3eY5NxueZCTiA33m6Au8VkRRG4VksSID4OWZ5wREmIqnAY8A3VLU10vUJh6o6VXUh7k/hS0VkfoSrNCIicjXQqKrvRrouo+giVV2Mu7v5yyJySaQrNEJxwGLgLlVdBJxklLrJLEi4Ww7Tfd5PA+oiVJfR1CAihQCefxsjXJ+QiUg87gCxRlUf9xyetM/jpaotwCu4x48m4/NcBKwWkYPAQ8AHROQBJuezAKCqdZ5/G4EngKVMzuepBWo9rVSAR3EHjbCfxYIEvAOUi0iZiCQA1wFPRrhOo+FJ4AbP6xtw9+1PeCIiwH3ATlX9hc+pyfo8uSKS6Xk9BVgB7GISPo+q3qaq01S1FPfvyV9U9TNMwmcBEJEUEUnzvgZWAtuYhM+jqkeBGhGp8By6HNjBKDyLzbgGROQq3H2tscBvVfX2yNZoeETkQWA57mWBG4B/Av4EPALMAA4DH1fV5ghVMWQicjHwGrCVU/3e38M9LjEZn+ds4H7cP1sxwCOq+mMRyWYSPo+XiCwHvq2qV0/WZxGRmbhbD+DurvmDqt4+iZ9nIXAvkADsBz6P52eOMJ7FgoQxxpiArLvJGGNMQBYkjDHGBGRBwhhjTEAWJIwxxgRkQcIYY0xAFiSMMcYEZEHCGGNMQP8fDg6MbheRggUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "tensor([-0.0002, -0.0566, -0.0184, -0.0087, -0.0452, -0.0060, -0.0095,  0.0134,\n",
      "        -0.0255])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "tensor([ 0.1049, -0.1839, -0.1614, -0.2898, -0.4136, -0.1853, -1.0820,  0.0228,\n",
      "        -0.5383])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "tensor([-99.5010, -99.4361, -99.6603, -99.4261, -99.8748, -99.4784, -99.5205,\n",
      "        -98.9463,  99.8487])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "tensor([-0.0240,  0.0137, -0.0089, -0.0115, -0.0052,  0.0093, -0.0056, -0.0569,\n",
      "        -0.0514])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for day in range(10):\n",
    "    # cut data short so no backwards flow of info\n",
    "    test_day = torch.tensor(fx['validation']['ohlcv'][day][:]).unsqueeze(0).cuda()\n",
    "    test_futures = torch.tensor(fx['validation']['future'][day][:]).unsqueeze(0).cuda()\n",
    "    after = torch.tensor(fx['validation']['ohlcv'][day][21*60:22*60])\n",
    "    with torch.no_grad():\n",
    "        # no access to futures\n",
    "        pred = model(test_day)[0][21*60]\n",
    "    torch.cuda.empty_cache()\n",
    "#     if (pred.abs() >= .9).any():\n",
    "    if True:\n",
    "        print(day)\n",
    "        print((pred.cpu() * 100))\n",
    "        \n",
    "        plt.plot(after.select(dim = 1, index = -1))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([-0., -0., -0., -0., 0., -0., -0., 0., -0.])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA060lEQVR4nO3dd3yc5ZXo8d8zTRp1S5YsyTK25CaDccNgCNhgU0ILJWxI2CTXS0hgk1CTbDZsNtlNdu9NLskmlBACS8iyNwE2EBscCN2mBmyMe8VFLmqWLHlUZ6Qpz/1jimVpJM1oNOWdOd/PR5/RvPPOzPO6zJmnnaO01gghhMhcpmQ3QAghRHJJIBBCiAwngUAIITKcBAIhhMhwEgiEECLDWZLdgLGYOHGinjZtWrKbIYQQhvLxxx8f11qXDj5uyEAwbdo0Nm7cmOxmCCGEoSilDoc7LkNDQgiR4SQQCCFEhpNAIIQQGU4CgRBCZDgJBEIIkeEkEAghRIaTQCCEEBnOkPsIhLHtb+lmf0s3l88tT3ZTEmpHQwev7WyO6NyiHBs3nz8NpVScWyWEBAKRYB1ONyuf2MCxThfb/vUycmyZ80/w/jc+4Y3dLYz22R4sEbJs1kRmlOXHv2Ei42XO/0KRdFpr7l21jQaHE4DNRxycP2NikluVOEfbnVwyZxKPr1w84nlr9xzjK/+1ke4+b4JaJjKdzBGIhHl6w1H+sr2Z25fPwKRgfV17spuUMFprGhxOqibYRz3XbvV/P+vt98S7WUIAEghEgnxyrIsf/XknS2dO5FuXzuKMykLWH2xLdrMSptPpobvPw+Si0QNBjs0MgLNfegQiMSQQiLhzub3c/tQm8rMt/MeN8zGZFOdUF7P5qIM+T2Z82NU7egGYHEGPIBgIeiUQiASRQCDi7scv7uKTY9384sYFlOVnA3BOdTH9Hh/b6juS3LrEqD/hnxeJaGhIegQiwSQQiLh6ZUcTT60/wm3Lalg262Qa9HOmFQNkzPBQQyAQRDY0JHMEIrEkEIi4enjdAWZPyufbl80+5fiEXBuzJ+VnzIRxg8NJttVEca5t1HNDQ0Nu6RGIxJBAIOKmy+VmZ2MHn55bjs0y9J/aOdXFfHz4BB6vLwmtS6yGE04mF9kj2iCWZTGhlAwNicSRQCDiZuPhE/g0LKkuDvv4kppievu97GzsTHDLEq/B4WTyhJyIzlVKkWM1y2SxSBgJBCJuNtS1YzEpFp02IezjoXmCuvSfJ6g/0RvR/ECQ3WaRQCASRgKBiJv1B9uYV1UYWgUzWFlBNtUTc9mQ5vMEvf0eTvS6I1oxFJRjM+OUyWKRIBIIRFw4+71sq+/gnOqSEc9bUl3Mhrp2fD6doJYlXkMUS0eDcmwyNCQSRwKBiIvNR07g8WmW1ISfHwg6p7qYTpeHPc1dMb9fa1dfTK8RL/WOyJeOBtltZpyyakgkiAQCERcf1rVjUnDW1PDzA0HnBCaSN8QwT9DR6+bzj33It5/dOubXiKfQHgLpEYgUJYFAxMWGujZOryygINs64nlVE3KYXGRnw6GxzxO8tL2Jfo+Pdz5pZetRx5hfJ14aHE4sJhXaVR0Ju1Umi0XiSCAQ467P42XzEQdLRpkfCArOE2g9tnmCVZvqmVaSQ1GOlYfW7h/Ta8RT/QknlUV2zKbIi8zIZLFIJAkEYtxtq++gz+MLDfuMZklNMce7+znQ2nPK8Re3NXLjox9wuK1nmGfC4bYeNh4+wY1nT+Er51fzxu5j7EqxfQkNUS4dBRkaEoklgUCMu+By0OA+gdEEVxYFn+fo7eeOpzdz+1Ob2VDXzgNv7hv2uas3N6AUXLdgMis/NY38LAu/Wjf8+cng30wWXSCw28yys1gkjAQCMe7W17Uze1I+EyLIqwMwrSSH0vws1te1sW5vC5f98h1e3t7Ety6dxd99ahovbGnkSFvvkOdprVm9uYHzakqoLLJTaLey8lPTeHlHM/uOxbYKabz0e3y0dPWNrUfg9o55uEyIaEggEOPK4/Xx8aH2iIeFwJ9SYUl1MS9vb+bm331EUY6V5795PndePJOvXzQds0nx67eGjv1vOnKCw229XL9wcujYVy6oxm418/C61JgraOpwonV0K4bAn4HU69P0Z0AeJpF8EgjEuNrZ2ElPv3fU/QODragtw+3zcduyGtbcfgFzJxcCMKkgm88vnsKfNtWHah0HrdrUQLbVxBVnVoSOFefa+PK5U1mztZG648PPLSRKaDNZlD0Cu1VqEojEkUAgxlW08wNB1y+czJYfXMa9V84h23pqSoq/v2g6AI++fSB0rM/j5cVtTVx+Rjl5WZZTzr9laTVWs4lHwvQiEu1kQZrIEs4FSZUykUgSCMS4Wl/XRvXEXMoKIl8zD/7hocKc8HsOJhfZuWFRFc98dJSWThcA6/a00OF0c/2iqiHnl+Vnc9M5p7FqUwNH24fOLSRSvcOJUlBeGN2fh10CgUggCQQiav0eH3c9s5n/98GhU3IE+XyaDXXtw6adjsXXL5qO16d59J2DAPxpUwNl+VmcPz38XoXbLqzBpBQ/+vNOuvuStx6/4YSTSfnZYesxjCRYpUyGhkQiSCAQUfvTpnpe2NLID17Yyf96YgONgbH7Pc1ddLo8UU0UR2pqSS7Xzq/kD+sPs7+li7f2tnDtgkos5vD/hCsK7fzDp2fz5p4WrnjgnaRlOG1w9EY9UQwDh4ZkU5mIPwkEIipur49fv7Wf+VWF/Pt1c/n48Ak+ff87rNpUH6orsKQmsh3F0frG8hn0eXzc/F8f4fZqPhtmWGigry2r4dnbzsOkFJ9/7AP+90u7cCU4kVuDwxn10lEYMDQkiedEAkggEFF5YUsjR9ud3LFiJl86dyqv3L2U2ZPy+dYft/LzV/cyucg+pg++SMwoy+OqMys42u6ktjyfORUFoz5n8bRi/nLnUr645DT+8906PvPQe+xo6BjXdh1u6wkbYLw+TZPDFVOPQIaGRCJIIBAR8/o0v163n9MrCrh4ThngH7L5n9vO494ranF7Nctmlca1DbevmIHZpLhx8ZSIn5ObZeHfrzuTJ79yDp0uN1/67fpxmTdwub386M87ufBnb3HfK3uHPH6s04XHp6OqQxCUY/XPEchksUgECQQiYi9tb+Lg8R7uWDHjlCLsZpPitgun88G9K/jh1afHtQ215QW89Z2LWPmpaVE/98JZpTz25cU4et38vw8Ox9SOLUcdXPngu/zu/UNMKshi9eZ6+j2nbv5qGEMdgiB7qEcgcwQi/iQQiIj4fJpfrd3HzLI8Pn1GedhzSvKyhi1LOZ6mFOdElclzoPlTilg2q5TH3z04ponYfo+PX7y2lxse+SvOfi+/v2UJP/nsmZzodfP2J62nnDuWymRBso9AJJIEAhGR13Yd45Nj3dy+YgamMX4Ip4o7Vsygraefpzccjep5Hb1ubnjkrzy4dj/XLqjklbuXccHMiSydWcrEPBurNtWfcn6wR1A5lh6BVQKBSJyYAoFSqlgp9bpSal/gNmw5KqVUkVLqOaXUHqXUbqXUeQMeu0MptVcptVMpdV8s7RHxobXmobX7qJ6Yy9XzKpPdnJidPa2Yc2uKefTtAxGvItJa849/2sbupk5+86VF/OLGBRTa/RvgrGYTn5lfyZu7W+jodYeeU3/CSXGuLbQnIBomkyLbapJylSIhYu0RfA94U2s9E3gzcD+cB4BXtNa1wHxgN4BSajlwLTBPa30G8PMY2yPiYN3eFnY2dvKNQAK4dHDnipm0dPXx7MbIegV/WH+EV3Y2893LZ3P53Iohj9+wqIp+r48XtzeGjtWf6B3TsFBQjs0i+whEQsQaCK4Fngz8/iRw3eATlFIFwDLgtwBa636ttSPw8NeBn2qt+wKPtcTYHjHOtNY8+OZ+qibYuW5Alk+jO296CWdNncBv3j44ZJJ3sL3NXfzbi7tYNquUr15QE/acMyoLmFmWx+pNDaFjY91DEGS3SnEakRixBoJJWusmgMBtWZhzaoBW4HdKqc1KqceVUrmBx2YBS5VS65VSbyulzh7ujZRStyqlNiqlNra2tg53mhhn7+9vY8tRB1+/aDrWYXbxGpFSittXzKDB4WT15vphz3P2e7n9qU3kZ1v5j8/NH3Z+RCnFZxdVsfHwCQ639aC1pjHGQJAjxWlEgoz6P1sp9YZSakeYn2sjfA8LsAh4RGu9EOjh5BCSBZgAnAv8A/BHNXBd4gBa68e01ou11otLS+O7Vl2c9ODafZQXZPM3Z428i9eILppVyryqQh5edwDPMHn/f/ziLva1dPPLz8+nND9rxNe7bmElSvnTY7f19ONy+8a0mSxIylWKRBk1EGitL9Fazw3z8wJwTClVARC4DTe0Uw/Ua63XB+4/hz8wBB9bpf02AD5gYqwXJcbH+oNtbKhr57YLa8iyxH9ZaKIppbh9+QyOtPeyZmvjkMdf2tbE0xuO8PcXTmfpzNG/fFQU2vnU9BJWb24IpZ+OaWhIegQiQaJfznCqNcBK4KeB2xcGn6C1blZKHVVKzdZa7wUuBnYFHn4eWAG8pZSaBdiA4zG2KWM8+vYBppbkhJ28HA8Prd3PxLwsbjrntLi8fiq4ZM4kasvz+cnLe3h1Z/Mpj/11fxsLphTx7ctmRfx61y+s4jvPbmXNFn9gia1HYKGlyzXm5wsRqVgHfX8KXKqU2gdcGriPUqpSKfWXAefdAfxBKbUNWAD8n8DxJ4AapdQO4BlgpZYirRHp83j5yct7+Pvfb+KuZzafsmxxPGw6coL39h/n1mXVQwrFpBOTSfH9q+ZQmpfF4bbeU37OmFzAQzctjGpu5PK55ditZp7a4N+5HG1BmoHsMjQkEiSmHoHWug3/N/zBxxuBKwfc3wIsDnNeP/ClWNqQqYIf/ItOK+KlbU18eLCN+/5mPheOU66fX63dz4QcK19cMnVcXi+VLZ1ZytK7xufPLS/LwuVzy1m9uYH8LEtor8FY5FhlaEgkRvosA8kwDqc/ENx8fjXPf/N8CrKtrHxiA99fvZ2eGBOq7WjoYO2eFm65oJrcrFhHDzPP9YFltrEMC4FMFovEkUBgUI5Aj6Aox8rcyYX8+Y4LuHVZDU9tOMK/rNkZ02s/tHYf+dkW/tcYErsJOH/GRMoLsplWkjv6ySOw2yzSIxAJIV/3DKoj0CMostsAyLaa+acr53Cs08Vbe1vRWjPMStwR7W3u4tWdx7hzxQwKssc+rJHJzCbF07eeG8oXNFY5NjP9Xh8er2/YSmxCjAf512VQjt5+wN8jGGhJdQnHu/uoO94zptf91br95NrMfOWC6pjbmMmqJ+ZGXbB+sBypUiYSRAKBQQV7BIWDA0GNv17wWGr0Hmzt5sVtjXz5vGkU5dhib6SIiV2qlIkEkUBgUI5eN2aTIn/QZG7NxFwm5tlYP4ZA8MxHRzErxS3SG0gJUpNAJIoEAoNyOPspyLYMmQdQSnFOdXHUPQKvT/P85gaW15aNmkpBJIY9VK5SMpCK+JJAYFCOXvewwzdLqktocDipP9Eb8eu9v/84LV19fDaNMowanRSwF4kigcCgOpzuYTcrnVMd/TzBqk31FGRbWDEnXAJZkQwyNCQSRQKBQXU43UNWDAXNnpRPod3K+oORBYLuPg+v7jzG1fMr0zK5nFHZJRCIBJFAYFCOXjdFw/QITCbF2dOK2XAoskDwyo5mnG4vNyySYaFUEixx6XTLHIGILwkEBuXo7R9xieeS6mLqjvfQ0jl69spVm+qZWpLDotPClpwWSSJDQ5nhSFsvT60/MmqlvHiSQGBAXp+m0+WhYISEZsF5gtGWkTY6nHxwsI3rF04e005kET+yjyAz/PcHh/in1du59uH32d3UmZQ2SCAwoM5QeonhA8EZlQXk2syjThg/v6UBrU8mShOpI8cqPYJM0Olyk2sz09rVxzW/eo9fv7Ufry+x2fglEBhQKM/QMJPFABazibOmjbyfQGvN6k0NLJ46gakxJkgT489iNmEzmyQQpLkul4fKIjuv3bOMS0+fxH2v7OVzv/kr+1u66fN4h/z44hAkJOmcATkiCATgnyf42at7ae/ppzh36HzCjoZO9rV087+vnxuXdorY+ctVymRxOuvu85CXbaE418bDf7uINVsb+cHzO7jkF2+HPf+/bj6bi2aP7zJvCQQGFEw4V2gfOR/QksA8wUeH2vn0GeVDHl+1uR6b2cTVZ1aOfyPFuJCaBOmva8B8n1KKaxdMZkl1CWu2NuD2Dv32Xz1x/HvvEggMKJKhIYAzqwrJspjYUDc0ELi9PtZsaeTiOWVDEteJ1GG3mSX7aJrrcrmZXHRqEaPywmxuXTY9YW3IqDmCDXXtPPFe3ZjG2LTWPPdxfejbeDIFi9KMVgYxy2Jm4WlFrK9rG/LYG7uO0dbTz2cXVcWljWJ85NikXGW66+7zkJfkSoAZFQjWbG3gxy/u4ouPr48qDw/A/pZuvvPsVv7vK3vi1LrIhVJQR1APd0l1CbsaO+l0+Z/j8fp4eN1+7nxmM1OK7eNW41jER47VIknn0ly3y0N+tgSChPm3a+fyf284k231Di6//13+uPEoWkfWO9gVWN/73Mf1NDic8WzmqBy9bvKyLFgjqFq1pLoYn4aPD5/gYGs3n3v0A3726l4uO6OcNd+8AJslo/4JGI5degRpzevT9PR7yZNAkDhKKT5/9mm8cvcyzqgs4LvPbeNr//0xrV19oz53d1MXFpNCa3js7QMJaO3wHM7+iHoDAAtPm4DVrLj/jX1c+eC7HGzt4cGbFvLw3y5iQpiVRCK1yGRxeuvu8/f2ZGgoCaYU5/D0187ln6+awzv7Wrn6oXdH7X7vbupkRlkeNyyq4umPjkaUuiFeOnqHTzg3mN1mZl5VEVuPOji3poTX7lnGNfNllZBR2CUQpLWuwJBtsuuDZ2QgAH9itq8ureGBzy/gWGcf2+s7Rjx/d1Mnp1cU8I3l0/F4ffznuwcT1NKhHCOkoA7nR9ecwa+/uIjf/d3ZTCqIrY6uSKwcmxmnrBpKW6EegQwNJVcwJ8/Wesew57R199HS1cecigKmluRy7YLJ/P7DI7R1jz6kFA/+hHORB4K5kwu58swKySVkQDk2mSxOZ90u/9+tTBYnWUleFpOL7GwdoUewp7kLgDkVBQB8c/kMXB4vv32vLiFtHKzD6Rl1M5lID3arGZfbF5e0AiL5ulwyR5AyFkzxj6EPJ5gRcE5FPgAzyvK48swK/vuDw3QE1vQnitaaDmd0PQJhXKFylTI8lJa6+qRHkDLmVRVSf8I57FDPrqZOSvOzKMk7WdT99uUz6O7z8Lu/JrZX0Nvvxe3VI2YeFelDahKkt+Bkcb5MFiff/ClFAGwbZnhod1NXaFgoaE5FAZeePokn3qsL/WUmQqQJ50R6sAerlEkgSEvdMjSUOuZOLkSp8BPGbq+P/S1doWGhge5cMZNOl4d/fn4HPX2JmdA7mXBOAkEmCPUIpFxlWuru82BSJ/+ek0UCAf5oPKM0L2yP4EBrN26v5vRBPQLwJ3W7c8UM1mxt5IoH3uWjCGsEx6IjlGdIJoszgRSwT29dLn+eoWSv6JNAEDA/MGE8OOXEyYnioYEA4FuXzeaZr52LRnPjox/wk7/sxhXHib1IM4+K9BCsUiZDQ+mpy+VJ+vwASCAImV9VSFtP/5A8QruburCZTSPmAF9SU8LLdy3jC2efxqPvHOSaX73HjoaRN6iNlcwRZJacwByB9AjSU3efO+krhkACQUhwwnjr0VM/wHc3dTJzUt6oCd7ysiz85LNn8rubz8bR6+a6h9/nwTf34fH6xrWdwRTURTI0lBFODg3JHEE6Cg4NJZsEgoDa8gJsZhPbBk0Yh1sxNJLls8t47Z5lXDWvgl+8/gk3POKvPTpeHM5+bBYT2Vb5q8sEoX0E0iNIS8EylckmnyYBNouJOZUFbBmwsay1q4/j3X1RBQKAohwbD3zBn+HzSHsvVz347pgL4gzW0evPM5TsySWRGLKPIL2lxRyBUqpYKfW6Umpf4HbCMOcVKaWeU0rtUUrtVkqdFzi+QCn1oVJqi1Jqo1LqnFjaE6v5VYXsaOjAG/jAHryjOFpXzavg1XuWccGMifz4xV387eMfcrQ9uoI4g3U43bKZLIPYZWdxWkuXoaHvAW9qrWcCbwbuh/MA8IrWuhaYD+wOHL8P+JHWegHww8D9pJlXVURPv5cDrf6hnD3NgUBQHl2PYKCy/GweX7mY+26Yx46GTq544F3++FHkBXEGc0SRgloYn81swmxSMkeQprr73BSkwdDQtcCTgd+fBK4bfIJSqgBYBvwWQGvdr7V2BB7WQPBTthBojLE9MVkwpRAglHdod1MX5QXZMRdwUUpx49lTePmupf6COH/axlef3EhLV/Q1DfwpqGWiOFMopcixSk2CdOT2+nC5fWnRI5iktW4CCNyWhTmnBmgFfqeU2qyUelwpFVyLeTfwM6XUUeDnwL3DvZFS6tbA8NHG1tbWGJsdXs3EPPKyLKGNZbubOsc8LBROsCDOD64+nff2H+fTv3yHV3Y0R/UaHVGmoBbGJ+Uq01MovYQRegRKqTeUUjvC/Fwb4XtYgEXAI1rrhUAPJ4eQvg7co7WeAtxDoNcQjtb6Ma31Yq314tLS+BRcN5kUZ04uZGu9gz6Pl/0t3VFPFEfyHrdcUM1Ldy5l8gQ7dzy9KVRYPhLRFqURxiflKtNTV6gWQfL/P48aCLTWl2it54b5eQE4ppSqAAjctoR5iXqgXmu9PnD/OfyBAWAlsCrw+7NAUieLAeZNKWR3Uye7m7rw+PS4B4KgGWV5fPvS2bi9mr2Begej6ff46O33ymRxhrHbLBII0lBXn/8LYDoMDa3B/2FO4PaFwSdorZuBo0qp2YFDFwO7Ar83AhcGfl8B7IuxPTGbX1WE26tZvakeGPuKoUjUBl47uDppNJJeIjP5y1XKZHG6CQ4NpcJkcawt+CnwR6XULcAR4HMASqlK4HGt9ZWB8+4A/qCUsgEHgZsDx78GPKCUsgAu4NYY2xOz4A7j1ZsbyLKYmFYyfGqJWJUXZFOUY40iEAQyj+bIZHEmybGZQ7VtRfroSqE5gphaoLVuw/8Nf/DxRuDKAfe3AIvDnPcecFYsbRhvlYXZTMyzcby7n3lVhVhGSS0RC6UUc8oL2NUU2dDQyfQS0iPIJHarmdau5NTHFvETKlyfBkNDaUcpxfyqIiC2/QORmlNRwCfNXaFNbCMJBQIZGsooMlmcnk6WqUz+/2cJBGHMCwSC2jjODwTVVuTjdHs53NYz6rnBzKOyaiizyGRxejpZplJ6BClpSU0xAAtPC5sxY1wFC97sjmB4KFidTDKPZpYcmxmn7CxOO90uDxaTIsuS/I/h5LcgBZ1bU8I7/7CcBYGJ43iaUZaH2aQimjDudLpRKjW+QYjEybGZ6XV7x5yWRKQmf8K55FcnAwkEwzqtJCch75NtNTO9NDeiQBDcTGYyJf8fjkgcu82M1tDnGd/aFiK5UiUFNcS+fFSMg9ryAj4+fGLU8xy9knk0EwXLVfb2e8m2JrfI+Wh+/ureU1K5j2TBlCK+8+nZo5+YprpcHvKzUuP/s/QIUsCcigIaHM5QYfrhOJxu2UOQgU6Wq0zteQKfT/PYOwc52NqN0+0d8efoiV4efms/bd2Zuyy2y+WWHoE4Kbh7eXdzJ+fWlAx7XkdvvwSCDGQ3SJWytp5++r0+brtwOis/NW3Ec7fXd/CZX73H25+08tlFVYlpYIrp7vNQXpCd7GYA0iNICSdXDo08T+CQojQZyShVypo6nABUFI7+4XZGZQGl+Vms3RMuPVlmCE4WpwIJBCmgND+L4lzbqIGgwylFaTKR3TCBwF9fo7LIPuq5JpPiolmlvPNJKx5vZk6Cp9JksQSCFKCUYk5FPntGyELq82kpU5mhgnMEqZ54rsnh7xGUR9AjAFhRW0anyxPRQol01J0i9YpBAkHKmFNewN7mrmG/HXW5PGgtCecykXGGhlzYLCZKIqzod8HMiVjNirV7M294yOX20u9NjepkIIEgZcypKKDP4+PQMKkmHMHMo9IjyDh2qzECQWOHi4rC7Ig3SOVnWzl7WjHrMnCeoDuUZ0gCgRggmNdouEykknk0c+UYZNVQk8MZ0UTxQCtqy/jkWDf1J3rj1KrU1O2SQCDCmFGWh2WEVBNSlCZzndxHkOKBoMNFZeHoE8UDXTTbX+Z83d741CFPVaFaBLKhTAyUZTEzoyyPPcMEAocEgoyVbTWhFCmdeM7r0xzrdEU8URw0vTSX04pzMm54KJXKVIIEgpQyp6Jg2CykHb3BOQKZLM40Sins1tSuSXC8uw+PT1MRwdLRgZRSrKgt468HjuNyp+71jbcuGRoSw6ktz6e508WJnv4hjwXnCGSyODMFM5CmqsbA0tHKKHsEAMtry3C5fXxwoG28m5WyZI5ADGvOCDuMHU43OTYzthTIXS4Sz24zp/RkcXAzWUWUcwQAS6qLsVvNGbXLuDuFqpOBBIKUEgwEu8IFAsk8mtFyrJaUTjp3MhBE3yPItpo5f8ZE1u1tyZiaC8HqZLlZqZFNVgJBCinNz2JiXlbYHcYdknk0o9lTvG5xk8NJttU05sUMy2tLqT/hZH9L9zi3LDV19XmwWUxkWSQQiDDmVOSHHRrqcPZLjyCD5RhgaKiy0D7malvLA8tIM2V4qMvloSBF5gdAAkHKmVNRwL5j3bgHpZpw9ErCuUyWk+I9gsYOJxVFY0+pXFlkp7Y8P2MCQbfLkzJLR0ECQco5o7KAfq+Pn7+2l/4BpQkdknk0o9ltFpwpvGqoyeGivCD6ieKBVtSWsfHwidBEajrr7kudhHMggSDlXDG3ghsWVfHo2we57uH32dPcidaajl43BTI0lLFyrOaUnSz2eH20dLmojKFHAHB2dTFen2Z7fcc4tSx1dbnc0iMQw7NZTPzHjfN57Mtn0dLl4pqH3ufBN/fT7/VRJJvJMlYqTxa3dPXh02NbOjrQ/KoiALbVO2JvVIrrcqVOLQKQQJCyLjujnFfvXsaK2jJ++cYngKSXyGSJmize39LF7z88jM8X+TLOUGWyGHsExbk2phTb2ZoBgcA/NJQ6gSB1WiKGKMnL4pEvLeL5LQ38et0BFp5WlOwmiSTJsZnx+DT9Hl9cNhX6fJon3q/jvlf9c1NzKvI5a2pxRM9tdAQqk8XYIwCYV1XEliOOmF8n1XW5POTL0JCIlFKK6xdW8fq3LqS2vCDZzRFJYg9WKYtDr+Boey83/eeH/PtLuzl/eglmk4pq9U5zYDNZtAnnwllQVUSDw8nx7r6YXytVaa1TqkwlSCAQwhBCVcrGsVyl1ppnNhzh8vvfYWdjJ/f9zTye+LuzOWvqBNbuiTwtdGOHk1ybeVzWxc+rKgTSe57A6fbi9WlZNSSEiE405Sq//Nv1fPHxDznaPnyxl5ZOF7c8uZHvrdrOvKoiXrl7KTcunhLKBrq7qTM09j+aJoeLiqKxbyYbaO7kQkwKth5N35VD3aFaBNIjEEJEIViucrShoePdfby77zjv72/j8vvf4X8+OjIkf8+L2xq57P53eH//cf7lM6fzh68uoWpCTujxFbX+Xb5vRVgspqkj+spkw8nNsjCzLD+tJ4y7UqxMJUggEMIQIq1S9lFdOwAP3bSQM6sK+cc/beeWJzfS0unC0dvPHU9v5vanNjO1JJeX7lzKzedXYzKd+k1+Zlkek4vsEc8TBGsVj5f5UwrZVt+RtgnoUq0WAciqISEMwR4aGhp5jmB9XTt2q5nL55Zz1ZkVPPnBIX768h4uu/8dbGYT7T39fPvSWXz9oulYzOG/ByqlWF5byqpNDfR5vCMmRuv3+Dje3RfzHoKB5lUV8ceN9dSfcDKlOGf0JxhMd4qVqQTpEQhhCGX5WQAcGWHcH/yB4KypE7CaTZhMipvPr+Yvdy2lZmIuxbk2nv/m+dxx8cxhg0DQitoyevu9rD/YPuJ5xzpdaE3Mu4oHCm4sS9fhoWAK6lTqEUggEMIAqibYmVoycm3fjl43e5o7Oaf61PX/00vzWPWN83n5rqXMnVwY0fudVzORLItp1OGhWArSDGd2eT42i4mtRx3j9pqpJDhHkDaTxUqpYqXU60qpfYHbCWHOma2U2jLgp1MpdXekzxdCBIZrZpfx1wNtw04Yf3SoHa0ZEggGvkak7DYzn5peMmqxmNCu4nGcI7BZTJxeUcDWNM05FBwaKkij5aPfA97UWs8E3gzcP4XWeq/WeoHWegFwFtALrI70+UIIvxW1ZfR5fHxw8HjYxzccasdmNrFgStG4vd/htl7qjvcMe05wV3G0RetHs2BKETsaOvBGkerCKIKTxalSnQxiDwTXAk8Gfn8SuG6U8y8GDmitD4/x+UJkrCU1xeTYzKwbZrPX+rp2FkwpIts6Ph8wF0VQLKa5w0l+tmXchznmVRXS2+9Ny4pl3X1u7FbzqPM0iRRrSyZprZsAArdlo5z/BeDpsTxfKXWrUmqjUmpja2vkux6FSBdZFn9t37V7hg7XdPd52NHQMeyw0FhMKc5hZlke6/YOHwgaA5XJxtv8QK8mHSeMUy3hHEQQCJRSbyildoT5uTaaN1JK2YBrgGfH0lCt9WNa68Va68WlpaVjeQkhDG9FbRkNDif7Bn1T3nT4BF6fHtdAEHy/DXXtwxaLaepwjkuOocGqS3LJz7Kk5YRxZ4qloIYIAoHW+hKt9dwwPy8Ax5RSFQCB25GWGFwBbNJaHxtwLJrnC5HxLprt/xI0eLhmQ107ZpPirKnju95ieW0Zbq/mvX3he+FNjtgL0oRjMinmBTaWpZvuFMs8CrEPDa0BVgZ+Xwm8MMK5N3HqsFC0zxci41UU2plTUTAkEKyva2Pu5EJyx/kD5qypE8jPtoSdl3C5vbT19I/r0tGB5lUVsbupE1cKl+gciy6XO6USzkHsgeCnwKVKqX3ApYH7KKUqlVJ/CZ6klMoJPL4qkucLIYa3oraUjw+foKPXvzHJ5fay9WgHS8Z5WAjAajaxbFZp2GWkxzqDewjGv0cA/o1lHp9md1NnXF4/Wbr7UqtwPcQYCLTWbVrri7XWMwO37YHjjVrrKwec16u1LtFad0TyfCHE8FbUluH1ad7d7/+WvuWog36vLy6BAGD57DJauvr4cNAu41BBmnFeOho0f0owJXV6DQ91uww4WSyESC0LpkxgQo41NDy0/mA7SsHiCCuKRevTZ0xicpGd7/5pK52B9AhwcjNZPCaLAcoLsinLz0q7CeNUq1cMEgiEMByzSXHhrFLe3tuKz6fZcKiN2vICCuNU0zo/28qDNy2g0eHi+6t3hIaIgukl4rF8FPw7oedVFaXVElKfT9Pdn36TxUKIJFheW0ZbTz8fHznBx4dPxG1YKOisqcXcc8lM/ry1kWc31gPQ6HBSlGMNZUaNh/lVhRxo7TmlJ2JkPf0etCbtJouFEElw4axSTAoefHMfLnf85gcG+vpFMzivpoR/WbOT/S1dNHe44rZiKGjhaf7lsB8fOhHX90mU4H4MGRoSQsSsKMfGotMm8O4+f96hsxMQCMwmxf1fWIDdZub2pzZzqK0nbiuGghZPm4Ddah5xd7ORpGKZSpBAIIRhLQ+UlJxRlsfEvKyEvOekgmx+/rl57Gnu4kBr/ANBttXM+TNKwqbVMKLOFKxOBhIIhDCsYG3h8U4rMfr7TuKWC6qB+C0dHWh5bRn1J5wcaDV+ArruFKxXDFKqUgjDqi3P555LZnHFmeUJf+/vXj4bq9nEVWdWxP29lg/IgjqjLD/u7xdP3aEegUwWCyHGgVKKuy6ZyaxJif9wzLKY+d4VtUybmBv396osslNbnj9qtTQjCJapTLU5gtRqjRBChLG8toz/fOcgnS53Uit7+XyaDw+2hcpNBpmUYklN8ahtS9VVQ6nVGiGECGNFbRmPvHWAdz85zlXz4j8cFU6jw8l3n9vGe/vDV4irKMzmvr+Zx9KZw6fJD04W59lS66M3tVojhBBhLJxSRKHdn1Yj0YFAa82qTQ3865934vVp/u3aM1g0KN13W3c/P35xF1/+7Qa+fO5U7r2ylpwwH/bdLn/COZMp8vrRiSCBQAiR8ixmkz+txict+Hw6YR+kx7v7+KdV23lt1zHOmVbMzz83n9NKcsKe++IdF/CzV/fyxPt1vLuvlf+4cT5nDcr/1N3nTrkVQyCBQAhhECtqy1iztZHtDR2hUpbx9MqOZr6/ejtdLg//dGUtt1xQg3mEAJRtNfODq0/n0tMn8Z1nt/K533zAmZMLUerkcw619VCaoD0f0ZBAIIQwhAtnlaKUfxlpPANBh9PNj9bsZNXmBs6oLODpWxdEtTLr3JoSXrl7Gb98/ZMhJUXnVRVxyZzRSrsnngQCIYQhTMi1sXBKEev2tnDPpbPi8h7v7mvlu89to6WrjzsvnskdK2ZgNUe/yj4vy8IPrj49Di2MD9lHIIQwjBW1ZWyr76ClyzWur+tye/nhCzv48m83kGMzs+rrn+Jbl84aUxAwosy4SiFEWgjmV3pr79AayrH49VsH+O8PDnPLBdW8dOfShMxBpBIJBEIIwzi9ooDygmzWjfMu49d2NrOkupgfXH062db41VdIVRIIhBCGoZRieW0p7+47jtvrG5fXbOpwsqe5K5TELxNJIBBCGMry2WV093n46FD7qOf+/sPDvLClYcRz1u3xDzNJIBBCCINYPM2/SWtPU9eo5z76zgH+/aXdeEboPazd00LVBDszyvLGrY1GI4FACGEoE3KsZFlMNHU4RzzP59M0d7ho7eobNj+Qy+3l/f3HWT677JSNX5lGAoEQwlCUUlQW2WnqGHkJaVtPP26vv6rZ6s3hh4fW17XjdHszelgIJBAIIQyoojB71EAQ7DFMLrLz6s7mUArogdbtaSHbauK86SVxaadRSCAQQhhOeWE2TY6Rh4YaHf5AcduFNbjcPl7e3nTK41pr1u5p4VPTJ2bkktGBJBAIIQynstDOsa4+vL7hC9oHewRXzK1gWkkOqzadOjx08HgPR9p7WT57+PoBmUICgRDCcCqKsvH69IipJpo6XNgsJibm2bh+YRUf1rXRMKAXEdyUtjzD5wdAAoEQwoAqC+0AI84TNHW4qCjMRinF9QsnozU8P2DSeO2eFmZNyqNqQvj6AplEAoEQwnAqirIBaHKMEAgcTsoL/OedVpLD2dMmsHpzA1prulxuNtS1S28gQAKBEMJwKgqCPYLhJ4ybOlxUFtlD9z+7qIr9Ld1sb+jgvX3H8fg0K2ZLIAAJBEIIAyqwW8ixmUMrgwbz+jTNnf6hoaArz6zAZjGxalMDa/e0kJ9tGVJ7OFNJYRohhOEopagozKa5M3yP4Hi3f0VRxYAeQaHdyqVzJrFmayMmpVg2qzRj6g2MRv4UhBCGVFlkH7ZH0BhYHVQ5oEcAcP3CybT39HO8u0+GhQaQQCCEMKTyguxh5wiCq4nKBwWCC2eXUpxrQym4SPYPhMjQkBDCkCqK7LR09eH2+oYM8ZzsEdhPOW41m/jGRdPZd6ybkryshLU11cUUCJRSxcD/ANOAQ8CNWusTg86ZHTgnqAb4odb6fqXUz4DPAP3AAeBmrbUjljYJITJDZWE2WsOxTteQvQBNHS6yrSaKcqxDnvfVpTWJaqJhxDo09D3gTa31TODNwP1TaK33aq0XaK0XAGcBvcDqwMOvA3O11vOAT4B7Y2yPECJDBCeCm8NsKmvucFFZaM/o1NLRiDUQXAs8Gfj9SeC6Uc6/GDigtT4MoLV+TWsdTAn4IVAVY3uEEBkiuDS0MUwgaOxwhjadidHFGggmaa2bAAK3o03DfwF4epjHvgK8PNwTlVK3KqU2KqU2tra2jqmxQoj0EQwE4bKQNjlclBfYhxwX4Y06R6CUegMoD/PQ96N5I6WUDbiGMMM/SqnvAx7gD8M9X2v9GPAYwOLFi4dPOSiEyAj52VbysyxD8g15vD5aulxUSo8gYqMGAq31JcM9ppQ6ppSq0Fo3KaUqgJYRXuoKYJPW+tig11gJXA1crLWWD3ghRMQqirJDK4SCjnX14dNQUSg9gkjFOjS0BlgZ+H0l8MII597EoGEhpdTlwD8C12ite2NsixAiw5QX2mnuPLVH0BzYWyBzBJGLNRD8FLhUKbUPuDRwH6VUpVLqL8GTlFI5gcdXDXr+r4B84HWl1Bal1G9ibI8QIoNUFmYP2V0cvD94D4EYXkz7CLTWbfhXAg0+3ghcOeB+LzCkKKjWekYs7y+EyGwVhXaOd/fR5/GSZfGXmwzuNh68q1gMT1JMCCEMKzj8c6yjL3Ss0eEi12amIFsSJ0RKAoEQwrBCS0gH5Bxq7nBRUSSbyaIhgUAIYVgVYUpWNnU4T6lDIEYngUAIYVjBvQKNA3oEjR0uCQRRkkAghDCsHJuFQrs1VLu43+PjeHef7CGIkgQCIYShVRSerEtwrNOF1siu4ihJIBBCGJo/EPh7BMFb6RFERwKBEMLQKorsAwJBoCCN9AiiIoFACGFolYXZtPf043J7Q7uKy6VHEBUJBEIIQxu4hLSpw0l+toW8LNlMFg0JBEIIQxtYl6DR4ZIcQ2MggUAIYWjBkpVNHS6aO6Uy2VhIIBBCGNrANBNNDpesGBoDCQRCCEPLtpopzrVRd7yXtp5+2VU8BhIIhBCGV16QzeajJwAkEIyBBAIhhOFVFmVzsLUn8LsMDUVLAoEQwvAGzgtIjyB6EgiEEIY3cKWQTBZHTwKBEMLwgr2Aohwrdps5ya0xHgkEQgjDC/YCpDcwNhIIhBCGF9xNXCnzA2MigUAIYXiTCrMAZFfxGElmJiGE4WVZzPzzVXM4t6Yk2U0xJAkEQoi08NWlNclugmHJ0JAQQmQ4CQRCCJHhJBAIIUSGk0AghBAZTgKBEEJkOAkEQgiR4SQQCCFEhpNAIIQQGU5prZPdhqgppVqBw2N8+kTg+Dg2J9nS6XrS6VpArieVpdO1QOTXM1VrXTr4oCEDQSyUUhu11ouT3Y7xkk7Xk07XAnI9qSydrgVivx4ZGhJCiAwngUAIITJcJgaCx5LdgHGWTteTTtcCcj2pLJ2uBWK8noybIxBCCHGqTOwRCCGEGEACgRBCZLiMCgRKqcuVUnuVUvuVUt9LdnuipZR6QinVopTaMeBYsVLqdaXUvsDthGS2MVJKqSlKqXVKqd1KqZ1KqbsCxw13PUqpbKXUBqXU1sC1/Chw3HDXMpBSyqyU2qyUejFw37DXo5Q6pJTarpTaopTaGDhmyOtRShUppZ5TSu0J/P85L9ZryZhAoJQyAw8DVwCnAzcppU5Pbqui9l/A5YOOfQ94U2s9E3gzcN8IPMC3tdZzgHOBbwb+Pox4PX3ACq31fGABcLlS6lyMeS0D3QXsHnDf6NezXGu9YMB6e6NezwPAK1rrWmA+/r+j2K5Fa50RP8B5wKsD7t8L3Jvsdo3hOqYBOwbc3wtUBH6vAPYmu41jvK4XgEuNfj1ADrAJWGLkawGqAh8oK4AXA8eMfD2HgImDjhnueoACoI7AQp/xupaM6REAk4GjA+7XB44Z3SStdRNA4LYsye2JmlJqGrAQWI9BrycwjLIFaAFe11ob9loC7ge+C/gGHDPy9WjgNaXUx0qpWwPHjHg9NUAr8LvAsN3jSqlcYryWTAoEKswxWTubZEqpPOBPwN1a685kt2estNZerfUC/N+kz1FKzU1yk8ZMKXU10KK1/jjZbRlH52utF+EfGv6mUmpZshs0RhZgEfCI1noh0MM4DGllUiCoB6YMuF8FNCapLePpmFKqAiBw25Lk9kRMKWXFHwT+oLVeFThs2OsB0Fo7gLfwz+UY9VrOB65RSh0CngFWKKV+j3GvB611Y+C2BVgNnIMxr6ceqA/0OAGewx8YYrqWTAoEHwEzlVLVSikb8AVgTZLbNB7WACsDv6/EP9ae8pRSCvgtsFtr/YsBDxnuepRSpUqposDvduASYA8GvBYArfW9WusqrfU0/P9P1mqtv4RBr0cplauUyg/+DlwG7MCA16O1bgaOKqVmBw5dDOwi1mtJ9uRHgidargQ+AQ4A3092e8bQ/qeBJsCN/5vBLUAJ/km9fYHb4mS3M8JruQD/0Nw2YEvg50ojXg8wD9gcuJYdwA8Dxw13LWGu7SJOThYb8nrwj6tvDfzsDP7fN/D1LAA2Bv69PQ9MiPVaJMWEEEJkuEwaGhJCCBGGBAIhhMhwEgiEECLDSSAQQogMJ4FACCEynAQCIYTIcBIIhBAiw/1/4BnO3u+xgSkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "tensor([0., 0., -0., 0., -0., -0., -0., -0., -0.])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for day in range(2):\n",
    "    # cut data short so no backwards flow of info\n",
    "    test_day = torch.tensor(fx['validation']['ohlcv'][day][:21*60 + 1]).unsqueeze(0).cuda()\n",
    "    after = torch.tensor(fx['validation']['ohlcv'][day][21*60:22*60])\n",
    "    with torch.no_grad():\n",
    "        # no access to futures\n",
    "        pred = model(test_day)[0][-1]\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "#     if (pred.abs() >= .9).any():\n",
    "    if True:\n",
    "        print(day)\n",
    "#         plt.pcolormesh(pred.cpu().unsqueeze(0))\n",
    "        plt.show()\n",
    "        print((pred.cpu() * 100).round())\n",
    "        \n",
    "        plt.plot(after.select(dim = 1, index = -1))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: graph these instead of showing raw data (maybe even on same plot or at least side by side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = fx['train'],\n",
    "    eval_dataset = fx['validation'],\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "soft_profit, soft_trade = trainer.predict(fx['validation']).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32432432432432434"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(soft_profit.sum(axis = (1, 2)) < 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11143883"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_profit.sum(axis = (1, 2)).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(soft_profit.sum(axis = (1, 2)), bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.,  0.,  0.,  3.,  5.,  4.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        1.,  1.,  0.,  0.,  0.,  1.,  2., 12., 51., 63., 21.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full trade percent on 24 hours\n",
    "(((np.abs(soft_trade) > .2) & (np.abs(soft_trade) < 10)).mean(axis = (0, 2)).reshape(-1, 60).mean(axis = 1) * 100).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-f6be64cbd057>:2: RuntimeWarning: Mean of empty slice\n",
      "  np.nanmean(np.where(((np.abs(soft_trade) > .2) & (np.abs(soft_trade) < 10)), soft_profit > 0, np.nan), axis = (0, 2)).reshape(-1, 60).mean(axis = 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([       nan,        nan,        nan, 0.56027644, 0.53576666,\n",
       "       0.53156235,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "       0.55270133, 0.59862689, 0.53907986, 0.36340438])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full trade accuracy on 24 hours\n",
    "np.nanmean(np.where(((np.abs(soft_trade) > .2) & (np.abs(soft_trade) < 10)), soft_profit > 0, np.nan), axis = (0, 2)).reshape(-1, 60).mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-e15bc04a1bb7>:2: RuntimeWarning: Mean of empty slice\n",
      "  np.nanmean(np.where(np.abs(soft_trade) > .2, soft_profit, np.nan), axis = (0, 2)).reshape(-1, 60).mean(axis = 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([            nan,             nan,             nan,  1.19459346e-05,\n",
       "       -1.50452900e-06,  1.25091155e-05,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "                   nan,             nan,             nan,             nan,\n",
       "       -5.69391432e-06,  2.98715604e-05,  6.57019045e-05,  1.34570055e-05],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full trade profit on 24 hours\n",
    "np.nanmean(np.where(np.abs(soft_trade) > .2, soft_profit, np.nan), axis = (0, 2)).reshape(-1, 60).mean(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03007695, 0.03849474, 0.04355293, 0.0473536 , 0.05259009,\n",
       "       0.05819257, 0.05682245, 0.04026839, 0.02685811])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percent full trades on all timeframes\n",
    "(np.abs(soft_trade) > .3).mean(axis = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.30826833, 0.4141882 , 0.46907994, 0.49762188, 0.5442541 ,\n",
       "       0.56684406, 0.57175888, 0.57772081, 0.56079665])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full trade accuracy on all timeframes\n",
    "np.nanmean(np.where(np.abs(soft_trade) > .3, soft_profit > 0, np.nan), axis = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.3517876e-05, 1.1040257e-04, 1.2627446e-04, 1.3709566e-04,\n",
       "       1.4370347e-04, 1.5203208e-04, 1.7406879e-04, 2.0122110e-04,\n",
       "       2.4339784e-04], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full trade gain on all timeframes\n",
    "np.nanmean(np.where((np.abs(soft_trade) > .3) & (soft_profit > 0), soft_profit, np.nan), axis = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.33010820e-05, -8.14286104e-05, -8.72624005e-05, -9.02208776e-05,\n",
       "       -9.90652115e-05, -1.06105064e-04, -1.28709653e-04, -1.66588317e-04,\n",
       "       -1.84737873e-04], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# full trade loss on all timeframes\n",
    "np.nanmean(np.where((np.abs(soft_trade) > .3) & (soft_profit < 0), soft_profit, np.nan), axis = (0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del trainer\n",
    "# del model\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGConv Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='243' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 243/8804 01:30 < 53:23, 2.67 it/s, Epoch 0.03/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.245500</td>\n",
       "      <td>1.132017</td>\n",
       "      <td>0.1147</td>\n",
       "      <td>0.1439</td>\n",
       "      <td>99.1161</td>\n",
       "      <td>99.4033</td>\n",
       "      <td>49.2069</td>\n",
       "      <td>1.0193</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>46.9388</td>\n",
       "      <td>0.6161</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>85.7143</td>\n",
       "      <td>1.0415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-fdc73cc0e116>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# raw standardized trade loss, losses + .1 (summed per day per timeframe)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1748\u001b[0m             \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1749\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1751\u001b[0m                 \u001b[1;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    679\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa: F811\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[1;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2356\u001b[1;33m         return self._getitem(\n\u001b[0m\u001b[0;32m   2357\u001b[0m             \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2358\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[1;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[0;32m   2339\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2340\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2341\u001b[1;33m         formatted_output = format_table(\n\u001b[0m\u001b[0;32m   2342\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2343\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[0mpa_table_to_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_names\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m         \u001b[0mformatted_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table_to_format\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquery_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformatted_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMutableMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mRowFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumnFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchFormat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"row\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"column\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mformat_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPythonFormatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFormatter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mformat_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython_arrow_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m             \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython_features_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mextract_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPythonArrowExtractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseArrowExtractor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pydict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36m_unnest\u001b[1;34m(py_dict)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_dict\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;34m\"\"\"Return the first element of a batch (dict) as a row (dict)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpy_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 2e-4, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# raw standardized trade loss, losses + .1 (summed per day per timeframe)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='496' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 496/8804 02:44 < 45:58, 3.01 it/s, Epoch 0.06/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.103200</td>\n",
       "      <td>0.839063</td>\n",
       "      <td>0.2513</td>\n",
       "      <td>0.1471</td>\n",
       "      <td>99.3798</td>\n",
       "      <td>99.4084</td>\n",
       "      <td>49.635</td>\n",
       "      <td>1.0310</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>64.2857</td>\n",
       "      <td>6.3325</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>100.</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.863500</td>\n",
       "      <td>0.838906</td>\n",
       "      <td>0.2515</td>\n",
       "      <td>0.1472</td>\n",
       "      <td>99.4069</td>\n",
       "      <td>99.4101</td>\n",
       "      <td>49.6352</td>\n",
       "      <td>1.0311</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-bb1c47517fd0>:27: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit < 0)].mean(),\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:22: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit < 0)].mean(),\n",
      "<ipython-input-2-bb1c47517fd0>:25: RuntimeWarning: Mean of empty slice.\n",
      "  'small trade accuracy': (soft_profit[(abs_trade < .4) & (abs_trade >= .2)] > 0).mean() * 100,\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:26: RuntimeWarning: Mean of empty slice.\n",
      "  'small trade g/l': soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit > 0)].mean()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e5269718eda4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# log standardized trade loss, losses + 1 (summed per day per timeframe)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1748\u001b[0m             \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1749\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1751\u001b[0m                 \u001b[1;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    679\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 723\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    724\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\u001b[0m in \u001b[0;36mpin_memory\u001b[1;34m(data, device)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;31m# The mapping type may not support `__init__(iterable)`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;31m# The mapping type may not support `__init__(iterable)`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\pin_memory.py\u001b[0m in \u001b[0;36mpin_memory\u001b[1;34m(data, device)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 2e-4, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# log standardized trade loss, losses + 1 (summed per day per timeframe)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='670' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 670/8804 03:29 < 42:30, 3.19 it/s, Epoch 0.08/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>6.435000</td>\n",
       "      <td>3.307872</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>99.4052</td>\n",
       "      <td>99.4102</td>\n",
       "      <td>49.0756</td>\n",
       "      <td>1.0153</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.366600</td>\n",
       "      <td>3.307817</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>99.4094</td>\n",
       "      <td>99.4102</td>\n",
       "      <td>49.0756</td>\n",
       "      <td>1.0153</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.363900</td>\n",
       "      <td>3.307807</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>0.1406</td>\n",
       "      <td>99.4099</td>\n",
       "      <td>99.4102</td>\n",
       "      <td>49.0756</td>\n",
       "      <td>1.0153</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-bb1c47517fd0>:20: RuntimeWarning: Mean of empty slice.\n",
      "  'medium trade accuracy': (soft_profit[(abs_trade < .7) & (abs_trade >= .4)] > 0).mean() * 100,\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:21: RuntimeWarning: Mean of empty slice.\n",
      "  'medium trade g/l': soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit > 0)].mean()\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:22: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit < 0)].mean(),\n",
      "<ipython-input-2-bb1c47517fd0>:25: RuntimeWarning: Mean of empty slice.\n",
      "  'small trade accuracy': (soft_profit[(abs_trade < .4) & (abs_trade >= .2)] > 0).mean() * 100,\n",
      "<ipython-input-2-bb1c47517fd0>:26: RuntimeWarning: Mean of empty slice.\n",
      "  'small trade g/l': soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit > 0)].mean()\n",
      "<ipython-input-2-bb1c47517fd0>:27: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit < 0)].mean(),\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-6f5ae221c3b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# presquare standardized trade loss, losses + 1 (summed per day per timeframe)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1748\u001b[0m             \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1749\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1751\u001b[0m                 \u001b[1;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    679\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa: F811\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[1;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2356\u001b[1;33m         return self._getitem(\n\u001b[0m\u001b[0;32m   2357\u001b[0m             \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2358\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[1;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[0;32m   2339\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2340\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2341\u001b[1;33m         formatted_output = format_table(\n\u001b[0m\u001b[0;32m   2342\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2343\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[0mpa_table_to_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_names\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m         \u001b[0mformatted_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table_to_format\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquery_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformatted_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMutableMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mRowFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumnFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchFormat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"row\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"column\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mformat_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPythonFormatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFormatter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mformat_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython_arrow_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m             \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython_features_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mextract_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPythonArrowExtractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseArrowExtractor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pydict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36m_unnest\u001b[1;34m(py_dict)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_dict\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;34m\"\"\"Return the first element of a batch (dict) as a row (dict)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpy_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 2e-4, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# presquare standardized trade loss, losses + 1 (summed per day per timeframe)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='863' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 863/8804 04:41 < 43:18, 3.06 it/s, Epoch 0.10/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.699000</td>\n",
       "      <td>1.281750</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>0.146</td>\n",
       "      <td>98.8801</td>\n",
       "      <td>99.3897</td>\n",
       "      <td>49.2939</td>\n",
       "      <td>1.0234</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>63.0435</td>\n",
       "      <td>0.9882</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>57.1429</td>\n",
       "      <td>0.9437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.292200</td>\n",
       "      <td>1.327885</td>\n",
       "      <td>0.0461</td>\n",
       "      <td>0.1584</td>\n",
       "      <td>98.6804</td>\n",
       "      <td>98.5581</td>\n",
       "      <td>49.1911</td>\n",
       "      <td>1.0050</td>\n",
       "      <td>0.4379</td>\n",
       "      <td>50.5238</td>\n",
       "      <td>1.0331</td>\n",
       "      <td>0.2166</td>\n",
       "      <td>47.2316</td>\n",
       "      <td>0.9886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.255800</td>\n",
       "      <td>1.284796</td>\n",
       "      <td>0.1078</td>\n",
       "      <td>0.2387</td>\n",
       "      <td>99.1283</td>\n",
       "      <td>99.0681</td>\n",
       "      <td>49.3508</td>\n",
       "      <td>1.0119</td>\n",
       "      <td>0.1773</td>\n",
       "      <td>49.2353</td>\n",
       "      <td>1.004</td>\n",
       "      <td>0.0849</td>\n",
       "      <td>47.7887</td>\n",
       "      <td>1.0334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.248600</td>\n",
       "      <td>1.291166</td>\n",
       "      <td>0.1108</td>\n",
       "      <td>0.2360</td>\n",
       "      <td>99.2292</td>\n",
       "      <td>99.1844</td>\n",
       "      <td>49.3861</td>\n",
       "      <td>1.0110</td>\n",
       "      <td>0.1162</td>\n",
       "      <td>50.4488</td>\n",
       "      <td>0.9394</td>\n",
       "      <td>0.0552</td>\n",
       "      <td>47.4480</td>\n",
       "      <td>1.1860</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-cb8239a2336c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# square standardized trade loss, losses + .1 (summed per day per timeframe)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1773\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1774\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1775\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1776\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1777\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2522\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2523\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2525\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2553\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2554\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2555\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2556\u001b[0m         \u001b[1;31m# Save past state if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2557\u001b[0m         \u001b[1;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\trader_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, ohlcv, labels, future, std_future)\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0membed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_embed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mohlcv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\trader_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, mod)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgconv_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_gconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmod\u001b[0m \u001b[1;31m# residual\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\gconv_standalone.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, u, return_kernel)\u001b[0m\n\u001b[0;32m    427\u001b[0m                 torch.arange(k.size(-1), device=k.device)+1).view(1, 1, -1))\n\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_norm_initialized\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             self.kernel_norm_initialized = torch.tensor(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 2e-4, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# square standardized trade loss, losses + .1 (summed per day per timeframe)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='496' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 496/8804 02:22 < 40:02, 3.46 it/s, Epoch 0.06/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>26.331900</td>\n",
       "      <td>11.772779</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>99.0804</td>\n",
       "      <td>99.0062</td>\n",
       "      <td>49.1585</td>\n",
       "      <td>0.9969</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>48.8866</td>\n",
       "      <td>0.9926</td>\n",
       "      <td>0.1018</td>\n",
       "      <td>50.</td>\n",
       "      <td>0.9637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>11.396300</td>\n",
       "      <td>11.379965</td>\n",
       "      <td>0.1782</td>\n",
       "      <td>0.1454</td>\n",
       "      <td>99.4101</td>\n",
       "      <td>99.4101</td>\n",
       "      <td>49.4153</td>\n",
       "      <td>1.0243</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-bb1c47517fd0>:22: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit < 0)].mean(),\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:25: RuntimeWarning: Mean of empty slice.\n",
      "  'small trade accuracy': (soft_profit[(abs_trade < .4) & (abs_trade >= .2)] > 0).mean() * 100,\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:26: RuntimeWarning: Mean of empty slice.\n",
      "  'small trade g/l': soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit > 0)].mean()\n",
      "<ipython-input-2-bb1c47517fd0>:27: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit < 0)].mean(),\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7caa0b0c648a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# square standardized trade loss, 2x losses + 1 (summed per day per timeframe)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1773\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1774\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1775\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1776\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1777\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2522\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2523\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2525\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2553\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2554\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2555\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2556\u001b[0m         \u001b[1;31m# Save past state if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2557\u001b[0m         \u001b[1;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\trader_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, ohlcv, labels, future, std_future)\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0membed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_embed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mohlcv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\trader_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, mod)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgconv_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_gconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmod\u001b[0m \u001b[1;31m# residual\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\gconv_standalone.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, u, return_kernel)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[1;31m# Compute D term in state space equation - essentially a skip connection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 456\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcontract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bhl,ch->bchl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m         \u001b[1;31m# Reshape to flatten channels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 2e-4, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# square standardized trade loss, 2x losses + 1 (summed per day per timeframe)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='455' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 455/8804 02:10 < 40:07, 3.47 it/s, Epoch 0.05/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.799400</td>\n",
       "      <td>2.396652</td>\n",
       "      <td>-0.0972</td>\n",
       "      <td>-0.1417</td>\n",
       "      <td>99.3976</td>\n",
       "      <td>99.4102</td>\n",
       "      <td>49.0252</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.358600</td>\n",
       "      <td>2.396541</td>\n",
       "      <td>-0.0972</td>\n",
       "      <td>-0.1417</td>\n",
       "      <td>99.4083</td>\n",
       "      <td>99.4102</td>\n",
       "      <td>49.0252</td>\n",
       "      <td>0.9817</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-bb1c47517fd0>:20: RuntimeWarning: Mean of empty slice.\n",
      "  'medium trade accuracy': (soft_profit[(abs_trade < .7) & (abs_trade >= .4)] > 0).mean() * 100,\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:21: RuntimeWarning: Mean of empty slice.\n",
      "  'medium trade g/l': soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit > 0)].mean()\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:22: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit < 0)].mean(),\n",
      "<ipython-input-2-bb1c47517fd0>:25: RuntimeWarning: Mean of empty slice.\n",
      "  'small trade accuracy': (soft_profit[(abs_trade < .4) & (abs_trade >= .2)] > 0).mean() * 100,\n",
      "<ipython-input-2-bb1c47517fd0>:26: RuntimeWarning: Mean of empty slice.\n",
      "  'small trade g/l': soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit > 0)].mean()\n",
      "<ipython-input-2-bb1c47517fd0>:27: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit < 0)].mean(),\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f2b40ba977cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# square standardized trade loss, losses + 1 (summed per day per timeframe)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1748\u001b[0m             \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1749\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1751\u001b[0m                 \u001b[1;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    679\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa: F811\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[1;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2356\u001b[1;33m         return self._getitem(\n\u001b[0m\u001b[0;32m   2357\u001b[0m             \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2358\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[1;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[0;32m   2339\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2340\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2341\u001b[1;33m         formatted_output = format_table(\n\u001b[0m\u001b[0;32m   2342\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2343\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[0mpa_table_to_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_names\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m         \u001b[0mformatted_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table_to_format\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquery_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformatted_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMutableMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mRowFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumnFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchFormat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"row\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"column\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mformat_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPythonFormatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFormatter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mformat_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython_arrow_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m             \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython_features_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mextract_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPythonArrowExtractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseArrowExtractor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pydict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36m_unnest\u001b[1;34m(py_dict)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_dict\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;34m\"\"\"Return the first element of a batch (dict) as a row (dict)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpy_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 2e-4, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# square standardized trade loss, losses + 1 (summed per day per timeframe)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='431' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 431/8804 02:24 < 47:02, 2.97 it/s, Epoch 0.05/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.268300</td>\n",
       "      <td>2.372262</td>\n",
       "      <td>0.0912</td>\n",
       "      <td>0.1450</td>\n",
       "      <td>99.3996</td>\n",
       "      <td>99.4100</td>\n",
       "      <td>49.2785</td>\n",
       "      <td>1.0112</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.363100</td>\n",
       "      <td>2.372129</td>\n",
       "      <td>0.0913</td>\n",
       "      <td>0.1451</td>\n",
       "      <td>99.4088</td>\n",
       "      <td>99.4101</td>\n",
       "      <td>49.2784</td>\n",
       "      <td>1.0112</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-bb1c47517fd0>:21: RuntimeWarning: Mean of empty slice.\n",
      "  'medium trade g/l': soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit > 0)].mean()\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:25: RuntimeWarning: Mean of empty slice.\n",
      "  'small trade accuracy': (soft_profit[(abs_trade < .4) & (abs_trade >= .2)] > 0).mean() * 100,\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:26: RuntimeWarning: Mean of empty slice.\n",
      "  'small trade g/l': soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit > 0)].mean()\n",
      "<ipython-input-2-bb1c47517fd0>:27: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit < 0)].mean(),\n",
      "<ipython-input-2-bb1c47517fd0>:22: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit < 0)].mean(),\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b6c1065d3889>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# raw standardized trade loss, losses + 1 (summed per day per timeframe)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1748\u001b[0m             \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1749\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1751\u001b[0m                 \u001b[1;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    679\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa: F811\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[1;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2356\u001b[1;33m         return self._getitem(\n\u001b[0m\u001b[0;32m   2357\u001b[0m             \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2358\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[1;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[0;32m   2339\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2340\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2341\u001b[1;33m         formatted_output = format_table(\n\u001b[0m\u001b[0;32m   2342\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2343\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[0mpa_table_to_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_names\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m         \u001b[0mformatted_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table_to_format\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquery_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformatted_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMutableMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mRowFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumnFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchFormat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"row\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"column\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mformat_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPythonFormatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFormatter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mformat_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython_arrow_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m             \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython_features_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mextract_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPythonArrowExtractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseArrowExtractor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pydict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36m_unnest\u001b[1;34m(py_dict)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_dict\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;34m\"\"\"Return the first element of a batch (dict) as a row (dict)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpy_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 2e-4, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# raw standardized trade loss, losses + 1 (summed per day per timeframe)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3426' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3426/8804 18:49 < 29:34, 3.03 it/s, Epoch 0.39/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.929900</td>\n",
       "      <td>3.768107</td>\n",
       "      <td>0.0539</td>\n",
       "      <td>0.1823</td>\n",
       "      <td>31.1271</td>\n",
       "      <td>5.5132</td>\n",
       "      <td>52.5665</td>\n",
       "      <td>1.0714</td>\n",
       "      <td>26.9522</td>\n",
       "      <td>50.1052</td>\n",
       "      <td>0.9845</td>\n",
       "      <td>30.5463</td>\n",
       "      <td>49.3106</td>\n",
       "      <td>0.9902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.740600</td>\n",
       "      <td>3.982159</td>\n",
       "      <td>0.0379</td>\n",
       "      <td>0.0775</td>\n",
       "      <td>39.6059</td>\n",
       "      <td>19.8340</td>\n",
       "      <td>49.9416</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>23.554</td>\n",
       "      <td>49.9327</td>\n",
       "      <td>0.9742</td>\n",
       "      <td>23.3743</td>\n",
       "      <td>50.2068</td>\n",
       "      <td>0.9906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.491600</td>\n",
       "      <td>3.732116</td>\n",
       "      <td>0.0914</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>27.2457</td>\n",
       "      <td>11.4002</td>\n",
       "      <td>50.5067</td>\n",
       "      <td>1.0611</td>\n",
       "      <td>8.3187</td>\n",
       "      <td>51.4202</td>\n",
       "      <td>0.9879</td>\n",
       "      <td>25.8674</td>\n",
       "      <td>49.7704</td>\n",
       "      <td>0.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.299800</td>\n",
       "      <td>3.144496</td>\n",
       "      <td>0.1149</td>\n",
       "      <td>0.2531</td>\n",
       "      <td>27.3579</td>\n",
       "      <td>16.784</td>\n",
       "      <td>51.3720</td>\n",
       "      <td>1.0514</td>\n",
       "      <td>7.2662</td>\n",
       "      <td>50.7864</td>\n",
       "      <td>0.9628</td>\n",
       "      <td>11.2447</td>\n",
       "      <td>49.9059</td>\n",
       "      <td>0.9894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.905900</td>\n",
       "      <td>2.593794</td>\n",
       "      <td>0.2288</td>\n",
       "      <td>0.4867</td>\n",
       "      <td>14.6086</td>\n",
       "      <td>8.0054</td>\n",
       "      <td>56.1537</td>\n",
       "      <td>1.2084</td>\n",
       "      <td>4.0879</td>\n",
       "      <td>53.2024</td>\n",
       "      <td>1.0663</td>\n",
       "      <td>5.1752</td>\n",
       "      <td>52.1881</td>\n",
       "      <td>0.9671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.928600</td>\n",
       "      <td>2.666212</td>\n",
       "      <td>0.0813</td>\n",
       "      <td>0.2952</td>\n",
       "      <td>13.4875</td>\n",
       "      <td>7.4764</td>\n",
       "      <td>50.1618</td>\n",
       "      <td>1.1143</td>\n",
       "      <td>3.2620</td>\n",
       "      <td>50.8087</td>\n",
       "      <td>1.0125</td>\n",
       "      <td>4.4686</td>\n",
       "      <td>51.7290</td>\n",
       "      <td>1.0173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.697900</td>\n",
       "      <td>2.789548</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>0.2107</td>\n",
       "      <td>13.5287</td>\n",
       "      <td>9.0544</td>\n",
       "      <td>49.8094</td>\n",
       "      <td>1.0264</td>\n",
       "      <td>2.673</td>\n",
       "      <td>50.2360</td>\n",
       "      <td>1.0262</td>\n",
       "      <td>3.2743</td>\n",
       "      <td>51.1464</td>\n",
       "      <td>1.1130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.702700</td>\n",
       "      <td>2.993182</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>7.9011</td>\n",
       "      <td>4.4768</td>\n",
       "      <td>49.2663</td>\n",
       "      <td>0.9660</td>\n",
       "      <td>1.6485</td>\n",
       "      <td>45.6230</td>\n",
       "      <td>0.9454</td>\n",
       "      <td>2.0215</td>\n",
       "      <td>49.5177</td>\n",
       "      <td>1.0106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.713400</td>\n",
       "      <td>3.248822</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>10.1692</td>\n",
       "      <td>6.1419</td>\n",
       "      <td>49.9363</td>\n",
       "      <td>0.9710</td>\n",
       "      <td>2.5123</td>\n",
       "      <td>48.095</td>\n",
       "      <td>0.9650</td>\n",
       "      <td>3.0488</td>\n",
       "      <td>49.2732</td>\n",
       "      <td>0.9701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.737300</td>\n",
       "      <td>2.635870</td>\n",
       "      <td>0.0226</td>\n",
       "      <td>0.0877</td>\n",
       "      <td>8.7933</td>\n",
       "      <td>5.4228</td>\n",
       "      <td>50.7797</td>\n",
       "      <td>0.9494</td>\n",
       "      <td>2.4038</td>\n",
       "      <td>50.3058</td>\n",
       "      <td>1.0048</td>\n",
       "      <td>2.5930</td>\n",
       "      <td>49.9759</td>\n",
       "      <td>1.0272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.667300</td>\n",
       "      <td>2.868226</td>\n",
       "      <td>0.0364</td>\n",
       "      <td>0.2551</td>\n",
       "      <td>7.9147</td>\n",
       "      <td>3.9863</td>\n",
       "      <td>50.9521</td>\n",
       "      <td>1.0474</td>\n",
       "      <td>1.6266</td>\n",
       "      <td>52.3974</td>\n",
       "      <td>0.9462</td>\n",
       "      <td>2.3096</td>\n",
       "      <td>51.3544</td>\n",
       "      <td>1.0058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.474400</td>\n",
       "      <td>2.501009</td>\n",
       "      <td>0.0474</td>\n",
       "      <td>0.3668</td>\n",
       "      <td>5.6369</td>\n",
       "      <td>2.4521</td>\n",
       "      <td>52.7108</td>\n",
       "      <td>1.1396</td>\n",
       "      <td>1.5352</td>\n",
       "      <td>50.8592</td>\n",
       "      <td>1.0921</td>\n",
       "      <td>2.0084</td>\n",
       "      <td>48.7981</td>\n",
       "      <td>1.0845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.505500</td>\n",
       "      <td>2.423974</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>0.4582</td>\n",
       "      <td>4.4967</td>\n",
       "      <td>2.0449</td>\n",
       "      <td>54.4796</td>\n",
       "      <td>1.1813</td>\n",
       "      <td>1.2204</td>\n",
       "      <td>49.1029</td>\n",
       "      <td>1.0399</td>\n",
       "      <td>1.4109</td>\n",
       "      <td>48.9395</td>\n",
       "      <td>1.0205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.153000</td>\n",
       "      <td>2.188326</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>0.3262</td>\n",
       "      <td>5.9427</td>\n",
       "      <td>3.3856</td>\n",
       "      <td>52.4531</td>\n",
       "      <td>1.1366</td>\n",
       "      <td>1.6659</td>\n",
       "      <td>48.332</td>\n",
       "      <td>1.0510</td>\n",
       "      <td>1.7114</td>\n",
       "      <td>48.7601</td>\n",
       "      <td>1.0350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.402600</td>\n",
       "      <td>2.456040</td>\n",
       "      <td>0.0297</td>\n",
       "      <td>0.1755</td>\n",
       "      <td>6.6012</td>\n",
       "      <td>3.9029</td>\n",
       "      <td>52.3324</td>\n",
       "      <td>1.0237</td>\n",
       "      <td>1.5399</td>\n",
       "      <td>48.4223</td>\n",
       "      <td>1.0111</td>\n",
       "      <td>1.6564</td>\n",
       "      <td>47.7213</td>\n",
       "      <td>0.9992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.314600</td>\n",
       "      <td>2.211251</td>\n",
       "      <td>0.0464</td>\n",
       "      <td>0.3472</td>\n",
       "      <td>5.2011</td>\n",
       "      <td>2.983</td>\n",
       "      <td>52.0099</td>\n",
       "      <td>1.0783</td>\n",
       "      <td>1.3219</td>\n",
       "      <td>50.0079</td>\n",
       "      <td>1.1041</td>\n",
       "      <td>1.4087</td>\n",
       "      <td>51.0955</td>\n",
       "      <td>1.1385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.219100</td>\n",
       "      <td>2.413304</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>0.1750</td>\n",
       "      <td>6.1049</td>\n",
       "      <td>3.9543</td>\n",
       "      <td>51.2802</td>\n",
       "      <td>1.0218</td>\n",
       "      <td>1.4003</td>\n",
       "      <td>46.5038</td>\n",
       "      <td>1.0567</td>\n",
       "      <td>1.4087</td>\n",
       "      <td>47.9571</td>\n",
       "      <td>1.0375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-48274bd44358>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# square standardized trade loss, losses 2x (summed per day per timeframe)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1748\u001b[0m             \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1749\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1751\u001b[0m                 \u001b[1;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    679\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa: F811\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[1;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2356\u001b[1;33m         return self._getitem(\n\u001b[0m\u001b[0;32m   2357\u001b[0m             \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2358\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[1;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[0;32m   2338\u001b[0m         \u001b[0mformat_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformat_kwargs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mformat_kwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2339\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2340\u001b[1;33m         \u001b[0mpa_subtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2341\u001b[0m         formatted_output = format_table(\n\u001b[0;32m   2342\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mquery_table\u001b[1;34m(table, key, indices)\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;31m# Query the main table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m         \u001b[0mpa_subtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_query_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    467\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_query_table_with_indices_mapping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36m_query_table\u001b[1;34m(table, key)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \"\"\"\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfast_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\table.py\u001b[0m in \u001b[0;36mfast_slice\u001b[1;34m(self, offset, length)\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mbatches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlength\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_offsets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[0mbatches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_offsets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_schema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 2e-4, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# square standardized trade loss, losses 2x (summed per day per timeframe)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2601' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2601/8804 13:03 < 31:08, 3.32 it/s, Epoch 0.30/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.089100</td>\n",
       "      <td>0.921855</td>\n",
       "      <td>0.0846</td>\n",
       "      <td>0.3101</td>\n",
       "      <td>31.5395</td>\n",
       "      <td>4.8777</td>\n",
       "      <td>53.5155</td>\n",
       "      <td>0.9666</td>\n",
       "      <td>28.745</td>\n",
       "      <td>50.5071</td>\n",
       "      <td>1.0045</td>\n",
       "      <td>30.8576</td>\n",
       "      <td>49.0672</td>\n",
       "      <td>1.0007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.957700</td>\n",
       "      <td>1.017222</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>47.1689</td>\n",
       "      <td>28.0160</td>\n",
       "      <td>50.3787</td>\n",
       "      <td>0.974</td>\n",
       "      <td>27.1046</td>\n",
       "      <td>50.0458</td>\n",
       "      <td>0.9672</td>\n",
       "      <td>20.6508</td>\n",
       "      <td>49.4448</td>\n",
       "      <td>0.9794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.922500</td>\n",
       "      <td>0.898547</td>\n",
       "      <td>0.1456</td>\n",
       "      <td>0.3357</td>\n",
       "      <td>39.4345</td>\n",
       "      <td>21.9537</td>\n",
       "      <td>51.3496</td>\n",
       "      <td>1.0159</td>\n",
       "      <td>18.4206</td>\n",
       "      <td>50.0127</td>\n",
       "      <td>0.9686</td>\n",
       "      <td>22.9927</td>\n",
       "      <td>49.9295</td>\n",
       "      <td>0.9695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.897200</td>\n",
       "      <td>0.895997</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.2305</td>\n",
       "      <td>31.9419</td>\n",
       "      <td>17.3018</td>\n",
       "      <td>50.2510</td>\n",
       "      <td>1.034</td>\n",
       "      <td>14.2036</td>\n",
       "      <td>50.5190</td>\n",
       "      <td>0.9727</td>\n",
       "      <td>16.9758</td>\n",
       "      <td>50.4782</td>\n",
       "      <td>0.9733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.825100</td>\n",
       "      <td>0.771203</td>\n",
       "      <td>0.2181</td>\n",
       "      <td>0.4399</td>\n",
       "      <td>23.663</td>\n",
       "      <td>13.9244</td>\n",
       "      <td>53.2556</td>\n",
       "      <td>1.0963</td>\n",
       "      <td>6.5054</td>\n",
       "      <td>50.9513</td>\n",
       "      <td>0.9382</td>\n",
       "      <td>9.3177</td>\n",
       "      <td>50.0660</td>\n",
       "      <td>0.9836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.802500</td>\n",
       "      <td>0.802199</td>\n",
       "      <td>0.1606</td>\n",
       "      <td>0.3758</td>\n",
       "      <td>26.9651</td>\n",
       "      <td>16.2087</td>\n",
       "      <td>52.1628</td>\n",
       "      <td>1.0717</td>\n",
       "      <td>7.8452</td>\n",
       "      <td>50.1854</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>11.0158</td>\n",
       "      <td>49.4444</td>\n",
       "      <td>0.9443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.832200</td>\n",
       "      <td>0.799641</td>\n",
       "      <td>0.1071</td>\n",
       "      <td>0.3558</td>\n",
       "      <td>18.0792</td>\n",
       "      <td>9.3398</td>\n",
       "      <td>51.4703</td>\n",
       "      <td>1.0967</td>\n",
       "      <td>5.4627</td>\n",
       "      <td>50.2109</td>\n",
       "      <td>0.9889</td>\n",
       "      <td>7.9180</td>\n",
       "      <td>50.2364</td>\n",
       "      <td>0.9994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.821700</td>\n",
       "      <td>0.819215</td>\n",
       "      <td>0.0547</td>\n",
       "      <td>0.1941</td>\n",
       "      <td>14.4979</td>\n",
       "      <td>6.3646</td>\n",
       "      <td>52.4763</td>\n",
       "      <td>0.9357</td>\n",
       "      <td>5.5735</td>\n",
       "      <td>50.2451</td>\n",
       "      <td>0.9431</td>\n",
       "      <td>7.8908</td>\n",
       "      <td>50.1784</td>\n",
       "      <td>0.9824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.760600</td>\n",
       "      <td>0.867052</td>\n",
       "      <td>0.0356</td>\n",
       "      <td>0.1477</td>\n",
       "      <td>14.1493</td>\n",
       "      <td>6.5528</td>\n",
       "      <td>52.9804</td>\n",
       "      <td>0.9447</td>\n",
       "      <td>4.9884</td>\n",
       "      <td>48.9705</td>\n",
       "      <td>0.943</td>\n",
       "      <td>6.8644</td>\n",
       "      <td>49.7357</td>\n",
       "      <td>0.9457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.780400</td>\n",
       "      <td>0.870831</td>\n",
       "      <td>0.0247</td>\n",
       "      <td>0.0808</td>\n",
       "      <td>13.4398</td>\n",
       "      <td>7.1109</td>\n",
       "      <td>50.5968</td>\n",
       "      <td>0.9568</td>\n",
       "      <td>4.2246</td>\n",
       "      <td>49.6298</td>\n",
       "      <td>0.967</td>\n",
       "      <td>5.6426</td>\n",
       "      <td>50.5830</td>\n",
       "      <td>0.9670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.749600</td>\n",
       "      <td>0.787069</td>\n",
       "      <td>0.0603</td>\n",
       "      <td>0.2908</td>\n",
       "      <td>13.9713</td>\n",
       "      <td>6.7805</td>\n",
       "      <td>51.7838</td>\n",
       "      <td>1.0502</td>\n",
       "      <td>4.6864</td>\n",
       "      <td>49.6907</td>\n",
       "      <td>0.9309</td>\n",
       "      <td>6.2764</td>\n",
       "      <td>49.2566</td>\n",
       "      <td>0.9538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.731200</td>\n",
       "      <td>0.750115</td>\n",
       "      <td>0.0619</td>\n",
       "      <td>0.3918</td>\n",
       "      <td>9.9751</td>\n",
       "      <td>4.3644</td>\n",
       "      <td>53.2182</td>\n",
       "      <td>1.0630</td>\n",
       "      <td>3.247</td>\n",
       "      <td>51.0822</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>4.3531</td>\n",
       "      <td>49.4036</td>\n",
       "      <td>0.9391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4/19 00:00 < 00:04, 3.44 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f0bf5bd8b903>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# square standardized trade loss (summed per day per timeframe)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1850\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1852\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1853\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2113\u001b[0m                     )\n\u001b[0;32m   2114\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2115\u001b[1;33m                 \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2116\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2809\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2810\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2811\u001b[1;33m         output = eval_loop(\n\u001b[0m\u001b[0;32m   2812\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2813\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Evaluation\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2977\u001b[0m         \u001b[0mobserved_num_examples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2978\u001b[0m         \u001b[1;31m# Main evaluation loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2979\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2980\u001b[0m             \u001b[1;31m# Update the observed num examples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2981\u001b[0m             \u001b[0mobserved_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_batch_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    679\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa: F811\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[1;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2356\u001b[1;33m         return self._getitem(\n\u001b[0m\u001b[0;32m   2357\u001b[0m             \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2358\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[1;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[0;32m   2339\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2340\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2341\u001b[1;33m         formatted_output = format_table(\n\u001b[0m\u001b[0;32m   2342\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2343\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[0mpa_table_to_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_names\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m         \u001b[0mformatted_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table_to_format\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquery_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformatted_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMutableMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mRowFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumnFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchFormat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"row\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"column\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mformat_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPythonFormatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFormatter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mformat_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython_arrow_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m             \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython_features_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mextract_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPythonArrowExtractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseArrowExtractor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pydict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36m_unnest\u001b[1;34m(py_dict)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_dict\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;34m\"\"\"Return the first element of a batch (dict) as a row (dict)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpy_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 2e-4, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# square standardized trade loss (summed per day per timeframe)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1083' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1083/8804 05:46 < 41:16, 3.12 it/s, Epoch 0.12/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.935700</td>\n",
       "      <td>1.905045</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.1037</td>\n",
       "      <td>26.9757</td>\n",
       "      <td>3.9857</td>\n",
       "      <td>51.5095</td>\n",
       "      <td>1.0714</td>\n",
       "      <td>19.4951</td>\n",
       "      <td>50.0524</td>\n",
       "      <td>0.9880</td>\n",
       "      <td>32.2691</td>\n",
       "      <td>49.6753</td>\n",
       "      <td>1.0024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.880300</td>\n",
       "      <td>1.931114</td>\n",
       "      <td>0.0779</td>\n",
       "      <td>0.1075</td>\n",
       "      <td>42.3429</td>\n",
       "      <td>20.9454</td>\n",
       "      <td>49.6229</td>\n",
       "      <td>0.9622</td>\n",
       "      <td>26.7082</td>\n",
       "      <td>50.3436</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>25.3010</td>\n",
       "      <td>50.8463</td>\n",
       "      <td>1.0342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.759700</td>\n",
       "      <td>1.773840</td>\n",
       "      <td>0.2331</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>19.6714</td>\n",
       "      <td>8.9802</td>\n",
       "      <td>53.2941</td>\n",
       "      <td>1.2738</td>\n",
       "      <td>6.174</td>\n",
       "      <td>52.6051</td>\n",
       "      <td>1.0065</td>\n",
       "      <td>12.1319</td>\n",
       "      <td>50.7667</td>\n",
       "      <td>1.0148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.636900</td>\n",
       "      <td>1.504781</td>\n",
       "      <td>0.3095</td>\n",
       "      <td>0.3795</td>\n",
       "      <td>25.4199</td>\n",
       "      <td>15.8807</td>\n",
       "      <td>54.6040</td>\n",
       "      <td>1.1606</td>\n",
       "      <td>7.1482</td>\n",
       "      <td>52.5002</td>\n",
       "      <td>1.0219</td>\n",
       "      <td>10.1815</td>\n",
       "      <td>48.9897</td>\n",
       "      <td>0.9594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.532700</td>\n",
       "      <td>1.349475</td>\n",
       "      <td>0.2846</td>\n",
       "      <td>0.3836</td>\n",
       "      <td>14.8668</td>\n",
       "      <td>9.2239</td>\n",
       "      <td>57.1676</td>\n",
       "      <td>1.164</td>\n",
       "      <td>3.9307</td>\n",
       "      <td>53.4154</td>\n",
       "      <td>0.9777</td>\n",
       "      <td>5.1829</td>\n",
       "      <td>52.2794</td>\n",
       "      <td>0.9151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-7-56d4e1671c9d>\", line 7, in <module>\n",
      "    trainer.train()\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\", line 1527, in train\n",
      "    return inner_training_loop(\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\", line 1749, in _inner_training_loop\n",
      "    for step, inputs in enumerate(epoch_iterator):\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 681, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 721, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 2356, in __getitem__\n",
      "    return self._getitem(\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 2341, in _getitem\n",
      "    formatted_output = format_table(\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\", line 517, in format_table\n",
      "    formatted_output = formatter(pa_table_to_format, query_type=query_type)\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\", line 282, in __call__\n",
      "    return self.format_row(pa_table)\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\", line 311, in format_row\n",
      "    row = self.python_arrow_extractor().extract_row(pa_table)\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\", line 141, in extract_row\n",
      "    return _unnest(pa_table.to_pydict())\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\", line 123, in _unnest\n",
      "    def _unnest(py_dict: Dict[str, List[T]]) -> Dict[str, T]:\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1170, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\inspect.py\", line 751, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\inspect.py\", line 720, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\inspect.py\", line 705, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"C:\\Users\\micha\\anaconda3\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-56d4e1671c9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# raw standardized trade loss, 2x on loss side only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1748\u001b[0m             \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1749\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[1;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2356\u001b[1;33m         return self._getitem(\n\u001b[0m\u001b[0;32m   2357\u001b[0m             \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[1;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[0;32m   2340\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2341\u001b[1;33m         formatted_output = format_table(\n\u001b[0m\u001b[0;32m   2342\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[0mpa_table_to_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_names\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m         \u001b[0mformatted_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table_to_format\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquery_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"row\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"column\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mformat_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mformat_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython_arrow_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mextract_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pydict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36m_unnest\u001b[1;34m(py_dict)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_dict\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;34m\"\"\"Return the first element of a batch (dict) as a row (dict)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2044\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2045\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2046\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2045\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2046\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2047\u001b[1;33m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[0;32m   2048\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[0;32m   2049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1434\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1436\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1437\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0;32m   1438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1336\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1337\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1191\u001b[0m         \u001b[1;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1193\u001b[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[0;32m   1194\u001b[0m                                                                tb_offset)\n\u001b[0;32m   1195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1151\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 2e-4, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# raw standardized trade loss, 2x on loss side only\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1952' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1952/8804 11:00 < 38:40, 2.95 it/s, Epoch 0.22/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.993200</td>\n",
       "      <td>0.964660</td>\n",
       "      <td>0.0460</td>\n",
       "      <td>0.1384</td>\n",
       "      <td>19.599</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>51.8084</td>\n",
       "      <td>0.9676</td>\n",
       "      <td>9.1668</td>\n",
       "      <td>51.2234</td>\n",
       "      <td>1.0098</td>\n",
       "      <td>31.8757</td>\n",
       "      <td>50.0002</td>\n",
       "      <td>1.0040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.960900</td>\n",
       "      <td>0.972504</td>\n",
       "      <td>0.0547</td>\n",
       "      <td>0.0977</td>\n",
       "      <td>25.8279</td>\n",
       "      <td>5.2262</td>\n",
       "      <td>50.7193</td>\n",
       "      <td>0.9806</td>\n",
       "      <td>16.9100</td>\n",
       "      <td>49.5739</td>\n",
       "      <td>0.9415</td>\n",
       "      <td>28.0905</td>\n",
       "      <td>50.8476</td>\n",
       "      <td>1.0253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.894500</td>\n",
       "      <td>0.904141</td>\n",
       "      <td>0.1768</td>\n",
       "      <td>0.3938</td>\n",
       "      <td>16.8007</td>\n",
       "      <td>6.2597</td>\n",
       "      <td>52.3895</td>\n",
       "      <td>1.3758</td>\n",
       "      <td>5.9450</td>\n",
       "      <td>52.2915</td>\n",
       "      <td>1.0567</td>\n",
       "      <td>10.6216</td>\n",
       "      <td>50.6317</td>\n",
       "      <td>0.985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.831100</td>\n",
       "      <td>0.745146</td>\n",
       "      <td>0.2550</td>\n",
       "      <td>0.3641</td>\n",
       "      <td>19.1567</td>\n",
       "      <td>10.5863</td>\n",
       "      <td>55.2956</td>\n",
       "      <td>1.1287</td>\n",
       "      <td>6.4279</td>\n",
       "      <td>52.9507</td>\n",
       "      <td>1.0936</td>\n",
       "      <td>8.6813</td>\n",
       "      <td>50.0222</td>\n",
       "      <td>0.9879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.758700</td>\n",
       "      <td>0.648805</td>\n",
       "      <td>0.2531</td>\n",
       "      <td>0.3996</td>\n",
       "      <td>10.9775</td>\n",
       "      <td>6.1827</td>\n",
       "      <td>58.0791</td>\n",
       "      <td>1.1902</td>\n",
       "      <td>3.2261</td>\n",
       "      <td>54.9644</td>\n",
       "      <td>1.0209</td>\n",
       "      <td>3.8804</td>\n",
       "      <td>54.1018</td>\n",
       "      <td>0.9605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.735500</td>\n",
       "      <td>0.686146</td>\n",
       "      <td>0.1634</td>\n",
       "      <td>0.357</td>\n",
       "      <td>11.6782</td>\n",
       "      <td>6.8536</td>\n",
       "      <td>54.994</td>\n",
       "      <td>1.2125</td>\n",
       "      <td>3.4254</td>\n",
       "      <td>50.2329</td>\n",
       "      <td>1.0103</td>\n",
       "      <td>3.7778</td>\n",
       "      <td>51.5084</td>\n",
       "      <td>0.9393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.719200</td>\n",
       "      <td>0.730055</td>\n",
       "      <td>0.1068</td>\n",
       "      <td>0.1743</td>\n",
       "      <td>15.1963</td>\n",
       "      <td>10.5458</td>\n",
       "      <td>50.8009</td>\n",
       "      <td>1.0819</td>\n",
       "      <td>3.6891</td>\n",
       "      <td>47.965</td>\n",
       "      <td>1.0938</td>\n",
       "      <td>3.9897</td>\n",
       "      <td>50.6442</td>\n",
       "      <td>1.0983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.734900</td>\n",
       "      <td>0.810391</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>0.0637</td>\n",
       "      <td>6.604</td>\n",
       "      <td>3.303</td>\n",
       "      <td>51.1854</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>1.7508</td>\n",
       "      <td>45.7507</td>\n",
       "      <td>0.8694</td>\n",
       "      <td>2.1919</td>\n",
       "      <td>49.0081</td>\n",
       "      <td>0.9450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.737100</td>\n",
       "      <td>0.685237</td>\n",
       "      <td>0.1014</td>\n",
       "      <td>0.4100</td>\n",
       "      <td>7.7093</td>\n",
       "      <td>4.4106</td>\n",
       "      <td>56.4954</td>\n",
       "      <td>1.1769</td>\n",
       "      <td>2.2827</td>\n",
       "      <td>50.8679</td>\n",
       "      <td>0.9654</td>\n",
       "      <td>2.4709</td>\n",
       "      <td>50.2131</td>\n",
       "      <td>0.9275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-99176610df5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# raw standardized trade loss with .1 push loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1773\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1774\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1775\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1776\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1777\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2522\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2523\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2525\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2553\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2554\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2555\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2556\u001b[0m         \u001b[1;31m# Save past state if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2557\u001b[0m         \u001b[1;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\trader_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, ohlcv, labels, future, std_future)\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0membed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_embed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mohlcv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\trader_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, mod)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgconv_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_gconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmod\u001b[0m \u001b[1;31m# residual\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\gconv_standalone.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, u, return_kernel)\u001b[0m\n\u001b[0;32m    427\u001b[0m                 torch.arange(k.size(-1), device=k.device)+1).view(1, 1, -1))\n\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_norm_initialized\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             self.kernel_norm_initialized = torch.tensor(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 2e-4, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# raw standardized trade loss with .1 push loss\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2656' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2656/8804 14:39 < 33:58, 3.02 it/s, Epoch 0.30/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.969300</td>\n",
       "      <td>1.007020</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.0087</td>\n",
       "      <td>33.3416</td>\n",
       "      <td>16.8075</td>\n",
       "      <td>48.7831</td>\n",
       "      <td>0.9766</td>\n",
       "      <td>13.4059</td>\n",
       "      <td>49.9992</td>\n",
       "      <td>1.0180</td>\n",
       "      <td>23.8844</td>\n",
       "      <td>49.7505</td>\n",
       "      <td>0.9932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.972700</td>\n",
       "      <td>0.952922</td>\n",
       "      <td>0.1404</td>\n",
       "      <td>0.2985</td>\n",
       "      <td>34.1567</td>\n",
       "      <td>10.9177</td>\n",
       "      <td>50.9823</td>\n",
       "      <td>1.0226</td>\n",
       "      <td>25.2639</td>\n",
       "      <td>51.0622</td>\n",
       "      <td>1.0191</td>\n",
       "      <td>28.2674</td>\n",
       "      <td>50.3777</td>\n",
       "      <td>1.0232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.932300</td>\n",
       "      <td>1.013579</td>\n",
       "      <td>0.1071</td>\n",
       "      <td>0.221</td>\n",
       "      <td>16.3423</td>\n",
       "      <td>4.0763</td>\n",
       "      <td>51.2317</td>\n",
       "      <td>1.2828</td>\n",
       "      <td>2.5177</td>\n",
       "      <td>53.7149</td>\n",
       "      <td>1.1067</td>\n",
       "      <td>17.7492</td>\n",
       "      <td>49.8696</td>\n",
       "      <td>0.9961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.865600</td>\n",
       "      <td>0.781606</td>\n",
       "      <td>0.2985</td>\n",
       "      <td>0.3574</td>\n",
       "      <td>22.8959</td>\n",
       "      <td>13.9106</td>\n",
       "      <td>54.8715</td>\n",
       "      <td>1.1796</td>\n",
       "      <td>5.5940</td>\n",
       "      <td>50.8024</td>\n",
       "      <td>0.9909</td>\n",
       "      <td>8.8335</td>\n",
       "      <td>50.3606</td>\n",
       "      <td>0.9364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.793200</td>\n",
       "      <td>0.712706</td>\n",
       "      <td>0.2849</td>\n",
       "      <td>0.4271</td>\n",
       "      <td>13.3489</td>\n",
       "      <td>9.1175</td>\n",
       "      <td>56.7635</td>\n",
       "      <td>1.2578</td>\n",
       "      <td>2.2797</td>\n",
       "      <td>52.8198</td>\n",
       "      <td>0.9779</td>\n",
       "      <td>2.6434</td>\n",
       "      <td>51.8047</td>\n",
       "      <td>0.9770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.764700</td>\n",
       "      <td>0.724855</td>\n",
       "      <td>0.1707</td>\n",
       "      <td>0.2853</td>\n",
       "      <td>10.332</td>\n",
       "      <td>4.7669</td>\n",
       "      <td>57.2229</td>\n",
       "      <td>1.0622</td>\n",
       "      <td>3.1546</td>\n",
       "      <td>56.2273</td>\n",
       "      <td>0.9365</td>\n",
       "      <td>4.5444</td>\n",
       "      <td>53.6402</td>\n",
       "      <td>0.9778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.769300</td>\n",
       "      <td>0.833231</td>\n",
       "      <td>-0.0068</td>\n",
       "      <td>-0.0114</td>\n",
       "      <td>12.9571</td>\n",
       "      <td>9.8757</td>\n",
       "      <td>48.6538</td>\n",
       "      <td>0.9312</td>\n",
       "      <td>1.5323</td>\n",
       "      <td>47.7237</td>\n",
       "      <td>1.0073</td>\n",
       "      <td>1.3993</td>\n",
       "      <td>49.4560</td>\n",
       "      <td>1.0166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.823600</td>\n",
       "      <td>0.752126</td>\n",
       "      <td>0.0273</td>\n",
       "      <td>0.1429</td>\n",
       "      <td>8.3880</td>\n",
       "      <td>5.3249</td>\n",
       "      <td>48.6861</td>\n",
       "      <td>0.9256</td>\n",
       "      <td>1.1708</td>\n",
       "      <td>46.4909</td>\n",
       "      <td>0.9474</td>\n",
       "      <td>1.3261</td>\n",
       "      <td>51.0851</td>\n",
       "      <td>1.0971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.816400</td>\n",
       "      <td>0.705045</td>\n",
       "      <td>0.1592</td>\n",
       "      <td>0.5226</td>\n",
       "      <td>9.4522</td>\n",
       "      <td>4.5892</td>\n",
       "      <td>56.1461</td>\n",
       "      <td>1.2751</td>\n",
       "      <td>1.2758</td>\n",
       "      <td>54.9081</td>\n",
       "      <td>1.051</td>\n",
       "      <td>1.5928</td>\n",
       "      <td>55.2763</td>\n",
       "      <td>1.0727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.817300</td>\n",
       "      <td>0.766090</td>\n",
       "      <td>0.0438</td>\n",
       "      <td>0.1218</td>\n",
       "      <td>9.0048</td>\n",
       "      <td>6.2312</td>\n",
       "      <td>49.9883</td>\n",
       "      <td>1.0058</td>\n",
       "      <td>1.7560</td>\n",
       "      <td>48.8510</td>\n",
       "      <td>1.0583</td>\n",
       "      <td>1.4582</td>\n",
       "      <td>49.5173</td>\n",
       "      <td>1.0534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.748400</td>\n",
       "      <td>0.808558</td>\n",
       "      <td>0.1412</td>\n",
       "      <td>0.3758</td>\n",
       "      <td>6.8230</td>\n",
       "      <td>4.0075</td>\n",
       "      <td>54.1864</td>\n",
       "      <td>1.2977</td>\n",
       "      <td>1.0453</td>\n",
       "      <td>57.7556</td>\n",
       "      <td>1.2013</td>\n",
       "      <td>1.1491</td>\n",
       "      <td>55.8076</td>\n",
       "      <td>1.0988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.728900</td>\n",
       "      <td>0.619413</td>\n",
       "      <td>0.0919</td>\n",
       "      <td>0.4491</td>\n",
       "      <td>4.8784</td>\n",
       "      <td>2.5069</td>\n",
       "      <td>53.3441</td>\n",
       "      <td>1.2169</td>\n",
       "      <td>1.091</td>\n",
       "      <td>55.0129</td>\n",
       "      <td>1.2043</td>\n",
       "      <td>1.5263</td>\n",
       "      <td>56.1074</td>\n",
       "      <td>1.1265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.734605</td>\n",
       "      <td>-0.0296</td>\n",
       "      <td>-0.0482</td>\n",
       "      <td>10.833</td>\n",
       "      <td>8.1569</td>\n",
       "      <td>48.8316</td>\n",
       "      <td>0.8843</td>\n",
       "      <td>1.6967</td>\n",
       "      <td>49.4653</td>\n",
       "      <td>0.9462</td>\n",
       "      <td>2.2099</td>\n",
       "      <td>48.0325</td>\n",
       "      <td>0.9944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-94f09ba18907>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# raw standardized trade loss with .1 push loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1748\u001b[0m             \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1749\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1751\u001b[0m                 \u001b[1;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    679\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa: F811\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[1;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2356\u001b[1;33m         return self._getitem(\n\u001b[0m\u001b[0;32m   2357\u001b[0m             \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2358\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[1;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[0;32m   2339\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2340\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2341\u001b[1;33m         formatted_output = format_table(\n\u001b[0m\u001b[0;32m   2342\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2343\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[0mpa_table_to_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_names\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m         \u001b[0mformatted_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table_to_format\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquery_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformatted_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMutableMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mRowFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumnFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchFormat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"row\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"column\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mformat_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPythonFormatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFormatter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mformat_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython_arrow_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m             \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython_features_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mextract_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPythonArrowExtractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseArrowExtractor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pydict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36m_unnest\u001b[1;34m(py_dict)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_dict\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;34m\"\"\"Return the first element of a batch (dict) as a row (dict)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpy_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 1e-3, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# raw standardized trade loss with .1 push loss\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='689' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 689/8804 03:38 < 43:04, 3.14 it/s, Epoch 0.08/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.955500</td>\n",
       "      <td>0.949130</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>43.1273</td>\n",
       "      <td>30.9720</td>\n",
       "      <td>49.0749</td>\n",
       "      <td>0.9693</td>\n",
       "      <td>10.0288</td>\n",
       "      <td>50.6665</td>\n",
       "      <td>1.0465</td>\n",
       "      <td>16.1600</td>\n",
       "      <td>49.2854</td>\n",
       "      <td>1.0088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.956800</td>\n",
       "      <td>0.992595</td>\n",
       "      <td>-0.0154</td>\n",
       "      <td>-0.0549</td>\n",
       "      <td>98.8661</td>\n",
       "      <td>98.8363</td>\n",
       "      <td>49.2996</td>\n",
       "      <td>0.9878</td>\n",
       "      <td>0.2982</td>\n",
       "      <td>49.021</td>\n",
       "      <td>1.063</td>\n",
       "      <td>0.1451</td>\n",
       "      <td>50.5747</td>\n",
       "      <td>1.0303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.983000</td>\n",
       "      <td>0.991899</td>\n",
       "      <td>-0.0137</td>\n",
       "      <td>-0.0277</td>\n",
       "      <td>98.4349</td>\n",
       "      <td>98.2998</td>\n",
       "      <td>49.3802</td>\n",
       "      <td>0.9845</td>\n",
       "      <td>0.5961</td>\n",
       "      <td>49.6414</td>\n",
       "      <td>1.0632</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>50.0574</td>\n",
       "      <td>1.0406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-493be9c20711>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# raw standardized trade loss only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1773\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1774\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1775\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1776\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1777\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2522\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2523\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2525\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2553\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2554\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2555\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2556\u001b[0m         \u001b[1;31m# Save past state if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2557\u001b[0m         \u001b[1;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\trader_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, ohlcv, labels, future, std_future)\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0membed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_embed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mohlcv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\trader_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, mod)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgconv_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_gconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmod\u001b[0m \u001b[1;31m# residual\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\gconv_standalone.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, u, return_kernel)\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;34m'cat'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_scales\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m                 kernel = F.interpolate(\n\u001b[0m\u001b[0;32m    416\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m                     \u001b[0mscale_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_scale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[0;32m   3931\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"linear\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3932\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0malign_corners\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3933\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupsample_linear1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_factors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3934\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"bilinear\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3935\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0malign_corners\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 1e-3, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# raw standardized trade loss only\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1401' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1401/8804 07:39 < 40:31, 3.04 it/s, Epoch 0.16/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.976400</td>\n",
       "      <td>1.001252</td>\n",
       "      <td>-0.0151</td>\n",
       "      <td>-0.0143</td>\n",
       "      <td>49.6754</td>\n",
       "      <td>33.3196</td>\n",
       "      <td>48.9861</td>\n",
       "      <td>1.0003</td>\n",
       "      <td>18.9422</td>\n",
       "      <td>50.237</td>\n",
       "      <td>0.9807</td>\n",
       "      <td>21.3081</td>\n",
       "      <td>49.737</td>\n",
       "      <td>0.9508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.988400</td>\n",
       "      <td>1.039093</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.1433</td>\n",
       "      <td>99.4043</td>\n",
       "      <td>99.404</td>\n",
       "      <td>49.01</td>\n",
       "      <td>0.9803</td>\n",
       "      <td>0.004</td>\n",
       "      <td>52.6316</td>\n",
       "      <td>0.7496</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>50.</td>\n",
       "      <td>1.4809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.009400</td>\n",
       "      <td>0.982469</td>\n",
       "      <td>0.1466</td>\n",
       "      <td>0.1544</td>\n",
       "      <td>99.3778</td>\n",
       "      <td>99.3723</td>\n",
       "      <td>49.5849</td>\n",
       "      <td>1.0106</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>45.2632</td>\n",
       "      <td>0.4408</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>39.0244</td>\n",
       "      <td>1.0017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.990900</td>\n",
       "      <td>0.988304</td>\n",
       "      <td>0.0434</td>\n",
       "      <td>0.2306</td>\n",
       "      <td>99.0826</td>\n",
       "      <td>98.9984</td>\n",
       "      <td>49.5388</td>\n",
       "      <td>0.9906</td>\n",
       "      <td>0.2182</td>\n",
       "      <td>48.6383</td>\n",
       "      <td>1.0226</td>\n",
       "      <td>0.0994</td>\n",
       "      <td>45.4355</td>\n",
       "      <td>0.9784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.985300</td>\n",
       "      <td>0.989877</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.1772</td>\n",
       "      <td>99.1226</td>\n",
       "      <td>99.0430</td>\n",
       "      <td>49.5361</td>\n",
       "      <td>0.9891</td>\n",
       "      <td>0.1884</td>\n",
       "      <td>47.8694</td>\n",
       "      <td>1.0360</td>\n",
       "      <td>0.0961</td>\n",
       "      <td>48.2646</td>\n",
       "      <td>0.8569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.984000</td>\n",
       "      <td>0.989869</td>\n",
       "      <td>0.0309</td>\n",
       "      <td>0.1438</td>\n",
       "      <td>99.2219</td>\n",
       "      <td>99.1705</td>\n",
       "      <td>49.4414</td>\n",
       "      <td>0.9918</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>49.5798</td>\n",
       "      <td>1.0093</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>47.7492</td>\n",
       "      <td>1.1179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/19 00:04 < 00:02, 2.92 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-3383bec22ada>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# raw trade loss only\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1850\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1852\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1853\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2113\u001b[0m                     )\n\u001b[0;32m   2114\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2115\u001b[1;33m                 \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2116\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2809\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2810\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2811\u001b[1;33m         output = eval_loop(\n\u001b[0m\u001b[0;32m   2812\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2813\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Evaluation\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2977\u001b[0m         \u001b[0mobserved_num_examples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2978\u001b[0m         \u001b[1;31m# Main evaluation loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2979\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2980\u001b[0m             \u001b[1;31m# Update the observed num examples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2981\u001b[0m             \u001b[0mobserved_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_batch_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    679\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa: F811\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[1;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2356\u001b[1;33m         return self._getitem(\n\u001b[0m\u001b[0;32m   2357\u001b[0m             \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2358\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[1;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[0;32m   2339\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2340\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2341\u001b[1;33m         formatted_output = format_table(\n\u001b[0m\u001b[0;32m   2342\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2343\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[0mpa_table_to_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_names\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m         \u001b[0mformatted_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table_to_format\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquery_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformatted_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMutableMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mRowFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumnFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchFormat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"row\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"column\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mformat_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPythonFormatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFormatter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mformat_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython_arrow_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m             \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython_features_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mextract_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPythonArrowExtractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseArrowExtractor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pydict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36m_unnest\u001b[1;34m(py_dict)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_dict\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;34m\"\"\"Return the first element of a batch (dict) as a row (dict)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpy_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 1e-3, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# raw trade loss only\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8804' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8804/8804 41:20, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.781300</td>\n",
       "      <td>2.756863</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>53.7656</td>\n",
       "      <td>37.4265</td>\n",
       "      <td>49.3535</td>\n",
       "      <td>0.9936</td>\n",
       "      <td>20.2774</td>\n",
       "      <td>50.2288</td>\n",
       "      <td>0.9773</td>\n",
       "      <td>20.009</td>\n",
       "      <td>49.7681</td>\n",
       "      <td>0.9776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.734300</td>\n",
       "      <td>2.741066</td>\n",
       "      <td>0.0451</td>\n",
       "      <td>0.0966</td>\n",
       "      <td>87.8799</td>\n",
       "      <td>85.2121</td>\n",
       "      <td>50.1085</td>\n",
       "      <td>0.9718</td>\n",
       "      <td>7.3155</td>\n",
       "      <td>49.3194</td>\n",
       "      <td>1.0178</td>\n",
       "      <td>3.6033</td>\n",
       "      <td>48.6501</td>\n",
       "      <td>0.9811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.707400</td>\n",
       "      <td>2.789460</td>\n",
       "      <td>-0.0393</td>\n",
       "      <td>-0.0523</td>\n",
       "      <td>79.8933</td>\n",
       "      <td>74.1092</td>\n",
       "      <td>49.3410</td>\n",
       "      <td>0.979</td>\n",
       "      <td>12.6284</td>\n",
       "      <td>48.7718</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>6.5652</td>\n",
       "      <td>48.9526</td>\n",
       "      <td>0.9985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.714400</td>\n",
       "      <td>2.716656</td>\n",
       "      <td>0.0872</td>\n",
       "      <td>0.1727</td>\n",
       "      <td>86.5141</td>\n",
       "      <td>83.4324</td>\n",
       "      <td>50.1421</td>\n",
       "      <td>0.9784</td>\n",
       "      <td>8.2281</td>\n",
       "      <td>49.3809</td>\n",
       "      <td>0.9992</td>\n",
       "      <td>4.0490</td>\n",
       "      <td>48.8283</td>\n",
       "      <td>1.0061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.691300</td>\n",
       "      <td>2.688407</td>\n",
       "      <td>0.2651</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>84.6416</td>\n",
       "      <td>80.8534</td>\n",
       "      <td>50.7632</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>9.4375</td>\n",
       "      <td>49.7586</td>\n",
       "      <td>1.0188</td>\n",
       "      <td>4.733</td>\n",
       "      <td>49.1838</td>\n",
       "      <td>1.0297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.692400</td>\n",
       "      <td>2.705835</td>\n",
       "      <td>0.1145</td>\n",
       "      <td>0.1616</td>\n",
       "      <td>80.5842</td>\n",
       "      <td>75.4575</td>\n",
       "      <td>50.3905</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>12.2319</td>\n",
       "      <td>49.5606</td>\n",
       "      <td>0.993</td>\n",
       "      <td>6.1151</td>\n",
       "      <td>49.1542</td>\n",
       "      <td>1.0176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.680000</td>\n",
       "      <td>2.706394</td>\n",
       "      <td>0.1271</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>81.3375</td>\n",
       "      <td>76.3626</td>\n",
       "      <td>50.5039</td>\n",
       "      <td>0.973</td>\n",
       "      <td>11.5962</td>\n",
       "      <td>49.9739</td>\n",
       "      <td>0.9832</td>\n",
       "      <td>5.9458</td>\n",
       "      <td>49.7291</td>\n",
       "      <td>0.9945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.679900</td>\n",
       "      <td>2.730697</td>\n",
       "      <td>0.0308</td>\n",
       "      <td>0.0512</td>\n",
       "      <td>80.1932</td>\n",
       "      <td>74.8317</td>\n",
       "      <td>49.9893</td>\n",
       "      <td>0.9740</td>\n",
       "      <td>12.328</td>\n",
       "      <td>48.925</td>\n",
       "      <td>0.9986</td>\n",
       "      <td>6.3755</td>\n",
       "      <td>48.7145</td>\n",
       "      <td>0.9989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.674000</td>\n",
       "      <td>2.735168</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>83.2793</td>\n",
       "      <td>78.9759</td>\n",
       "      <td>50.2112</td>\n",
       "      <td>0.9587</td>\n",
       "      <td>10.3582</td>\n",
       "      <td>49.7066</td>\n",
       "      <td>0.9779</td>\n",
       "      <td>5.2473</td>\n",
       "      <td>49.2250</td>\n",
       "      <td>0.9937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.647300</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-bb1c47517fd0>:15: RuntimeWarning: Mean of empty slice.\n",
      "  'full trade accuracy': (soft_profit[abs_trade >= .7] > 0).mean() * 100,\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:16: RuntimeWarning: Mean of empty slice.\n",
      "  'full trade g/l': soft_profit[(abs_trade >= .7) & (soft_profit > 0)].mean()\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:17: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade >= .7) & (soft_profit < 0)].mean(),\n",
      "<ipython-input-2-bb1c47517fd0>:20: RuntimeWarning: Mean of empty slice.\n",
      "  'medium trade accuracy': (soft_profit[(abs_trade < .7) & (abs_trade >= .4)] > 0).mean() * 100,\n",
      "<ipython-input-2-bb1c47517fd0>:21: RuntimeWarning: Mean of empty slice.\n",
      "  'medium trade g/l': soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit > 0)].mean()\n",
      "<ipython-input-2-bb1c47517fd0>:22: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit < 0)].mean(),\n",
      "<ipython-input-2-bb1c47517fd0>:25: RuntimeWarning: Mean of empty slice.\n",
      "  'small trade accuracy': (soft_profit[(abs_trade < .4) & (abs_trade >= .2)] > 0).mean() * 100,\n",
      "<ipython-input-2-bb1c47517fd0>:26: RuntimeWarning: Mean of empty slice.\n",
      "  'small trade g/l': soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit > 0)].mean()\n",
      "<ipython-input-2-bb1c47517fd0>:27: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit < 0)].mean(),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8804, training_loss=0.6134062666938501, metrics={'train_runtime': 2483.5506, 'train_samples_per_second': 14.178, 'train_steps_per_second': 3.545, 'total_flos': 0.0, 'train_loss': 0.6134062666938501, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 1e-3, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# .2 lq loss loss with raw trade loss NO PUSH LOSS\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2327' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2327/8804 11:19 < 31:32, 3.42 it/s, Epoch 0.26/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.819100</td>\n",
       "      <td>2.838762</td>\n",
       "      <td>-0.0095</td>\n",
       "      <td>-0.0138</td>\n",
       "      <td>36.4216</td>\n",
       "      <td>17.5197</td>\n",
       "      <td>49.351</td>\n",
       "      <td>0.9738</td>\n",
       "      <td>19.7983</td>\n",
       "      <td>50.1754</td>\n",
       "      <td>0.9648</td>\n",
       "      <td>23.8931</td>\n",
       "      <td>50.1074</td>\n",
       "      <td>0.9771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.787400</td>\n",
       "      <td>2.771592</td>\n",
       "      <td>0.1793</td>\n",
       "      <td>0.2182</td>\n",
       "      <td>50.5179</td>\n",
       "      <td>31.9630</td>\n",
       "      <td>49.971</td>\n",
       "      <td>1.0432</td>\n",
       "      <td>26.7961</td>\n",
       "      <td>49.9683</td>\n",
       "      <td>1.0113</td>\n",
       "      <td>20.2543</td>\n",
       "      <td>49.7485</td>\n",
       "      <td>1.0038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.757300</td>\n",
       "      <td>2.944452</td>\n",
       "      <td>-0.0065</td>\n",
       "      <td>-0.0059</td>\n",
       "      <td>34.0809</td>\n",
       "      <td>11.5596</td>\n",
       "      <td>49.0515</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>21.6668</td>\n",
       "      <td>49.2649</td>\n",
       "      <td>0.9768</td>\n",
       "      <td>34.5217</td>\n",
       "      <td>49.68</td>\n",
       "      <td>0.9793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.734000</td>\n",
       "      <td>2.651019</td>\n",
       "      <td>0.2243</td>\n",
       "      <td>0.2902</td>\n",
       "      <td>29.3414</td>\n",
       "      <td>10.8391</td>\n",
       "      <td>54.544</td>\n",
       "      <td>1.1707</td>\n",
       "      <td>18.3047</td>\n",
       "      <td>50.7613</td>\n",
       "      <td>1.0008</td>\n",
       "      <td>21.3139</td>\n",
       "      <td>49.4142</td>\n",
       "      <td>0.9797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.640100</td>\n",
       "      <td>2.554304</td>\n",
       "      <td>0.2461</td>\n",
       "      <td>0.4908</td>\n",
       "      <td>17.1460</td>\n",
       "      <td>6.066</td>\n",
       "      <td>56.8096</td>\n",
       "      <td>1.274</td>\n",
       "      <td>5.4457</td>\n",
       "      <td>54.4824</td>\n",
       "      <td>1.0372</td>\n",
       "      <td>11.9706</td>\n",
       "      <td>51.7426</td>\n",
       "      <td>0.9856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>2.527699</td>\n",
       "      <td>0.1655</td>\n",
       "      <td>0.3754</td>\n",
       "      <td>15.9433</td>\n",
       "      <td>7.9986</td>\n",
       "      <td>51.0429</td>\n",
       "      <td>1.1168</td>\n",
       "      <td>4.1646</td>\n",
       "      <td>53.5128</td>\n",
       "      <td>1.1079</td>\n",
       "      <td>5.5124</td>\n",
       "      <td>52.9319</td>\n",
       "      <td>1.0077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.592000</td>\n",
       "      <td>2.633106</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.005</td>\n",
       "      <td>17.8361</td>\n",
       "      <td>10.8554</td>\n",
       "      <td>48.7254</td>\n",
       "      <td>0.9214</td>\n",
       "      <td>4.3460</td>\n",
       "      <td>49.1675</td>\n",
       "      <td>0.9575</td>\n",
       "      <td>5.7405</td>\n",
       "      <td>51.1135</td>\n",
       "      <td>1.0264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.588000</td>\n",
       "      <td>2.577155</td>\n",
       "      <td>0.0436</td>\n",
       "      <td>0.1960</td>\n",
       "      <td>9.6322</td>\n",
       "      <td>6.13</td>\n",
       "      <td>49.7916</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>1.7207</td>\n",
       "      <td>45.9520</td>\n",
       "      <td>0.9836</td>\n",
       "      <td>2.2093</td>\n",
       "      <td>50.5852</td>\n",
       "      <td>1.0762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.576000</td>\n",
       "      <td>2.615435</td>\n",
       "      <td>0.0464</td>\n",
       "      <td>0.1289</td>\n",
       "      <td>11.6488</td>\n",
       "      <td>4.1609</td>\n",
       "      <td>50.2995</td>\n",
       "      <td>0.9181</td>\n",
       "      <td>2.7047</td>\n",
       "      <td>50.2178</td>\n",
       "      <td>0.9215</td>\n",
       "      <td>5.3606</td>\n",
       "      <td>50.0895</td>\n",
       "      <td>0.9885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.566500</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-bb1c47517fd0>:15: RuntimeWarning: Mean of empty slice.\n",
      "  'full trade accuracy': (soft_profit[abs_trade >= .7] > 0).mean() * 100,\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:16: RuntimeWarning: Mean of empty slice.\n",
      "  'full trade g/l': soft_profit[(abs_trade >= .7) & (soft_profit > 0)].mean()\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:17: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade >= .7) & (soft_profit < 0)].mean(),\n",
      "<ipython-input-2-bb1c47517fd0>:20: RuntimeWarning: Mean of empty slice.\n",
      "  'medium trade accuracy': (soft_profit[(abs_trade < .7) & (abs_trade >= .4)] > 0).mean() * 100,\n",
      "<ipython-input-2-bb1c47517fd0>:21: RuntimeWarning: Mean of empty slice.\n",
      "  'medium trade g/l': soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit > 0)].mean()\n",
      "<ipython-input-2-bb1c47517fd0>:22: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit < 0)].mean(),\n",
      "<ipython-input-2-bb1c47517fd0>:25: RuntimeWarning: Mean of empty slice.\n",
      "  'small trade accuracy': (soft_profit[(abs_trade < .4) & (abs_trade >= .2)] > 0).mean() * 100,\n",
      "<ipython-input-2-bb1c47517fd0>:26: RuntimeWarning: Mean of empty slice.\n",
      "  'small trade g/l': soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit > 0)].mean()\n",
      "<ipython-input-2-bb1c47517fd0>:27: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit < 0)].mean(),\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ce5335d59fa3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# .2 lq loss loss with raw trade loss + .1 push loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1748\u001b[0m             \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1749\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1751\u001b[0m                 \u001b[1;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    679\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa: F811\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[1;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2356\u001b[1;33m         return self._getitem(\n\u001b[0m\u001b[0;32m   2357\u001b[0m             \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2358\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[1;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[0;32m   2339\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2340\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2341\u001b[1;33m         formatted_output = format_table(\n\u001b[0m\u001b[0;32m   2342\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2343\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[0mpa_table_to_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_names\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m         \u001b[0mformatted_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table_to_format\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquery_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformatted_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMutableMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mRowFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumnFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchFormat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"row\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"column\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mformat_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPythonFormatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFormatter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mformat_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython_arrow_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m             \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython_features_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mextract_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPythonArrowExtractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseArrowExtractor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pydict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36m_unnest\u001b[1;34m(py_dict)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_dict\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;34m\"\"\"Return the first element of a batch (dict) as a row (dict)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpy_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 1e-3, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# .2 lq loss loss with raw trade loss + .1 push loss\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8804' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8804/8804 46:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.767900</td>\n",
       "      <td>1.738958</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>7.8634</td>\n",
       "      <td>0.0848</td>\n",
       "      <td>3.4440</td>\n",
       "      <td>0.3631</td>\n",
       "      <td>0.5307</td>\n",
       "      <td>9.7446</td>\n",
       "      <td>1.0446</td>\n",
       "      <td>2.9672</td>\n",
       "      <td>46.8496</td>\n",
       "      <td>0.9723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.746500</td>\n",
       "      <td>1.736832</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>0.1811</td>\n",
       "      <td>5.6434</td>\n",
       "      <td>0.0594</td>\n",
       "      <td>3.6842</td>\n",
       "      <td>0.4553</td>\n",
       "      <td>0.2954</td>\n",
       "      <td>7.7656</td>\n",
       "      <td>0.9964</td>\n",
       "      <td>0.951</td>\n",
       "      <td>38.5636</td>\n",
       "      <td>0.9212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.738700</td>\n",
       "      <td>1.734990</td>\n",
       "      <td>-0.0260</td>\n",
       "      <td>-0.0714</td>\n",
       "      <td>8.0954</td>\n",
       "      <td>0.7391</td>\n",
       "      <td>4.7545</td>\n",
       "      <td>1.0316</td>\n",
       "      <td>0.6006</td>\n",
       "      <td>39.6875</td>\n",
       "      <td>0.9568</td>\n",
       "      <td>4.1751</td>\n",
       "      <td>50.3908</td>\n",
       "      <td>0.9673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.739200</td>\n",
       "      <td>1.732199</td>\n",
       "      <td>0.0441</td>\n",
       "      <td>0.3006</td>\n",
       "      <td>7.8234</td>\n",
       "      <td>0.6734</td>\n",
       "      <td>0.2013</td>\n",
       "      <td>0.2858</td>\n",
       "      <td>0.2130</td>\n",
       "      <td>35.9276</td>\n",
       "      <td>0.6722</td>\n",
       "      <td>4.0413</td>\n",
       "      <td>53.9940</td>\n",
       "      <td>1.0092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.740500</td>\n",
       "      <td>1.732327</td>\n",
       "      <td>0.0503</td>\n",
       "      <td>0.3284</td>\n",
       "      <td>8.3881</td>\n",
       "      <td>0.7408</td>\n",
       "      <td>1.3512</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>0.2193</td>\n",
       "      <td>20.7798</td>\n",
       "      <td>1.1101</td>\n",
       "      <td>4.7617</td>\n",
       "      <td>55.6879</td>\n",
       "      <td>1.0050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.738800</td>\n",
       "      <td>1.734079</td>\n",
       "      <td>0.0266</td>\n",
       "      <td>0.1429</td>\n",
       "      <td>8.9884</td>\n",
       "      <td>0.2655</td>\n",
       "      <td>2.5530</td>\n",
       "      <td>0.4279</td>\n",
       "      <td>0.5202</td>\n",
       "      <td>25.4961</td>\n",
       "      <td>0.8175</td>\n",
       "      <td>7.1411</td>\n",
       "      <td>51.9843</td>\n",
       "      <td>0.9039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.738100</td>\n",
       "      <td>1.731660</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>0.1976</td>\n",
       "      <td>8.4354</td>\n",
       "      <td>0.8155</td>\n",
       "      <td>7.2625</td>\n",
       "      <td>1.0203</td>\n",
       "      <td>0.6450</td>\n",
       "      <td>59.6185</td>\n",
       "      <td>0.8404</td>\n",
       "      <td>5.4922</td>\n",
       "      <td>54.3666</td>\n",
       "      <td>0.9104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.728600</td>\n",
       "      <td>1.730940</td>\n",
       "      <td>-0.0016</td>\n",
       "      <td>-0.0105</td>\n",
       "      <td>6.9809</td>\n",
       "      <td>0.7686</td>\n",
       "      <td>0.7055</td>\n",
       "      <td>0.6301</td>\n",
       "      <td>0.1985</td>\n",
       "      <td>44.6954</td>\n",
       "      <td>0.7424</td>\n",
       "      <td>3.2619</td>\n",
       "      <td>54.9372</td>\n",
       "      <td>0.947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.726900</td>\n",
       "      <td>1.727873</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>0.1735</td>\n",
       "      <td>6.1500</td>\n",
       "      <td>0.9696</td>\n",
       "      <td>0.9033</td>\n",
       "      <td>0.4498</td>\n",
       "      <td>0.1461</td>\n",
       "      <td>62.5268</td>\n",
       "      <td>0.7402</td>\n",
       "      <td>1.6493</td>\n",
       "      <td>58.4308</td>\n",
       "      <td>0.7380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.729100</td>\n",
       "      <td>1.728224</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0534</td>\n",
       "      <td>6.608</td>\n",
       "      <td>0.6252</td>\n",
       "      <td>4.2028</td>\n",
       "      <td>0.7196</td>\n",
       "      <td>0.3623</td>\n",
       "      <td>39.0504</td>\n",
       "      <td>0.6199</td>\n",
       "      <td>1.772</td>\n",
       "      <td>57.1143</td>\n",
       "      <td>0.7667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.733000</td>\n",
       "      <td>1.728085</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>7.1051</td>\n",
       "      <td>0.9358</td>\n",
       "      <td>3.1755</td>\n",
       "      <td>0.5053</td>\n",
       "      <td>0.2068</td>\n",
       "      <td>67.171</td>\n",
       "      <td>0.6285</td>\n",
       "      <td>2.0017</td>\n",
       "      <td>59.0040</td>\n",
       "      <td>0.8868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.732000</td>\n",
       "      <td>1.727854</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.1709</td>\n",
       "      <td>6.8922</td>\n",
       "      <td>0.9952</td>\n",
       "      <td>3.2376</td>\n",
       "      <td>0.4952</td>\n",
       "      <td>0.3926</td>\n",
       "      <td>69.6149</td>\n",
       "      <td>1.0169</td>\n",
       "      <td>2.8846</td>\n",
       "      <td>59.3479</td>\n",
       "      <td>0.7832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.728000</td>\n",
       "      <td>1.728638</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0234</td>\n",
       "      <td>6.9644</td>\n",
       "      <td>0.9236</td>\n",
       "      <td>5.0802</td>\n",
       "      <td>0.6021</td>\n",
       "      <td>0.7445</td>\n",
       "      <td>62.9132</td>\n",
       "      <td>1.0667</td>\n",
       "      <td>2.6988</td>\n",
       "      <td>57.6672</td>\n",
       "      <td>0.8839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.725700</td>\n",
       "      <td>1.726201</td>\n",
       "      <td>0.0407</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>8.5882</td>\n",
       "      <td>0.9449</td>\n",
       "      <td>8.1880</td>\n",
       "      <td>0.4726</td>\n",
       "      <td>0.7985</td>\n",
       "      <td>68.151</td>\n",
       "      <td>1.0236</td>\n",
       "      <td>4.7292</td>\n",
       "      <td>55.6256</td>\n",
       "      <td>0.9481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.730600</td>\n",
       "      <td>1.727441</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>0.2594</td>\n",
       "      <td>7.0719</td>\n",
       "      <td>1.0009</td>\n",
       "      <td>2.7295</td>\n",
       "      <td>0.6423</td>\n",
       "      <td>0.207</td>\n",
       "      <td>74.8615</td>\n",
       "      <td>0.8878</td>\n",
       "      <td>1.4015</td>\n",
       "      <td>61.9522</td>\n",
       "      <td>0.7648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.732300</td>\n",
       "      <td>1.724114</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>-0.0055</td>\n",
       "      <td>8.4029</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.9874</td>\n",
       "      <td>0.639</td>\n",
       "      <td>0.7191</td>\n",
       "      <td>58.6862</td>\n",
       "      <td>1.2528</td>\n",
       "      <td>4.1938</td>\n",
       "      <td>57.7772</td>\n",
       "      <td>0.8852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.733100</td>\n",
       "      <td>1.732712</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>0.1342</td>\n",
       "      <td>8.2996</td>\n",
       "      <td>0.9846</td>\n",
       "      <td>4.0241</td>\n",
       "      <td>1.1595</td>\n",
       "      <td>0.5282</td>\n",
       "      <td>66.1469</td>\n",
       "      <td>1.1430</td>\n",
       "      <td>2.6786</td>\n",
       "      <td>56.7753</td>\n",
       "      <td>0.9803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.733900</td>\n",
       "      <td>1.725187</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>6.8688</td>\n",
       "      <td>0.832</td>\n",
       "      <td>4.4742</td>\n",
       "      <td>0.6534</td>\n",
       "      <td>0.6523</td>\n",
       "      <td>63.3792</td>\n",
       "      <td>1.5445</td>\n",
       "      <td>3.0047</td>\n",
       "      <td>58.8388</td>\n",
       "      <td>0.9952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.726300</td>\n",
       "      <td>1.724151</td>\n",
       "      <td>0.0349</td>\n",
       "      <td>0.3592</td>\n",
       "      <td>7.0265</td>\n",
       "      <td>0.9184</td>\n",
       "      <td>2.9064</td>\n",
       "      <td>1.1024</td>\n",
       "      <td>0.3123</td>\n",
       "      <td>62.4708</td>\n",
       "      <td>1.0148</td>\n",
       "      <td>2.5725</td>\n",
       "      <td>58.4046</td>\n",
       "      <td>0.8736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.727300</td>\n",
       "      <td>1.723299</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.2391</td>\n",
       "      <td>8.4639</td>\n",
       "      <td>1.0770</td>\n",
       "      <td>9.5653</td>\n",
       "      <td>1.7269</td>\n",
       "      <td>1.6205</td>\n",
       "      <td>66.0125</td>\n",
       "      <td>1.3505</td>\n",
       "      <td>4.6615</td>\n",
       "      <td>57.3033</td>\n",
       "      <td>0.8935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.727200</td>\n",
       "      <td>1.723592</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.25</td>\n",
       "      <td>9.7334</td>\n",
       "      <td>1.1091</td>\n",
       "      <td>12.1463</td>\n",
       "      <td>1.5074</td>\n",
       "      <td>1.8781</td>\n",
       "      <td>61.0204</td>\n",
       "      <td>1.1423</td>\n",
       "      <td>5.3659</td>\n",
       "      <td>54.9270</td>\n",
       "      <td>0.9060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.730100</td>\n",
       "      <td>1.719951</td>\n",
       "      <td>0.0472</td>\n",
       "      <td>0.4229</td>\n",
       "      <td>7.3559</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>8.8205</td>\n",
       "      <td>0.8117</td>\n",
       "      <td>0.5404</td>\n",
       "      <td>66.4287</td>\n",
       "      <td>1.0116</td>\n",
       "      <td>3.0455</td>\n",
       "      <td>61.1442</td>\n",
       "      <td>0.9172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.728900</td>\n",
       "      <td>1.723643</td>\n",
       "      <td>0.0439</td>\n",
       "      <td>0.3497</td>\n",
       "      <td>7.0903</td>\n",
       "      <td>0.9615</td>\n",
       "      <td>8.9361</td>\n",
       "      <td>4.2955</td>\n",
       "      <td>0.9478</td>\n",
       "      <td>63.5534</td>\n",
       "      <td>1.6731</td>\n",
       "      <td>2.7472</td>\n",
       "      <td>58.9479</td>\n",
       "      <td>0.9648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.724800</td>\n",
       "      <td>1.723284</td>\n",
       "      <td>0.0309</td>\n",
       "      <td>0.2804</td>\n",
       "      <td>7.8635</td>\n",
       "      <td>1.0094</td>\n",
       "      <td>3.8942</td>\n",
       "      <td>0.5700</td>\n",
       "      <td>0.8263</td>\n",
       "      <td>67.6845</td>\n",
       "      <td>1.1515</td>\n",
       "      <td>3.8927</td>\n",
       "      <td>57.8952</td>\n",
       "      <td>0.9109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.722100</td>\n",
       "      <td>1.722342</td>\n",
       "      <td>0.0422</td>\n",
       "      <td>0.3920</td>\n",
       "      <td>8.0857</td>\n",
       "      <td>1.0973</td>\n",
       "      <td>7.6492</td>\n",
       "      <td>2.1014</td>\n",
       "      <td>1.2932</td>\n",
       "      <td>63.0705</td>\n",
       "      <td>1.0818</td>\n",
       "      <td>3.7001</td>\n",
       "      <td>57.1875</td>\n",
       "      <td>0.9523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.728100</td>\n",
       "      <td>1.723404</td>\n",
       "      <td>0.0430</td>\n",
       "      <td>0.4276</td>\n",
       "      <td>7.6425</td>\n",
       "      <td>0.9278</td>\n",
       "      <td>5.3270</td>\n",
       "      <td>3.3813</td>\n",
       "      <td>1.0698</td>\n",
       "      <td>65.7602</td>\n",
       "      <td>1.4389</td>\n",
       "      <td>3.6165</td>\n",
       "      <td>60.3246</td>\n",
       "      <td>0.9902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.720100</td>\n",
       "      <td>1.721338</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>0.2982</td>\n",
       "      <td>7.3634</td>\n",
       "      <td>1.0074</td>\n",
       "      <td>6.076</td>\n",
       "      <td>1.4228</td>\n",
       "      <td>0.9004</td>\n",
       "      <td>65.6862</td>\n",
       "      <td>1.1703</td>\n",
       "      <td>3.4221</td>\n",
       "      <td>60.0475</td>\n",
       "      <td>0.9747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.725000</td>\n",
       "      <td>1.720008</td>\n",
       "      <td>0.0451</td>\n",
       "      <td>0.4053</td>\n",
       "      <td>8.0459</td>\n",
       "      <td>1.0825</td>\n",
       "      <td>11.6452</td>\n",
       "      <td>3.3392</td>\n",
       "      <td>1.7104</td>\n",
       "      <td>64.9820</td>\n",
       "      <td>1.3491</td>\n",
       "      <td>3.679</td>\n",
       "      <td>57.8097</td>\n",
       "      <td>0.9547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.718000</td>\n",
       "      <td>1.719815</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.4266</td>\n",
       "      <td>7.8523</td>\n",
       "      <td>1.2119</td>\n",
       "      <td>14.7810</td>\n",
       "      <td>1.4840</td>\n",
       "      <td>1.7934</td>\n",
       "      <td>64.4514</td>\n",
       "      <td>1.257</td>\n",
       "      <td>3.7469</td>\n",
       "      <td>57.4386</td>\n",
       "      <td>0.9367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.718300</td>\n",
       "      <td>1.721889</td>\n",
       "      <td>0.0421</td>\n",
       "      <td>0.4166</td>\n",
       "      <td>7.1662</td>\n",
       "      <td>1.0268</td>\n",
       "      <td>6.9869</td>\n",
       "      <td>1.6675</td>\n",
       "      <td>0.6542</td>\n",
       "      <td>69.2859</td>\n",
       "      <td>1.1334</td>\n",
       "      <td>2.9982</td>\n",
       "      <td>60.642</td>\n",
       "      <td>1.0252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.728900</td>\n",
       "      <td>1.720937</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>0.4045</td>\n",
       "      <td>8.3310</td>\n",
       "      <td>1.2336</td>\n",
       "      <td>16.1187</td>\n",
       "      <td>2.2286</td>\n",
       "      <td>2.0071</td>\n",
       "      <td>62.1019</td>\n",
       "      <td>1.0970</td>\n",
       "      <td>4.2304</td>\n",
       "      <td>56.6809</td>\n",
       "      <td>0.9541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.718800</td>\n",
       "      <td>1.721552</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.4284</td>\n",
       "      <td>8.9854</td>\n",
       "      <td>1.4927</td>\n",
       "      <td>25.3353</td>\n",
       "      <td>1.7108</td>\n",
       "      <td>2.3659</td>\n",
       "      <td>61.9612</td>\n",
       "      <td>1.0963</td>\n",
       "      <td>4.4651</td>\n",
       "      <td>55.6326</td>\n",
       "      <td>1.0139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.727000</td>\n",
       "      <td>1.721047</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>0.4432</td>\n",
       "      <td>7.5934</td>\n",
       "      <td>0.9853</td>\n",
       "      <td>9.6201</td>\n",
       "      <td>2.7408</td>\n",
       "      <td>0.8595</td>\n",
       "      <td>64.2242</td>\n",
       "      <td>1.0869</td>\n",
       "      <td>3.3662</td>\n",
       "      <td>60.0966</td>\n",
       "      <td>1.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.719400</td>\n",
       "      <td>1.719790</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.4972</td>\n",
       "      <td>9.0106</td>\n",
       "      <td>1.2764</td>\n",
       "      <td>20.5784</td>\n",
       "      <td>2.5759</td>\n",
       "      <td>1.8154</td>\n",
       "      <td>65.8415</td>\n",
       "      <td>1.1508</td>\n",
       "      <td>4.5524</td>\n",
       "      <td>58.1759</td>\n",
       "      <td>0.979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.721600</td>\n",
       "      <td>1.718027</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.4891</td>\n",
       "      <td>7.8056</td>\n",
       "      <td>1.0938</td>\n",
       "      <td>12.2307</td>\n",
       "      <td>4.046</td>\n",
       "      <td>1.3889</td>\n",
       "      <td>66.9369</td>\n",
       "      <td>1.2348</td>\n",
       "      <td>3.3301</td>\n",
       "      <td>59.6174</td>\n",
       "      <td>1.0333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.721200</td>\n",
       "      <td>1.719686</td>\n",
       "      <td>0.0512</td>\n",
       "      <td>0.4268</td>\n",
       "      <td>8.4993</td>\n",
       "      <td>1.1515</td>\n",
       "      <td>12.723</td>\n",
       "      <td>2.8373</td>\n",
       "      <td>1.6251</td>\n",
       "      <td>64.7995</td>\n",
       "      <td>1.0383</td>\n",
       "      <td>4.1605</td>\n",
       "      <td>58.0437</td>\n",
       "      <td>0.9637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.720800</td>\n",
       "      <td>1.718495</td>\n",
       "      <td>0.0553</td>\n",
       "      <td>0.5456</td>\n",
       "      <td>8.2826</td>\n",
       "      <td>1.1992</td>\n",
       "      <td>14.9378</td>\n",
       "      <td>2.8032</td>\n",
       "      <td>1.5785</td>\n",
       "      <td>67.6906</td>\n",
       "      <td>1.2444</td>\n",
       "      <td>3.9278</td>\n",
       "      <td>59.0671</td>\n",
       "      <td>1.0283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.721900</td>\n",
       "      <td>1.719210</td>\n",
       "      <td>0.0611</td>\n",
       "      <td>0.4696</td>\n",
       "      <td>8.5959</td>\n",
       "      <td>1.1566</td>\n",
       "      <td>13.6585</td>\n",
       "      <td>3.0975</td>\n",
       "      <td>1.515</td>\n",
       "      <td>67.4100</td>\n",
       "      <td>1.2233</td>\n",
       "      <td>4.0975</td>\n",
       "      <td>59.1801</td>\n",
       "      <td>1.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.719800</td>\n",
       "      <td>1.718794</td>\n",
       "      <td>0.0482</td>\n",
       "      <td>0.4866</td>\n",
       "      <td>8.2432</td>\n",
       "      <td>1.1839</td>\n",
       "      <td>14.5852</td>\n",
       "      <td>3.1694</td>\n",
       "      <td>1.6058</td>\n",
       "      <td>67.8571</td>\n",
       "      <td>1.2040</td>\n",
       "      <td>3.7726</td>\n",
       "      <td>58.8154</td>\n",
       "      <td>1.0435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.719900</td>\n",
       "      <td>1.718403</td>\n",
       "      <td>0.0533</td>\n",
       "      <td>0.4721</td>\n",
       "      <td>8.6269</td>\n",
       "      <td>1.2975</td>\n",
       "      <td>20.2025</td>\n",
       "      <td>2.5375</td>\n",
       "      <td>1.8996</td>\n",
       "      <td>66.8899</td>\n",
       "      <td>1.1571</td>\n",
       "      <td>3.9745</td>\n",
       "      <td>58.4149</td>\n",
       "      <td>1.0157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.724300</td>\n",
       "      <td>1.718422</td>\n",
       "      <td>0.0489</td>\n",
       "      <td>0.437</td>\n",
       "      <td>8.6555</td>\n",
       "      <td>1.2718</td>\n",
       "      <td>19.2506</td>\n",
       "      <td>2.5986</td>\n",
       "      <td>1.9012</td>\n",
       "      <td>66.9994</td>\n",
       "      <td>1.1644</td>\n",
       "      <td>4.0544</td>\n",
       "      <td>58.3906</td>\n",
       "      <td>1.0058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>1.720300</td>\n",
       "      <td>1.718283</td>\n",
       "      <td>0.0507</td>\n",
       "      <td>0.4561</td>\n",
       "      <td>8.6673</td>\n",
       "      <td>1.2603</td>\n",
       "      <td>18.2510</td>\n",
       "      <td>2.6042</td>\n",
       "      <td>1.8951</td>\n",
       "      <td>67.0867</td>\n",
       "      <td>1.1553</td>\n",
       "      <td>4.1519</td>\n",
       "      <td>58.4258</td>\n",
       "      <td>1.0176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>1.726200</td>\n",
       "      <td>1.718291</td>\n",
       "      <td>0.0494</td>\n",
       "      <td>0.4414</td>\n",
       "      <td>8.6476</td>\n",
       "      <td>1.2476</td>\n",
       "      <td>17.5178</td>\n",
       "      <td>2.6824</td>\n",
       "      <td>1.8644</td>\n",
       "      <td>67.1756</td>\n",
       "      <td>1.1592</td>\n",
       "      <td>4.0780</td>\n",
       "      <td>58.4761</td>\n",
       "      <td>1.0209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>1.717200</td>\n",
       "      <td>1.718279</td>\n",
       "      <td>0.0494</td>\n",
       "      <td>0.4431</td>\n",
       "      <td>8.6292</td>\n",
       "      <td>1.2433</td>\n",
       "      <td>17.3180</td>\n",
       "      <td>2.7285</td>\n",
       "      <td>1.8551</td>\n",
       "      <td>67.1576</td>\n",
       "      <td>1.1591</td>\n",
       "      <td>4.0584</td>\n",
       "      <td>58.5068</td>\n",
       "      <td>1.0206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8804, training_loss=1.7283145271069025, metrics={'train_runtime': 2770.0394, 'train_samples_per_second': 12.712, 'train_steps_per_second': 3.178, 'total_flos': 0.0, 'train_loss': 1.7283145271069025, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 1e-3, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# .2 lq loss loss with conditioned kelly betting\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8804' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8804/8804 45:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.769000</td>\n",
       "      <td>1.738859</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>7.7581</td>\n",
       "      <td>0.0675</td>\n",
       "      <td>5.1005</td>\n",
       "      <td>0.5092</td>\n",
       "      <td>0.6662</td>\n",
       "      <td>8.0607</td>\n",
       "      <td>0.9528</td>\n",
       "      <td>2.9350</td>\n",
       "      <td>46.1738</td>\n",
       "      <td>0.9952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.746900</td>\n",
       "      <td>1.741128</td>\n",
       "      <td>0.0327</td>\n",
       "      <td>0.2267</td>\n",
       "      <td>6.3100</td>\n",
       "      <td>0.0602</td>\n",
       "      <td>9.0121</td>\n",
       "      <td>0.9568</td>\n",
       "      <td>0.1225</td>\n",
       "      <td>23.4894</td>\n",
       "      <td>1.0995</td>\n",
       "      <td>0.9523</td>\n",
       "      <td>48.2536</td>\n",
       "      <td>0.8899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.739400</td>\n",
       "      <td>1.735708</td>\n",
       "      <td>-0.0179</td>\n",
       "      <td>-0.0499</td>\n",
       "      <td>8.0658</td>\n",
       "      <td>0.6686</td>\n",
       "      <td>5.8016</td>\n",
       "      <td>1.0877</td>\n",
       "      <td>0.676</td>\n",
       "      <td>36.2024</td>\n",
       "      <td>0.9409</td>\n",
       "      <td>4.3464</td>\n",
       "      <td>51.2067</td>\n",
       "      <td>0.9732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.739800</td>\n",
       "      <td>1.732127</td>\n",
       "      <td>0.0456</td>\n",
       "      <td>0.3137</td>\n",
       "      <td>7.8072</td>\n",
       "      <td>0.5820</td>\n",
       "      <td>0.2687</td>\n",
       "      <td>0.3573</td>\n",
       "      <td>0.2622</td>\n",
       "      <td>21.5905</td>\n",
       "      <td>0.8422</td>\n",
       "      <td>3.9435</td>\n",
       "      <td>53.7229</td>\n",
       "      <td>1.0972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.740600</td>\n",
       "      <td>1.733492</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>0.3737</td>\n",
       "      <td>8.0622</td>\n",
       "      <td>0.8037</td>\n",
       "      <td>0.7654</td>\n",
       "      <td>0.3936</td>\n",
       "      <td>0.1855</td>\n",
       "      <td>39.2355</td>\n",
       "      <td>1.0678</td>\n",
       "      <td>3.7263</td>\n",
       "      <td>56.8626</td>\n",
       "      <td>1.0014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.739100</td>\n",
       "      <td>1.737770</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.1573</td>\n",
       "      <td>8.93</td>\n",
       "      <td>0.0937</td>\n",
       "      <td>9.2325</td>\n",
       "      <td>0.4510</td>\n",
       "      <td>0.3389</td>\n",
       "      <td>49.4769</td>\n",
       "      <td>0.7329</td>\n",
       "      <td>7.5704</td>\n",
       "      <td>52.5144</td>\n",
       "      <td>0.9193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.738300</td>\n",
       "      <td>1.730162</td>\n",
       "      <td>0.0399</td>\n",
       "      <td>0.2174</td>\n",
       "      <td>8.3061</td>\n",
       "      <td>0.7052</td>\n",
       "      <td>7.3192</td>\n",
       "      <td>0.7868</td>\n",
       "      <td>0.7103</td>\n",
       "      <td>53.2443</td>\n",
       "      <td>0.8683</td>\n",
       "      <td>5.1824</td>\n",
       "      <td>54.0231</td>\n",
       "      <td>0.9193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.728600</td>\n",
       "      <td>1.729878</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>7.5909</td>\n",
       "      <td>0.8395</td>\n",
       "      <td>3.1673</td>\n",
       "      <td>0.8778</td>\n",
       "      <td>0.3588</td>\n",
       "      <td>51.2932</td>\n",
       "      <td>0.9267</td>\n",
       "      <td>4.1465</td>\n",
       "      <td>55.8679</td>\n",
       "      <td>0.9716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.726900</td>\n",
       "      <td>1.728634</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.1951</td>\n",
       "      <td>6.6059</td>\n",
       "      <td>1.0068</td>\n",
       "      <td>1.5742</td>\n",
       "      <td>0.5569</td>\n",
       "      <td>0.1426</td>\n",
       "      <td>61.4035</td>\n",
       "      <td>0.8197</td>\n",
       "      <td>2.1297</td>\n",
       "      <td>57.3366</td>\n",
       "      <td>0.8559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.729100</td>\n",
       "      <td>1.727880</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0441</td>\n",
       "      <td>7.2841</td>\n",
       "      <td>0.8298</td>\n",
       "      <td>4.1342</td>\n",
       "      <td>0.9723</td>\n",
       "      <td>0.4094</td>\n",
       "      <td>47.1982</td>\n",
       "      <td>0.6636</td>\n",
       "      <td>1.8726</td>\n",
       "      <td>59.1848</td>\n",
       "      <td>0.7415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.733400</td>\n",
       "      <td>1.727077</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0521</td>\n",
       "      <td>8.1417</td>\n",
       "      <td>0.9395</td>\n",
       "      <td>2.5527</td>\n",
       "      <td>0.3907</td>\n",
       "      <td>0.4939</td>\n",
       "      <td>67.9755</td>\n",
       "      <td>1.1045</td>\n",
       "      <td>4.1744</td>\n",
       "      <td>56.5095</td>\n",
       "      <td>0.9114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.731700</td>\n",
       "      <td>1.726370</td>\n",
       "      <td>0.0278</td>\n",
       "      <td>0.212</td>\n",
       "      <td>7.7957</td>\n",
       "      <td>1.0191</td>\n",
       "      <td>3.7242</td>\n",
       "      <td>0.5522</td>\n",
       "      <td>1.0133</td>\n",
       "      <td>68.3988</td>\n",
       "      <td>1.0751</td>\n",
       "      <td>4.0493</td>\n",
       "      <td>58.0831</td>\n",
       "      <td>0.8671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.728500</td>\n",
       "      <td>1.726186</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>6.4835</td>\n",
       "      <td>0.9484</td>\n",
       "      <td>2.0229</td>\n",
       "      <td>0.7284</td>\n",
       "      <td>0.3307</td>\n",
       "      <td>63.6192</td>\n",
       "      <td>1.0488</td>\n",
       "      <td>2.2802</td>\n",
       "      <td>60.6548</td>\n",
       "      <td>0.8396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.725600</td>\n",
       "      <td>1.725870</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>0.2377</td>\n",
       "      <td>7.9239</td>\n",
       "      <td>0.9193</td>\n",
       "      <td>2.3367</td>\n",
       "      <td>0.4379</td>\n",
       "      <td>0.5894</td>\n",
       "      <td>70.4582</td>\n",
       "      <td>0.9153</td>\n",
       "      <td>3.4769</td>\n",
       "      <td>56.6442</td>\n",
       "      <td>0.9212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.730900</td>\n",
       "      <td>1.726235</td>\n",
       "      <td>0.0379</td>\n",
       "      <td>0.2537</td>\n",
       "      <td>7.3354</td>\n",
       "      <td>1.0083</td>\n",
       "      <td>2.606</td>\n",
       "      <td>0.5146</td>\n",
       "      <td>0.2875</td>\n",
       "      <td>76.0972</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>1.5919</td>\n",
       "      <td>64.5117</td>\n",
       "      <td>0.8734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.732100</td>\n",
       "      <td>1.724037</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>8.7538</td>\n",
       "      <td>0.9107</td>\n",
       "      <td>2.5876</td>\n",
       "      <td>0.3747</td>\n",
       "      <td>0.7695</td>\n",
       "      <td>60.6098</td>\n",
       "      <td>1.1752</td>\n",
       "      <td>4.9829</td>\n",
       "      <td>57.7342</td>\n",
       "      <td>0.9469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.733000</td>\n",
       "      <td>1.732533</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.1427</td>\n",
       "      <td>9.0771</td>\n",
       "      <td>1.1029</td>\n",
       "      <td>8.9723</td>\n",
       "      <td>1.4017</td>\n",
       "      <td>1.0885</td>\n",
       "      <td>61.6247</td>\n",
       "      <td>1.3159</td>\n",
       "      <td>3.2494</td>\n",
       "      <td>54.7669</td>\n",
       "      <td>0.9912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.734100</td>\n",
       "      <td>1.725441</td>\n",
       "      <td>0.0281</td>\n",
       "      <td>0.3688</td>\n",
       "      <td>6.6768</td>\n",
       "      <td>0.9069</td>\n",
       "      <td>1.9545</td>\n",
       "      <td>1.0498</td>\n",
       "      <td>0.4341</td>\n",
       "      <td>67.9318</td>\n",
       "      <td>1.2718</td>\n",
       "      <td>2.5138</td>\n",
       "      <td>61.7388</td>\n",
       "      <td>1.0316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.726200</td>\n",
       "      <td>1.724716</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>0.3726</td>\n",
       "      <td>6.8530</td>\n",
       "      <td>0.8896</td>\n",
       "      <td>1.8519</td>\n",
       "      <td>1.2015</td>\n",
       "      <td>0.3352</td>\n",
       "      <td>63.2970</td>\n",
       "      <td>1.0702</td>\n",
       "      <td>2.5133</td>\n",
       "      <td>59.318</td>\n",
       "      <td>0.9735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.727200</td>\n",
       "      <td>1.724451</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>0.3301</td>\n",
       "      <td>9.0002</td>\n",
       "      <td>1.3549</td>\n",
       "      <td>22.2256</td>\n",
       "      <td>2.0864</td>\n",
       "      <td>2.3548</td>\n",
       "      <td>62.6755</td>\n",
       "      <td>1.1473</td>\n",
       "      <td>4.7831</td>\n",
       "      <td>55.3235</td>\n",
       "      <td>0.9366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.727300</td>\n",
       "      <td>1.722830</td>\n",
       "      <td>0.0392</td>\n",
       "      <td>0.2725</td>\n",
       "      <td>9.281</td>\n",
       "      <td>0.9775</td>\n",
       "      <td>7.2427</td>\n",
       "      <td>1.1304</td>\n",
       "      <td>1.8557</td>\n",
       "      <td>61.7407</td>\n",
       "      <td>1.1531</td>\n",
       "      <td>4.8544</td>\n",
       "      <td>55.8059</td>\n",
       "      <td>0.9369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.729900</td>\n",
       "      <td>1.719542</td>\n",
       "      <td>0.0544</td>\n",
       "      <td>0.4764</td>\n",
       "      <td>7.6136</td>\n",
       "      <td>0.9987</td>\n",
       "      <td>12.5078</td>\n",
       "      <td>0.8578</td>\n",
       "      <td>0.8789</td>\n",
       "      <td>67.161</td>\n",
       "      <td>1.0744</td>\n",
       "      <td>3.7388</td>\n",
       "      <td>60.0524</td>\n",
       "      <td>0.9473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.728800</td>\n",
       "      <td>1.723256</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.3608</td>\n",
       "      <td>6.8794</td>\n",
       "      <td>0.9598</td>\n",
       "      <td>10.4291</td>\n",
       "      <td>2.8674</td>\n",
       "      <td>1.1127</td>\n",
       "      <td>64.7362</td>\n",
       "      <td>1.4299</td>\n",
       "      <td>2.7293</td>\n",
       "      <td>59.3352</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.724700</td>\n",
       "      <td>1.723282</td>\n",
       "      <td>0.0327</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>8.0480</td>\n",
       "      <td>1.017</td>\n",
       "      <td>3.3015</td>\n",
       "      <td>0.6213</td>\n",
       "      <td>1.115</td>\n",
       "      <td>68.2128</td>\n",
       "      <td>1.1966</td>\n",
       "      <td>3.8747</td>\n",
       "      <td>57.4677</td>\n",
       "      <td>0.9166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.721900</td>\n",
       "      <td>1.722219</td>\n",
       "      <td>0.0500</td>\n",
       "      <td>0.4231</td>\n",
       "      <td>8.7054</td>\n",
       "      <td>1.1423</td>\n",
       "      <td>9.3017</td>\n",
       "      <td>1.3179</td>\n",
       "      <td>1.5957</td>\n",
       "      <td>63.8110</td>\n",
       "      <td>1.0741</td>\n",
       "      <td>4.8628</td>\n",
       "      <td>56.8188</td>\n",
       "      <td>0.9722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.727800</td>\n",
       "      <td>1.723737</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>0.3512</td>\n",
       "      <td>7.7875</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>4.0009</td>\n",
       "      <td>4.6840</td>\n",
       "      <td>0.8796</td>\n",
       "      <td>69.1441</td>\n",
       "      <td>1.4159</td>\n",
       "      <td>3.4336</td>\n",
       "      <td>61.9769</td>\n",
       "      <td>1.0392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.720000</td>\n",
       "      <td>1.721456</td>\n",
       "      <td>0.0361</td>\n",
       "      <td>0.3057</td>\n",
       "      <td>7.2876</td>\n",
       "      <td>1.0034</td>\n",
       "      <td>4.4373</td>\n",
       "      <td>0.4595</td>\n",
       "      <td>0.9739</td>\n",
       "      <td>70.4176</td>\n",
       "      <td>1.2070</td>\n",
       "      <td>3.349</td>\n",
       "      <td>60.5019</td>\n",
       "      <td>1.0193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.724900</td>\n",
       "      <td>1.720383</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>0.3712</td>\n",
       "      <td>8.0071</td>\n",
       "      <td>1.0395</td>\n",
       "      <td>8.3559</td>\n",
       "      <td>3.8999</td>\n",
       "      <td>1.6957</td>\n",
       "      <td>67.6608</td>\n",
       "      <td>1.3484</td>\n",
       "      <td>3.5782</td>\n",
       "      <td>57.9671</td>\n",
       "      <td>0.9836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.718000</td>\n",
       "      <td>1.719584</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.4406</td>\n",
       "      <td>8.2749</td>\n",
       "      <td>1.3311</td>\n",
       "      <td>19.5206</td>\n",
       "      <td>1.5512</td>\n",
       "      <td>2.1385</td>\n",
       "      <td>65.8053</td>\n",
       "      <td>1.2306</td>\n",
       "      <td>4.0873</td>\n",
       "      <td>56.6647</td>\n",
       "      <td>0.9933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.718300</td>\n",
       "      <td>1.722130</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.4426</td>\n",
       "      <td>7.2207</td>\n",
       "      <td>1.0191</td>\n",
       "      <td>5.6169</td>\n",
       "      <td>2.0325</td>\n",
       "      <td>0.7236</td>\n",
       "      <td>70.1585</td>\n",
       "      <td>1.1188</td>\n",
       "      <td>3.2451</td>\n",
       "      <td>61.4774</td>\n",
       "      <td>1.0422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.728900</td>\n",
       "      <td>1.720901</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>0.4028</td>\n",
       "      <td>8.3825</td>\n",
       "      <td>1.2033</td>\n",
       "      <td>14.7314</td>\n",
       "      <td>2.2674</td>\n",
       "      <td>2.0879</td>\n",
       "      <td>63.7085</td>\n",
       "      <td>1.1373</td>\n",
       "      <td>4.4607</td>\n",
       "      <td>55.7761</td>\n",
       "      <td>0.9632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.718500</td>\n",
       "      <td>1.721849</td>\n",
       "      <td>0.0574</td>\n",
       "      <td>0.4188</td>\n",
       "      <td>9.0979</td>\n",
       "      <td>1.5305</td>\n",
       "      <td>26.4954</td>\n",
       "      <td>1.7085</td>\n",
       "      <td>2.4162</td>\n",
       "      <td>62.8215</td>\n",
       "      <td>1.1169</td>\n",
       "      <td>4.6337</td>\n",
       "      <td>55.5368</td>\n",
       "      <td>1.0438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.726800</td>\n",
       "      <td>1.720917</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.4670</td>\n",
       "      <td>7.5939</td>\n",
       "      <td>1.0023</td>\n",
       "      <td>8.3854</td>\n",
       "      <td>1.7232</td>\n",
       "      <td>0.8491</td>\n",
       "      <td>67.1374</td>\n",
       "      <td>1.0934</td>\n",
       "      <td>3.5904</td>\n",
       "      <td>61.0751</td>\n",
       "      <td>1.0102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.719500</td>\n",
       "      <td>1.720018</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.4964</td>\n",
       "      <td>9.2096</td>\n",
       "      <td>1.3056</td>\n",
       "      <td>21.9471</td>\n",
       "      <td>2.1012</td>\n",
       "      <td>2.009</td>\n",
       "      <td>67.8051</td>\n",
       "      <td>1.2191</td>\n",
       "      <td>4.6807</td>\n",
       "      <td>56.438</td>\n",
       "      <td>0.9835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.721500</td>\n",
       "      <td>1.718033</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>0.4780</td>\n",
       "      <td>7.8690</td>\n",
       "      <td>1.0904</td>\n",
       "      <td>11.5329</td>\n",
       "      <td>4.1298</td>\n",
       "      <td>1.4223</td>\n",
       "      <td>69.3915</td>\n",
       "      <td>1.2011</td>\n",
       "      <td>3.4040</td>\n",
       "      <td>59.7164</td>\n",
       "      <td>1.0436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.721000</td>\n",
       "      <td>1.719720</td>\n",
       "      <td>0.0507</td>\n",
       "      <td>0.4363</td>\n",
       "      <td>8.4614</td>\n",
       "      <td>1.0944</td>\n",
       "      <td>10.0324</td>\n",
       "      <td>3.3798</td>\n",
       "      <td>1.5708</td>\n",
       "      <td>66.5317</td>\n",
       "      <td>1.0465</td>\n",
       "      <td>4.2918</td>\n",
       "      <td>58.7901</td>\n",
       "      <td>0.9655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.720800</td>\n",
       "      <td>1.718529</td>\n",
       "      <td>0.0546</td>\n",
       "      <td>0.5594</td>\n",
       "      <td>8.2820</td>\n",
       "      <td>1.1734</td>\n",
       "      <td>13.6764</td>\n",
       "      <td>2.9556</td>\n",
       "      <td>1.5812</td>\n",
       "      <td>69.0583</td>\n",
       "      <td>1.2769</td>\n",
       "      <td>3.9757</td>\n",
       "      <td>59.2358</td>\n",
       "      <td>1.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.721700</td>\n",
       "      <td>1.719168</td>\n",
       "      <td>0.0609</td>\n",
       "      <td>0.4577</td>\n",
       "      <td>8.6691</td>\n",
       "      <td>1.161</td>\n",
       "      <td>13.4722</td>\n",
       "      <td>3.2700</td>\n",
       "      <td>1.5584</td>\n",
       "      <td>69.256</td>\n",
       "      <td>1.2756</td>\n",
       "      <td>4.2007</td>\n",
       "      <td>59.1868</td>\n",
       "      <td>1.0233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.719400</td>\n",
       "      <td>1.718940</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>0.4883</td>\n",
       "      <td>8.4045</td>\n",
       "      <td>1.2014</td>\n",
       "      <td>15.7525</td>\n",
       "      <td>3.3626</td>\n",
       "      <td>1.7233</td>\n",
       "      <td>69.329</td>\n",
       "      <td>1.2806</td>\n",
       "      <td>3.9242</td>\n",
       "      <td>58.5944</td>\n",
       "      <td>1.0344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.720000</td>\n",
       "      <td>1.718732</td>\n",
       "      <td>0.0525</td>\n",
       "      <td>0.4688</td>\n",
       "      <td>8.7024</td>\n",
       "      <td>1.2874</td>\n",
       "      <td>20.2559</td>\n",
       "      <td>2.5770</td>\n",
       "      <td>1.9758</td>\n",
       "      <td>68.9218</td>\n",
       "      <td>1.2278</td>\n",
       "      <td>4.0318</td>\n",
       "      <td>57.7961</td>\n",
       "      <td>1.0376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.724300</td>\n",
       "      <td>1.718744</td>\n",
       "      <td>0.0488</td>\n",
       "      <td>0.4481</td>\n",
       "      <td>8.7554</td>\n",
       "      <td>1.2449</td>\n",
       "      <td>18.3349</td>\n",
       "      <td>2.6380</td>\n",
       "      <td>1.9764</td>\n",
       "      <td>69.2519</td>\n",
       "      <td>1.2314</td>\n",
       "      <td>4.2548</td>\n",
       "      <td>57.9561</td>\n",
       "      <td>1.0222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>1.720300</td>\n",
       "      <td>1.718558</td>\n",
       "      <td>0.0509</td>\n",
       "      <td>0.4666</td>\n",
       "      <td>8.7443</td>\n",
       "      <td>1.2449</td>\n",
       "      <td>17.8909</td>\n",
       "      <td>2.7539</td>\n",
       "      <td>1.9848</td>\n",
       "      <td>68.8679</td>\n",
       "      <td>1.2288</td>\n",
       "      <td>4.3105</td>\n",
       "      <td>58.1243</td>\n",
       "      <td>1.0273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>1.726400</td>\n",
       "      <td>1.718563</td>\n",
       "      <td>0.0495</td>\n",
       "      <td>0.4521</td>\n",
       "      <td>8.7126</td>\n",
       "      <td>1.2310</td>\n",
       "      <td>17.1015</td>\n",
       "      <td>2.9289</td>\n",
       "      <td>1.9531</td>\n",
       "      <td>68.9232</td>\n",
       "      <td>1.2314</td>\n",
       "      <td>4.2270</td>\n",
       "      <td>58.1489</td>\n",
       "      <td>1.0308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>1.717100</td>\n",
       "      <td>1.718547</td>\n",
       "      <td>0.0495</td>\n",
       "      <td>0.4532</td>\n",
       "      <td>8.6962</td>\n",
       "      <td>1.2274</td>\n",
       "      <td>16.9145</td>\n",
       "      <td>2.9775</td>\n",
       "      <td>1.9412</td>\n",
       "      <td>68.9263</td>\n",
       "      <td>1.2339</td>\n",
       "      <td>4.2082</td>\n",
       "      <td>58.1991</td>\n",
       "      <td>1.0318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8804, training_loss=1.7283660683075117, metrics={'train_runtime': 2759.6725, 'train_samples_per_second': 12.76, 'train_steps_per_second': 3.19, 'total_flos': 0.0, 'train_loss': 1.7283660683075117, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 1e-3, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# .2 lq loss loss with conditioned kelly betting\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='227' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 227/8804 00:59 < 37:52, 3.77 it/s, Epoch 0.03/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-bb1c47517fd0>:15: RuntimeWarning: Mean of empty slice.\n",
      "  'full trade accuracy': (soft_profit[abs_trade >= .7] > 0).mean() * 100,\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:16: RuntimeWarning: Mean of empty slice.\n",
      "  'full trade g/l': soft_profit[(abs_trade >= .7) & (soft_profit > 0)].mean()\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:17: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade >= .7) & (soft_profit < 0)].mean(),\n",
      "<ipython-input-2-bb1c47517fd0>:20: RuntimeWarning: Mean of empty slice.\n",
      "  'medium trade accuracy': (soft_profit[(abs_trade < .7) & (abs_trade >= .4)] > 0).mean() * 100,\n",
      "<ipython-input-2-bb1c47517fd0>:21: RuntimeWarning: Mean of empty slice.\n",
      "  'medium trade g/l': soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit > 0)].mean()\n",
      "<ipython-input-2-bb1c47517fd0>:22: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit < 0)].mean(),\n",
      "<ipython-input-2-bb1c47517fd0>:25: RuntimeWarning: Mean of empty slice.\n",
      "  'small trade accuracy': (soft_profit[(abs_trade < .4) & (abs_trade >= .2)] > 0).mean() * 100,\n",
      "<ipython-input-2-bb1c47517fd0>:26: RuntimeWarning: Mean of empty slice.\n",
      "  'small trade g/l': soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit > 0)].mean()\n",
      "<ipython-input-2-bb1c47517fd0>:27: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit < 0)].mean(),\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-04e6affbc126>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# maximize geometric mean of offset profits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1773\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1774\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1775\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1776\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1777\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2522\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2523\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2525\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2553\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2554\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2555\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2556\u001b[0m         \u001b[1;31m# Save past state if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2557\u001b[0m         \u001b[1;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\trader_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, ohlcv, labels, future, std_future)\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0membed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_embed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mohlcv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\trader_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, mod)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgconv_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_gconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmod\u001b[0m \u001b[1;31m# residual\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\gconv_standalone.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, u, return_kernel)\u001b[0m\n\u001b[0;32m    439\u001b[0m             \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mL\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_norm\u001b[0m  \u001b[1;31m# * (L / self.l_max) ** 0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m         \u001b[1;31m# Convolution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 2e-4, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# maximize geometric mean (or log sum) of absolute max offset profits\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6918' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6918/8804 33:54 < 09:14, 3.40 it/s, Epoch 0.79/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>-0.002900</td>\n",
       "      <td>-0.009641</td>\n",
       "      <td>0.1082</td>\n",
       "      <td>0.3004</td>\n",
       "      <td>45.2461</td>\n",
       "      <td>21.3637</td>\n",
       "      <td>51.4349</td>\n",
       "      <td>0.9909</td>\n",
       "      <td>34.5379</td>\n",
       "      <td>49.9499</td>\n",
       "      <td>1.0245</td>\n",
       "      <td>22.6936</td>\n",
       "      <td>48.5993</td>\n",
       "      <td>1.0265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>-0.007100</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>-0.0033</td>\n",
       "      <td>-0.0033</td>\n",
       "      <td>67.5751</td>\n",
       "      <td>56.6431</td>\n",
       "      <td>50.2984</td>\n",
       "      <td>0.9464</td>\n",
       "      <td>21.6975</td>\n",
       "      <td>49.6184</td>\n",
       "      <td>1.0091</td>\n",
       "      <td>10.9894</td>\n",
       "      <td>48.8353</td>\n",
       "      <td>1.0015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>-0.007400</td>\n",
       "      <td>-0.006289</td>\n",
       "      <td>0.2258</td>\n",
       "      <td>0.2413</td>\n",
       "      <td>74.9352</td>\n",
       "      <td>68.0277</td>\n",
       "      <td>51.0784</td>\n",
       "      <td>0.9868</td>\n",
       "      <td>15.6031</td>\n",
       "      <td>49.6705</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>8.1277</td>\n",
       "      <td>49.8281</td>\n",
       "      <td>0.9940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>-0.006100</td>\n",
       "      <td>-0.006362</td>\n",
       "      <td>0.2072</td>\n",
       "      <td>0.1902</td>\n",
       "      <td>65.178</td>\n",
       "      <td>54.4574</td>\n",
       "      <td>51.2344</td>\n",
       "      <td>0.9881</td>\n",
       "      <td>19.5850</td>\n",
       "      <td>49.8738</td>\n",
       "      <td>1.0026</td>\n",
       "      <td>12.4446</td>\n",
       "      <td>49.4097</td>\n",
       "      <td>1.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>-0.009600</td>\n",
       "      <td>-0.013506</td>\n",
       "      <td>0.2812</td>\n",
       "      <td>0.2433</td>\n",
       "      <td>57.7408</td>\n",
       "      <td>43.3538</td>\n",
       "      <td>51.8721</td>\n",
       "      <td>0.9979</td>\n",
       "      <td>20.4477</td>\n",
       "      <td>50.6002</td>\n",
       "      <td>1.0255</td>\n",
       "      <td>16.53</td>\n",
       "      <td>49.9101</td>\n",
       "      <td>1.0092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>-0.009900</td>\n",
       "      <td>-0.002515</td>\n",
       "      <td>0.1248</td>\n",
       "      <td>0.1207</td>\n",
       "      <td>65.9809</td>\n",
       "      <td>55.5016</td>\n",
       "      <td>51.2105</td>\n",
       "      <td>0.9607</td>\n",
       "      <td>17.4079</td>\n",
       "      <td>50.1267</td>\n",
       "      <td>0.9573</td>\n",
       "      <td>12.3114</td>\n",
       "      <td>49.5244</td>\n",
       "      <td>0.9845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>-0.011200</td>\n",
       "      <td>-0.002951</td>\n",
       "      <td>0.1398</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>67.645</td>\n",
       "      <td>57.9914</td>\n",
       "      <td>51.2711</td>\n",
       "      <td>0.9645</td>\n",
       "      <td>15.4293</td>\n",
       "      <td>49.7145</td>\n",
       "      <td>0.9673</td>\n",
       "      <td>11.5859</td>\n",
       "      <td>49.3507</td>\n",
       "      <td>0.9842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>-0.007600</td>\n",
       "      <td>-0.000706</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.0624</td>\n",
       "      <td>60.5708</td>\n",
       "      <td>47.8157</td>\n",
       "      <td>50.4137</td>\n",
       "      <td>0.9274</td>\n",
       "      <td>18.2570</td>\n",
       "      <td>48.8675</td>\n",
       "      <td>0.9659</td>\n",
       "      <td>15.3177</td>\n",
       "      <td>49.0718</td>\n",
       "      <td>0.9718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>-0.013300</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>-0.0563</td>\n",
       "      <td>-0.0553</td>\n",
       "      <td>59.3051</td>\n",
       "      <td>46.1962</td>\n",
       "      <td>51.0554</td>\n",
       "      <td>0.9027</td>\n",
       "      <td>19.0757</td>\n",
       "      <td>49.777</td>\n",
       "      <td>0.9215</td>\n",
       "      <td>14.943</td>\n",
       "      <td>49.4533</td>\n",
       "      <td>0.9507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>-0.011600</td>\n",
       "      <td>-0.002716</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>62.3959</td>\n",
       "      <td>51.1839</td>\n",
       "      <td>51.0571</td>\n",
       "      <td>0.9312</td>\n",
       "      <td>16.3621</td>\n",
       "      <td>49.3497</td>\n",
       "      <td>0.9527</td>\n",
       "      <td>13.7669</td>\n",
       "      <td>48.8594</td>\n",
       "      <td>0.9609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>-0.013500</td>\n",
       "      <td>-0.002676</td>\n",
       "      <td>0.0405</td>\n",
       "      <td>0.0414</td>\n",
       "      <td>61.3296</td>\n",
       "      <td>49.9123</td>\n",
       "      <td>51.2354</td>\n",
       "      <td>0.9395</td>\n",
       "      <td>15.6362</td>\n",
       "      <td>49.5622</td>\n",
       "      <td>0.9502</td>\n",
       "      <td>14.1949</td>\n",
       "      <td>48.9977</td>\n",
       "      <td>0.9574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>-0.011400</td>\n",
       "      <td>-0.009849</td>\n",
       "      <td>0.1612</td>\n",
       "      <td>0.2291</td>\n",
       "      <td>52.8229</td>\n",
       "      <td>40.5116</td>\n",
       "      <td>50.5747</td>\n",
       "      <td>0.9942</td>\n",
       "      <td>13.7229</td>\n",
       "      <td>51.3122</td>\n",
       "      <td>1.0070</td>\n",
       "      <td>16.3541</td>\n",
       "      <td>50.3169</td>\n",
       "      <td>1.0076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>-0.013700</td>\n",
       "      <td>-0.003145</td>\n",
       "      <td>-0.0251</td>\n",
       "      <td>-0.0270</td>\n",
       "      <td>51.9637</td>\n",
       "      <td>38.1851</td>\n",
       "      <td>51.4055</td>\n",
       "      <td>0.9183</td>\n",
       "      <td>15.8564</td>\n",
       "      <td>49.2526</td>\n",
       "      <td>0.9274</td>\n",
       "      <td>18.3839</td>\n",
       "      <td>48.1711</td>\n",
       "      <td>0.9397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>-0.015300</td>\n",
       "      <td>-0.005160</td>\n",
       "      <td>0.1143</td>\n",
       "      <td>0.1646</td>\n",
       "      <td>53.4620</td>\n",
       "      <td>41.3444</td>\n",
       "      <td>51.5809</td>\n",
       "      <td>0.9235</td>\n",
       "      <td>14.6597</td>\n",
       "      <td>51.0513</td>\n",
       "      <td>0.9884</td>\n",
       "      <td>14.9478</td>\n",
       "      <td>50.8737</td>\n",
       "      <td>1.0166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>-0.015400</td>\n",
       "      <td>-0.005251</td>\n",
       "      <td>0.1055</td>\n",
       "      <td>0.1414</td>\n",
       "      <td>53.4139</td>\n",
       "      <td>42.1476</td>\n",
       "      <td>51.7889</td>\n",
       "      <td>0.9254</td>\n",
       "      <td>13.9357</td>\n",
       "      <td>50.5488</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>13.5385</td>\n",
       "      <td>50.2811</td>\n",
       "      <td>1.0106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>-0.016300</td>\n",
       "      <td>-0.006177</td>\n",
       "      <td>0.0708</td>\n",
       "      <td>0.0944</td>\n",
       "      <td>57.1256</td>\n",
       "      <td>45.8965</td>\n",
       "      <td>51.1266</td>\n",
       "      <td>0.9470</td>\n",
       "      <td>14.3261</td>\n",
       "      <td>50.2675</td>\n",
       "      <td>0.9755</td>\n",
       "      <td>13.9870</td>\n",
       "      <td>49.8468</td>\n",
       "      <td>0.963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>-0.017400</td>\n",
       "      <td>-0.001025</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.0372</td>\n",
       "      <td>54.1005</td>\n",
       "      <td>43.3456</td>\n",
       "      <td>51.3197</td>\n",
       "      <td>0.9152</td>\n",
       "      <td>12.3359</td>\n",
       "      <td>50.5334</td>\n",
       "      <td>0.9542</td>\n",
       "      <td>13.4212</td>\n",
       "      <td>49.7588</td>\n",
       "      <td>0.9541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>-0.017200</td>\n",
       "      <td>-0.003736</td>\n",
       "      <td>0.0525</td>\n",
       "      <td>0.0744</td>\n",
       "      <td>49.8598</td>\n",
       "      <td>38.0012</td>\n",
       "      <td>51.6863</td>\n",
       "      <td>0.9062</td>\n",
       "      <td>13.6653</td>\n",
       "      <td>50.8378</td>\n",
       "      <td>0.9511</td>\n",
       "      <td>14.4435</td>\n",
       "      <td>49.8942</td>\n",
       "      <td>0.9753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>-0.017100</td>\n",
       "      <td>-0.006188</td>\n",
       "      <td>0.1458</td>\n",
       "      <td>0.1598</td>\n",
       "      <td>52.8046</td>\n",
       "      <td>41.7847</td>\n",
       "      <td>52.0365</td>\n",
       "      <td>0.9217</td>\n",
       "      <td>13.2808</td>\n",
       "      <td>51.3614</td>\n",
       "      <td>0.9769</td>\n",
       "      <td>13.2539</td>\n",
       "      <td>50.8615</td>\n",
       "      <td>1.0154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>-0.013700</td>\n",
       "      <td>-0.003421</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>52.3031</td>\n",
       "      <td>40.8314</td>\n",
       "      <td>51.3595</td>\n",
       "      <td>0.9071</td>\n",
       "      <td>13.3164</td>\n",
       "      <td>50.6734</td>\n",
       "      <td>0.9619</td>\n",
       "      <td>14.0259</td>\n",
       "      <td>50.3048</td>\n",
       "      <td>0.9677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>-0.020100</td>\n",
       "      <td>-0.003120</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>0.1188</td>\n",
       "      <td>53.7658</td>\n",
       "      <td>43.0496</td>\n",
       "      <td>51.6101</td>\n",
       "      <td>0.9211</td>\n",
       "      <td>11.9331</td>\n",
       "      <td>51.4448</td>\n",
       "      <td>0.9654</td>\n",
       "      <td>13.6020</td>\n",
       "      <td>51.1970</td>\n",
       "      <td>1.0086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>-0.016600</td>\n",
       "      <td>-0.007453</td>\n",
       "      <td>0.0781</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>45.4615</td>\n",
       "      <td>33.5947</td>\n",
       "      <td>51.4512</td>\n",
       "      <td>0.9273</td>\n",
       "      <td>12.4543</td>\n",
       "      <td>50.3759</td>\n",
       "      <td>0.9777</td>\n",
       "      <td>14.5751</td>\n",
       "      <td>49.9753</td>\n",
       "      <td>0.9958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>-0.018100</td>\n",
       "      <td>-0.007483</td>\n",
       "      <td>0.0909</td>\n",
       "      <td>0.1436</td>\n",
       "      <td>45.5099</td>\n",
       "      <td>34.0044</td>\n",
       "      <td>51.3437</td>\n",
       "      <td>0.9368</td>\n",
       "      <td>11.8563</td>\n",
       "      <td>50.7946</td>\n",
       "      <td>0.9716</td>\n",
       "      <td>13.9714</td>\n",
       "      <td>50.4974</td>\n",
       "      <td>1.0014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>-0.016100</td>\n",
       "      <td>-0.007679</td>\n",
       "      <td>0.1106</td>\n",
       "      <td>0.1551</td>\n",
       "      <td>50.7087</td>\n",
       "      <td>39.6383</td>\n",
       "      <td>51.3896</td>\n",
       "      <td>0.9427</td>\n",
       "      <td>12.9559</td>\n",
       "      <td>50.2245</td>\n",
       "      <td>0.9839</td>\n",
       "      <td>13.247</td>\n",
       "      <td>50.5817</td>\n",
       "      <td>1.0228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>-0.016200</td>\n",
       "      <td>-0.005937</td>\n",
       "      <td>0.0804</td>\n",
       "      <td>0.1269</td>\n",
       "      <td>47.5414</td>\n",
       "      <td>37.0353</td>\n",
       "      <td>51.4526</td>\n",
       "      <td>0.937</td>\n",
       "      <td>11.7079</td>\n",
       "      <td>50.7512</td>\n",
       "      <td>0.9560</td>\n",
       "      <td>11.8598</td>\n",
       "      <td>50.0404</td>\n",
       "      <td>0.9766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>-0.016100</td>\n",
       "      <td>-0.005867</td>\n",
       "      <td>0.0724</td>\n",
       "      <td>0.0919</td>\n",
       "      <td>48.4969</td>\n",
       "      <td>38.3677</td>\n",
       "      <td>51.5618</td>\n",
       "      <td>0.9233</td>\n",
       "      <td>11.0209</td>\n",
       "      <td>50.8047</td>\n",
       "      <td>0.9591</td>\n",
       "      <td>11.6375</td>\n",
       "      <td>49.7187</td>\n",
       "      <td>0.9754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>-0.017800</td>\n",
       "      <td>-0.005941</td>\n",
       "      <td>0.1169</td>\n",
       "      <td>0.1488</td>\n",
       "      <td>51.8943</td>\n",
       "      <td>42.0277</td>\n",
       "      <td>51.6211</td>\n",
       "      <td>0.9324</td>\n",
       "      <td>11.6465</td>\n",
       "      <td>50.3263</td>\n",
       "      <td>0.9846</td>\n",
       "      <td>11.2884</td>\n",
       "      <td>50.6595</td>\n",
       "      <td>1.0153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>-0.017200</td>\n",
       "      <td>-0.007720</td>\n",
       "      <td>0.1290</td>\n",
       "      <td>0.1758</td>\n",
       "      <td>47.1334</td>\n",
       "      <td>36.8771</td>\n",
       "      <td>51.6632</td>\n",
       "      <td>0.9347</td>\n",
       "      <td>11.5583</td>\n",
       "      <td>50.9928</td>\n",
       "      <td>0.9774</td>\n",
       "      <td>11.5239</td>\n",
       "      <td>51.3939</td>\n",
       "      <td>1.0122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>-0.018300</td>\n",
       "      <td>-0.007011</td>\n",
       "      <td>0.0929</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>46.4306</td>\n",
       "      <td>35.9093</td>\n",
       "      <td>51.5955</td>\n",
       "      <td>0.9304</td>\n",
       "      <td>11.6903</td>\n",
       "      <td>51.0012</td>\n",
       "      <td>0.9559</td>\n",
       "      <td>11.7942</td>\n",
       "      <td>50.3656</td>\n",
       "      <td>0.9881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>-0.019100</td>\n",
       "      <td>-0.006882</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>0.1629</td>\n",
       "      <td>47.0864</td>\n",
       "      <td>36.7515</td>\n",
       "      <td>51.9855</td>\n",
       "      <td>0.9291</td>\n",
       "      <td>11.7034</td>\n",
       "      <td>50.7261</td>\n",
       "      <td>0.9595</td>\n",
       "      <td>11.5596</td>\n",
       "      <td>50.7113</td>\n",
       "      <td>0.9993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>-0.019900</td>\n",
       "      <td>-0.008388</td>\n",
       "      <td>0.1074</td>\n",
       "      <td>0.1572</td>\n",
       "      <td>47.4776</td>\n",
       "      <td>37.4224</td>\n",
       "      <td>51.6726</td>\n",
       "      <td>0.9417</td>\n",
       "      <td>11.5012</td>\n",
       "      <td>50.6605</td>\n",
       "      <td>0.9755</td>\n",
       "      <td>11.0669</td>\n",
       "      <td>49.9548</td>\n",
       "      <td>0.9955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>-0.020300</td>\n",
       "      <td>-0.007333</td>\n",
       "      <td>0.1182</td>\n",
       "      <td>0.1677</td>\n",
       "      <td>48.0125</td>\n",
       "      <td>38.3953</td>\n",
       "      <td>51.8559</td>\n",
       "      <td>0.935</td>\n",
       "      <td>11.1936</td>\n",
       "      <td>50.8407</td>\n",
       "      <td>0.9586</td>\n",
       "      <td>10.5235</td>\n",
       "      <td>50.9249</td>\n",
       "      <td>1.0067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>-0.018900</td>\n",
       "      <td>-0.007001</td>\n",
       "      <td>0.1049</td>\n",
       "      <td>0.1515</td>\n",
       "      <td>47.1624</td>\n",
       "      <td>37.4987</td>\n",
       "      <td>51.8869</td>\n",
       "      <td>0.9331</td>\n",
       "      <td>11.0998</td>\n",
       "      <td>50.5730</td>\n",
       "      <td>0.9668</td>\n",
       "      <td>10.5035</td>\n",
       "      <td>50.0988</td>\n",
       "      <td>0.981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>-0.020700</td>\n",
       "      <td>-0.006536</td>\n",
       "      <td>0.1115</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>47.7597</td>\n",
       "      <td>38.1202</td>\n",
       "      <td>52.0116</td>\n",
       "      <td>0.9271</td>\n",
       "      <td>11.174</td>\n",
       "      <td>50.5277</td>\n",
       "      <td>0.9583</td>\n",
       "      <td>10.5836</td>\n",
       "      <td>50.6892</td>\n",
       "      <td>0.9877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0830fa9147a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# lo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1775\u001b[0m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1776\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1777\u001b[1;33m                 if (\n\u001b[0m\u001b[0;32m   1778\u001b[0m                     \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1779\u001b[0m                     \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 2e-4, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# log + 1 minimax\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8804' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8804/8804 42:27, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>-0.048900</td>\n",
       "      <td>-0.076420</td>\n",
       "      <td>0.0698</td>\n",
       "      <td>0.0783</td>\n",
       "      <td>21.5128</td>\n",
       "      <td>1.7383</td>\n",
       "      <td>46.206</td>\n",
       "      <td>0.8778</td>\n",
       "      <td>14.0813</td>\n",
       "      <td>50.1736</td>\n",
       "      <td>1.0546</td>\n",
       "      <td>27.1476</td>\n",
       "      <td>50.4106</td>\n",
       "      <td>1.0528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>-0.066900</td>\n",
       "      <td>-0.069012</td>\n",
       "      <td>-0.0039</td>\n",
       "      <td>-0.0037</td>\n",
       "      <td>28.9478</td>\n",
       "      <td>5.9561</td>\n",
       "      <td>47.3188</td>\n",
       "      <td>0.8578</td>\n",
       "      <td>26.2494</td>\n",
       "      <td>49.6786</td>\n",
       "      <td>0.9957</td>\n",
       "      <td>21.6242</td>\n",
       "      <td>50.3402</td>\n",
       "      <td>1.0132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>-0.069400</td>\n",
       "      <td>-0.078662</td>\n",
       "      <td>0.1598</td>\n",
       "      <td>0.2249</td>\n",
       "      <td>23.4153</td>\n",
       "      <td>2.3324</td>\n",
       "      <td>56.2475</td>\n",
       "      <td>1.2961</td>\n",
       "      <td>17.6274</td>\n",
       "      <td>50.6199</td>\n",
       "      <td>1.0829</td>\n",
       "      <td>26.9159</td>\n",
       "      <td>49.9396</td>\n",
       "      <td>0.9827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>-0.086100</td>\n",
       "      <td>-0.087207</td>\n",
       "      <td>0.2103</td>\n",
       "      <td>0.3177</td>\n",
       "      <td>23.5167</td>\n",
       "      <td>3.9965</td>\n",
       "      <td>55.7086</td>\n",
       "      <td>1.4685</td>\n",
       "      <td>16.2641</td>\n",
       "      <td>52.618</td>\n",
       "      <td>1.1229</td>\n",
       "      <td>24.4605</td>\n",
       "      <td>48.5259</td>\n",
       "      <td>0.9138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>-0.085700</td>\n",
       "      <td>-0.108328</td>\n",
       "      <td>0.1567</td>\n",
       "      <td>0.2445</td>\n",
       "      <td>16.2038</td>\n",
       "      <td>1.4209</td>\n",
       "      <td>61.4442</td>\n",
       "      <td>1.5789</td>\n",
       "      <td>7.6073</td>\n",
       "      <td>49.4332</td>\n",
       "      <td>1.0425</td>\n",
       "      <td>19.4749</td>\n",
       "      <td>51.0955</td>\n",
       "      <td>1.1009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>-0.095600</td>\n",
       "      <td>-0.108088</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.3747</td>\n",
       "      <td>19.6842</td>\n",
       "      <td>6.1166</td>\n",
       "      <td>56.2230</td>\n",
       "      <td>1.4703</td>\n",
       "      <td>9.6443</td>\n",
       "      <td>51.8461</td>\n",
       "      <td>1.0848</td>\n",
       "      <td>15.6687</td>\n",
       "      <td>49.4274</td>\n",
       "      <td>1.0622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>-0.095700</td>\n",
       "      <td>-0.112252</td>\n",
       "      <td>0.1383</td>\n",
       "      <td>0.2302</td>\n",
       "      <td>16.3836</td>\n",
       "      <td>2.5651</td>\n",
       "      <td>58.1138</td>\n",
       "      <td>1.6966</td>\n",
       "      <td>8.6573</td>\n",
       "      <td>47.6423</td>\n",
       "      <td>0.9676</td>\n",
       "      <td>17.2535</td>\n",
       "      <td>49.8302</td>\n",
       "      <td>1.0978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>-0.098500</td>\n",
       "      <td>-0.110279</td>\n",
       "      <td>0.1600</td>\n",
       "      <td>0.3912</td>\n",
       "      <td>12.9341</td>\n",
       "      <td>2.3853</td>\n",
       "      <td>56.5090</td>\n",
       "      <td>1.5069</td>\n",
       "      <td>6.1132</td>\n",
       "      <td>54.1362</td>\n",
       "      <td>1.4374</td>\n",
       "      <td>10.7970</td>\n",
       "      <td>48.9985</td>\n",
       "      <td>1.0690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>-0.097600</td>\n",
       "      <td>-0.106073</td>\n",
       "      <td>0.1484</td>\n",
       "      <td>0.4322</td>\n",
       "      <td>10.4854</td>\n",
       "      <td>2.0071</td>\n",
       "      <td>55.6185</td>\n",
       "      <td>1.6219</td>\n",
       "      <td>3.8239</td>\n",
       "      <td>56.2485</td>\n",
       "      <td>1.4738</td>\n",
       "      <td>7.9972</td>\n",
       "      <td>52.0236</td>\n",
       "      <td>1.0425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>-0.096300</td>\n",
       "      <td>-0.104373</td>\n",
       "      <td>0.1179</td>\n",
       "      <td>0.3204</td>\n",
       "      <td>11.6298</td>\n",
       "      <td>1.9857</td>\n",
       "      <td>52.6413</td>\n",
       "      <td>1.8844</td>\n",
       "      <td>4.1293</td>\n",
       "      <td>50.5227</td>\n",
       "      <td>1.0849</td>\n",
       "      <td>10.4766</td>\n",
       "      <td>50.1309</td>\n",
       "      <td>1.1359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>-0.101300</td>\n",
       "      <td>-0.119245</td>\n",
       "      <td>0.1005</td>\n",
       "      <td>0.2941</td>\n",
       "      <td>8.3135</td>\n",
       "      <td>0.8925</td>\n",
       "      <td>63.0447</td>\n",
       "      <td>1.7507</td>\n",
       "      <td>2.5202</td>\n",
       "      <td>53.9760</td>\n",
       "      <td>1.1921</td>\n",
       "      <td>7.3197</td>\n",
       "      <td>49.7913</td>\n",
       "      <td>1.0252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>-0.105000</td>\n",
       "      <td>-0.117817</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>0.3235</td>\n",
       "      <td>11.1324</td>\n",
       "      <td>2.2219</td>\n",
       "      <td>54.1696</td>\n",
       "      <td>1.7264</td>\n",
       "      <td>4.6728</td>\n",
       "      <td>52.2962</td>\n",
       "      <td>1.0999</td>\n",
       "      <td>9.4668</td>\n",
       "      <td>49.4691</td>\n",
       "      <td>1.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>-0.102700</td>\n",
       "      <td>-0.117398</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.0448</td>\n",
       "      <td>11.9571</td>\n",
       "      <td>2.6726</td>\n",
       "      <td>51.6094</td>\n",
       "      <td>1.0604</td>\n",
       "      <td>7.1484</td>\n",
       "      <td>46.7589</td>\n",
       "      <td>0.9295</td>\n",
       "      <td>8.7032</td>\n",
       "      <td>48.4599</td>\n",
       "      <td>1.0217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>-0.113500</td>\n",
       "      <td>-0.122327</td>\n",
       "      <td>0.0787</td>\n",
       "      <td>0.1514</td>\n",
       "      <td>12.8387</td>\n",
       "      <td>3.9837</td>\n",
       "      <td>51.8597</td>\n",
       "      <td>1.2048</td>\n",
       "      <td>6.4575</td>\n",
       "      <td>47.2711</td>\n",
       "      <td>0.9270</td>\n",
       "      <td>7.5539</td>\n",
       "      <td>50.0697</td>\n",
       "      <td>1.0852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>-0.114300</td>\n",
       "      <td>-0.126516</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>0.0614</td>\n",
       "      <td>12.9379</td>\n",
       "      <td>5.1263</td>\n",
       "      <td>49.4539</td>\n",
       "      <td>0.9546</td>\n",
       "      <td>6.2188</td>\n",
       "      <td>46.6894</td>\n",
       "      <td>0.9784</td>\n",
       "      <td>6.3565</td>\n",
       "      <td>49.8942</td>\n",
       "      <td>1.1156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>-0.111400</td>\n",
       "      <td>-0.126377</td>\n",
       "      <td>0.0823</td>\n",
       "      <td>0.2448</td>\n",
       "      <td>9.6923</td>\n",
       "      <td>3.1324</td>\n",
       "      <td>54.3557</td>\n",
       "      <td>1.5576</td>\n",
       "      <td>4.1476</td>\n",
       "      <td>46.3735</td>\n",
       "      <td>0.8964</td>\n",
       "      <td>5.0931</td>\n",
       "      <td>51.6368</td>\n",
       "      <td>0.9487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>-0.116800</td>\n",
       "      <td>-0.115540</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>0.0815</td>\n",
       "      <td>10.4429</td>\n",
       "      <td>4.5122</td>\n",
       "      <td>50.9521</td>\n",
       "      <td>1.0920</td>\n",
       "      <td>4.2744</td>\n",
       "      <td>44.9321</td>\n",
       "      <td>0.8067</td>\n",
       "      <td>4.5489</td>\n",
       "      <td>49.7364</td>\n",
       "      <td>1.0874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>-0.117800</td>\n",
       "      <td>-0.120477</td>\n",
       "      <td>0.0380</td>\n",
       "      <td>0.0934</td>\n",
       "      <td>10.5196</td>\n",
       "      <td>4.5744</td>\n",
       "      <td>50.4491</td>\n",
       "      <td>1.1496</td>\n",
       "      <td>4.2064</td>\n",
       "      <td>44.3246</td>\n",
       "      <td>0.7591</td>\n",
       "      <td>4.9840</td>\n",
       "      <td>50.3274</td>\n",
       "      <td>1.1136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>-0.129700</td>\n",
       "      <td>-0.127075</td>\n",
       "      <td>0.1279</td>\n",
       "      <td>0.398</td>\n",
       "      <td>9.9667</td>\n",
       "      <td>3.6806</td>\n",
       "      <td>54.9238</td>\n",
       "      <td>1.384</td>\n",
       "      <td>4.0179</td>\n",
       "      <td>50.9745</td>\n",
       "      <td>1.0525</td>\n",
       "      <td>5.2152</td>\n",
       "      <td>51.6535</td>\n",
       "      <td>1.1657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>-0.121300</td>\n",
       "      <td>-0.127270</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>0.0714</td>\n",
       "      <td>10.4203</td>\n",
       "      <td>5.0871</td>\n",
       "      <td>50.7697</td>\n",
       "      <td>1.0643</td>\n",
       "      <td>3.9231</td>\n",
       "      <td>45.5746</td>\n",
       "      <td>0.7751</td>\n",
       "      <td>4.6739</td>\n",
       "      <td>50.1015</td>\n",
       "      <td>1.1029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>-0.126700</td>\n",
       "      <td>-0.125129</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>0.1643</td>\n",
       "      <td>8.387</td>\n",
       "      <td>3.0129</td>\n",
       "      <td>53.6287</td>\n",
       "      <td>1.3082</td>\n",
       "      <td>3.8315</td>\n",
       "      <td>46.1492</td>\n",
       "      <td>0.772</td>\n",
       "      <td>4.2916</td>\n",
       "      <td>51.2464</td>\n",
       "      <td>0.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>-0.124300</td>\n",
       "      <td>-0.138620</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.4941</td>\n",
       "      <td>7.2756</td>\n",
       "      <td>1.9288</td>\n",
       "      <td>59.3145</td>\n",
       "      <td>1.6034</td>\n",
       "      <td>3.2115</td>\n",
       "      <td>55.4481</td>\n",
       "      <td>1.1907</td>\n",
       "      <td>4.5124</td>\n",
       "      <td>53.5308</td>\n",
       "      <td>1.1487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>-0.130500</td>\n",
       "      <td>-0.133097</td>\n",
       "      <td>0.0785</td>\n",
       "      <td>0.3264</td>\n",
       "      <td>7.8196</td>\n",
       "      <td>2.7757</td>\n",
       "      <td>56.4538</td>\n",
       "      <td>1.5688</td>\n",
       "      <td>3.1965</td>\n",
       "      <td>47.1718</td>\n",
       "      <td>0.9155</td>\n",
       "      <td>3.7199</td>\n",
       "      <td>48.9082</td>\n",
       "      <td>0.8448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>-0.129700</td>\n",
       "      <td>-0.127183</td>\n",
       "      <td>0.0771</td>\n",
       "      <td>0.2914</td>\n",
       "      <td>7.0948</td>\n",
       "      <td>2.2198</td>\n",
       "      <td>56.8369</td>\n",
       "      <td>1.7921</td>\n",
       "      <td>2.7511</td>\n",
       "      <td>48.1087</td>\n",
       "      <td>0.9738</td>\n",
       "      <td>3.7726</td>\n",
       "      <td>47.4337</td>\n",
       "      <td>0.8533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>-0.128300</td>\n",
       "      <td>-0.119761</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0133</td>\n",
       "      <td>8.7526</td>\n",
       "      <td>3.9799</td>\n",
       "      <td>49.9227</td>\n",
       "      <td>0.9824</td>\n",
       "      <td>3.7038</td>\n",
       "      <td>45.0945</td>\n",
       "      <td>0.7258</td>\n",
       "      <td>3.7207</td>\n",
       "      <td>50.1107</td>\n",
       "      <td>1.0555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>-0.128800</td>\n",
       "      <td>-0.131804</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.0192</td>\n",
       "      <td>8.031</td>\n",
       "      <td>3.4820</td>\n",
       "      <td>49.9521</td>\n",
       "      <td>1.0069</td>\n",
       "      <td>3.4776</td>\n",
       "      <td>46.2731</td>\n",
       "      <td>0.7587</td>\n",
       "      <td>3.3842</td>\n",
       "      <td>49.3776</td>\n",
       "      <td>0.9184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>-0.125600</td>\n",
       "      <td>-0.132624</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.1369</td>\n",
       "      <td>7.8673</td>\n",
       "      <td>3.2517</td>\n",
       "      <td>53.0928</td>\n",
       "      <td>1.2502</td>\n",
       "      <td>3.4927</td>\n",
       "      <td>46.5578</td>\n",
       "      <td>0.7618</td>\n",
       "      <td>3.1966</td>\n",
       "      <td>46.8115</td>\n",
       "      <td>0.8457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>-0.133500</td>\n",
       "      <td>-0.137178</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>0.1042</td>\n",
       "      <td>7.259</td>\n",
       "      <td>3.1478</td>\n",
       "      <td>52.1316</td>\n",
       "      <td>1.1837</td>\n",
       "      <td>3.2256</td>\n",
       "      <td>47.9909</td>\n",
       "      <td>0.8003</td>\n",
       "      <td>2.7179</td>\n",
       "      <td>45.7838</td>\n",
       "      <td>0.7655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>-0.124200</td>\n",
       "      <td>-0.140503</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.1432</td>\n",
       "      <td>6.9495</td>\n",
       "      <td>2.9042</td>\n",
       "      <td>52.998</td>\n",
       "      <td>1.3043</td>\n",
       "      <td>3.1247</td>\n",
       "      <td>47.9094</td>\n",
       "      <td>0.7783</td>\n",
       "      <td>2.8204</td>\n",
       "      <td>46.7966</td>\n",
       "      <td>0.7797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>-0.135700</td>\n",
       "      <td>-0.142482</td>\n",
       "      <td>0.0509</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>7.5018</td>\n",
       "      <td>3.4104</td>\n",
       "      <td>53.3953</td>\n",
       "      <td>1.3339</td>\n",
       "      <td>3.0721</td>\n",
       "      <td>46.2954</td>\n",
       "      <td>0.7505</td>\n",
       "      <td>2.9497</td>\n",
       "      <td>46.9476</td>\n",
       "      <td>0.8512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>-0.133700</td>\n",
       "      <td>-0.142190</td>\n",
       "      <td>0.0413</td>\n",
       "      <td>0.1451</td>\n",
       "      <td>6.9317</td>\n",
       "      <td>3.366</td>\n",
       "      <td>52.8825</td>\n",
       "      <td>1.3249</td>\n",
       "      <td>2.8456</td>\n",
       "      <td>46.9366</td>\n",
       "      <td>0.7239</td>\n",
       "      <td>2.5046</td>\n",
       "      <td>44.4463</td>\n",
       "      <td>0.8317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>-0.137900</td>\n",
       "      <td>-0.140712</td>\n",
       "      <td>0.0452</td>\n",
       "      <td>0.1588</td>\n",
       "      <td>6.9125</td>\n",
       "      <td>3.1271</td>\n",
       "      <td>53.4111</td>\n",
       "      <td>1.3614</td>\n",
       "      <td>2.9123</td>\n",
       "      <td>46.4805</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>2.7408</td>\n",
       "      <td>47.7116</td>\n",
       "      <td>0.8051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>-0.139300</td>\n",
       "      <td>-0.141125</td>\n",
       "      <td>0.0552</td>\n",
       "      <td>0.1909</td>\n",
       "      <td>7.2751</td>\n",
       "      <td>3.4676</td>\n",
       "      <td>53.9452</td>\n",
       "      <td>1.4371</td>\n",
       "      <td>2.9587</td>\n",
       "      <td>46.5586</td>\n",
       "      <td>0.7661</td>\n",
       "      <td>2.9136</td>\n",
       "      <td>46.4052</td>\n",
       "      <td>0.8507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>-0.145200</td>\n",
       "      <td>-0.146556</td>\n",
       "      <td>0.0538</td>\n",
       "      <td>0.195</td>\n",
       "      <td>6.6889</td>\n",
       "      <td>3.0032</td>\n",
       "      <td>54.6351</td>\n",
       "      <td>1.4974</td>\n",
       "      <td>2.8843</td>\n",
       "      <td>46.5729</td>\n",
       "      <td>0.7757</td>\n",
       "      <td>2.7437</td>\n",
       "      <td>46.0571</td>\n",
       "      <td>0.8319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>-0.137100</td>\n",
       "      <td>-0.145292</td>\n",
       "      <td>0.0536</td>\n",
       "      <td>0.2332</td>\n",
       "      <td>5.5796</td>\n",
       "      <td>2.3512</td>\n",
       "      <td>55.9537</td>\n",
       "      <td>1.6126</td>\n",
       "      <td>2.5191</td>\n",
       "      <td>47.4647</td>\n",
       "      <td>0.9201</td>\n",
       "      <td>2.3390</td>\n",
       "      <td>45.5153</td>\n",
       "      <td>0.741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>-0.137600</td>\n",
       "      <td>-0.147197</td>\n",
       "      <td>0.0342</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>5.9999</td>\n",
       "      <td>2.5343</td>\n",
       "      <td>53.1372</td>\n",
       "      <td>1.3887</td>\n",
       "      <td>2.8227</td>\n",
       "      <td>48.2361</td>\n",
       "      <td>0.8250</td>\n",
       "      <td>2.5387</td>\n",
       "      <td>45.9071</td>\n",
       "      <td>0.7187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>-0.142000</td>\n",
       "      <td>-0.145608</td>\n",
       "      <td>0.0331</td>\n",
       "      <td>0.1321</td>\n",
       "      <td>5.8271</td>\n",
       "      <td>2.5184</td>\n",
       "      <td>53.4076</td>\n",
       "      <td>1.3944</td>\n",
       "      <td>2.7765</td>\n",
       "      <td>48.2913</td>\n",
       "      <td>0.8229</td>\n",
       "      <td>2.4492</td>\n",
       "      <td>45.8172</td>\n",
       "      <td>0.7052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>-0.140600</td>\n",
       "      <td>-0.148408</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>0.1694</td>\n",
       "      <td>5.695</td>\n",
       "      <td>2.4202</td>\n",
       "      <td>54.2846</td>\n",
       "      <td>1.4697</td>\n",
       "      <td>2.7342</td>\n",
       "      <td>48.3907</td>\n",
       "      <td>0.8596</td>\n",
       "      <td>2.3877</td>\n",
       "      <td>45.3339</td>\n",
       "      <td>0.7409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>-0.140100</td>\n",
       "      <td>-0.151271</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>0.1451</td>\n",
       "      <td>5.6411</td>\n",
       "      <td>2.3947</td>\n",
       "      <td>53.6358</td>\n",
       "      <td>1.4007</td>\n",
       "      <td>2.8135</td>\n",
       "      <td>48.5936</td>\n",
       "      <td>0.8773</td>\n",
       "      <td>2.3578</td>\n",
       "      <td>45.3211</td>\n",
       "      <td>0.7245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>-0.142300</td>\n",
       "      <td>-0.150449</td>\n",
       "      <td>0.0373</td>\n",
       "      <td>0.1571</td>\n",
       "      <td>5.5109</td>\n",
       "      <td>2.3137</td>\n",
       "      <td>54.0223</td>\n",
       "      <td>1.4192</td>\n",
       "      <td>2.7733</td>\n",
       "      <td>48.6371</td>\n",
       "      <td>0.889</td>\n",
       "      <td>2.3276</td>\n",
       "      <td>45.4778</td>\n",
       "      <td>0.7306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>-0.147600</td>\n",
       "      <td>-0.150704</td>\n",
       "      <td>0.0362</td>\n",
       "      <td>0.1521</td>\n",
       "      <td>5.5068</td>\n",
       "      <td>2.3199</td>\n",
       "      <td>53.962</td>\n",
       "      <td>1.4238</td>\n",
       "      <td>2.7689</td>\n",
       "      <td>48.5973</td>\n",
       "      <td>0.8801</td>\n",
       "      <td>2.3399</td>\n",
       "      <td>45.6530</td>\n",
       "      <td>0.7205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>-0.137700</td>\n",
       "      <td>-0.149356</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.1576</td>\n",
       "      <td>5.4331</td>\n",
       "      <td>2.2725</td>\n",
       "      <td>54.1800</td>\n",
       "      <td>1.4363</td>\n",
       "      <td>2.7442</td>\n",
       "      <td>48.6701</td>\n",
       "      <td>0.8928</td>\n",
       "      <td>2.3124</td>\n",
       "      <td>45.6734</td>\n",
       "      <td>0.7200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>-0.144000</td>\n",
       "      <td>-0.149648</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>0.1575</td>\n",
       "      <td>5.4255</td>\n",
       "      <td>2.2699</td>\n",
       "      <td>54.155</td>\n",
       "      <td>1.4379</td>\n",
       "      <td>2.7368</td>\n",
       "      <td>48.7065</td>\n",
       "      <td>0.8939</td>\n",
       "      <td>2.3105</td>\n",
       "      <td>45.6744</td>\n",
       "      <td>0.7179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>-0.147700</td>\n",
       "      <td>-0.149757</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.1579</td>\n",
       "      <td>5.4213</td>\n",
       "      <td>2.2679</td>\n",
       "      <td>54.1609</td>\n",
       "      <td>1.4375</td>\n",
       "      <td>2.7351</td>\n",
       "      <td>48.7095</td>\n",
       "      <td>0.8947</td>\n",
       "      <td>2.3095</td>\n",
       "      <td>45.6544</td>\n",
       "      <td>0.7188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8804, training_loss=-0.1189865780412907, metrics={'train_runtime': 2551.065, 'train_samples_per_second': 13.803, 'train_steps_per_second': 3.451, 'total_flos': 0.0, 'train_loss': -0.1189865780412907, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 2e-4, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# one sided sharpe maximization\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='820' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 820/8804 04:03 < 39:38, 3.36 it/s, Epoch 0.09/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>-0.000500</td>\n",
       "      <td>0.000972</td>\n",
       "      <td>-0.0502</td>\n",
       "      <td>-0.0637</td>\n",
       "      <td>42.9118</td>\n",
       "      <td>21.5487</td>\n",
       "      <td>48.8036</td>\n",
       "      <td>0.9863</td>\n",
       "      <td>28.1917</td>\n",
       "      <td>48.7865</td>\n",
       "      <td>1.0167</td>\n",
       "      <td>22.3319</td>\n",
       "      <td>48.8589</td>\n",
       "      <td>1.0020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>-0.086100</td>\n",
       "      <td>0.010977</td>\n",
       "      <td>-0.3247</td>\n",
       "      <td>-0.1710</td>\n",
       "      <td>97.1243</td>\n",
       "      <td>96.5743</td>\n",
       "      <td>48.3084</td>\n",
       "      <td>0.9613</td>\n",
       "      <td>1.4662</td>\n",
       "      <td>48.0834</td>\n",
       "      <td>0.986</td>\n",
       "      <td>0.7082</td>\n",
       "      <td>48.9547</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>-0.067300</td>\n",
       "      <td>-0.074641</td>\n",
       "      <td>-0.0557</td>\n",
       "      <td>-0.0378</td>\n",
       "      <td>97.4249</td>\n",
       "      <td>96.952</td>\n",
       "      <td>48.9278</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>1.2685</td>\n",
       "      <td>49.0588</td>\n",
       "      <td>0.9915</td>\n",
       "      <td>0.6386</td>\n",
       "      <td>48.8733</td>\n",
       "      <td>0.9982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>-0.096800</td>\n",
       "      <td>-0.100016</td>\n",
       "      <td>0.0488</td>\n",
       "      <td>0.0259</td>\n",
       "      <td>98.2115</td>\n",
       "      <td>97.9437</td>\n",
       "      <td>49.3568</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>0.7586</td>\n",
       "      <td>49.6632</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>0.3732</td>\n",
       "      <td>48.7846</td>\n",
       "      <td>1.0294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0a2f87228fc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# standardized profit cubed maximization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1848\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0msteps_in_epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1850\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer_callback.py\u001b[0m in \u001b[0;36mon_step_end\u001b[1;34m(self, args, state, control)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_step_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"on_step_end\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer_callback.py\u001b[0m in \u001b[0;36mcall_event\u001b[1;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m             result = getattr(callback, event)(\n\u001b[0m\u001b[0;32m    398\u001b[0m                 \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\notebook.py\u001b[0m in \u001b[0;36mon_step_end\u001b[1;34m(self, args, state, control, **kwargs)\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mon_step_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34mf\"{state.epoch:.2f}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m         self.training_tracker.update(\n\u001b[0m\u001b[0;32m    290\u001b[0m             \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[0mcomment\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mf\"Epoch {epoch}/{state.num_train_epochs}\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\notebook.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, value, force_update, comment)\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage_time_per_item\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredicted_remaining\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage_time_per_item\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_bar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\notebook.py\u001b[0m in \u001b[0;36mupdate_bar\u001b[1;34m(self, value, comment)\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34mf\", {1/self.average_time_per_item:.2f} it/s\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"]\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomment\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34mf\", {self.comment}]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\notebook.py\u001b[0m in \u001b[0;36mdisplay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdisp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhtml_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisplay_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHTML\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhtml_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrite_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, obj, **kwargs)\u001b[0m\n\u001b[0;32m    391\u001b[0m             \u001b[0madditional\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mupdate_display\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m         \"\"\"\n\u001b[1;32m--> 393\u001b[1;33m         \u001b[0mupdate_display\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisplay_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36mupdate_display\u001b[1;34m(obj, display_id, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m     \"\"\"\n\u001b[0;32m    342\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'update'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m     \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisplay_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisplay_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36mdisplay\u001b[1;34m(include, exclude, metadata, transient, display_id, *objs, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m                 \u001b[1;31m# kwarg-specified metadata gets precedence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m                 \u001b[0m_merge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmd_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m             \u001b[0mpublish_display_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmd_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdisplay_id\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mDisplayHandle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdisplay_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\display.py\u001b[0m in \u001b[0;36mpublish_display_data\u001b[1;34m(data, metadata, source, transient, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'transient'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m     display_pub.publish(\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mmetadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\u001b[0m in \u001b[0;36mpublish\u001b[1;34m(self, data, metadata, source, transient, update)\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[0mIf\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msend\u001b[0m \u001b[0man\u001b[0m \u001b[0mupdate_display_data\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdisplay_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \"\"\"\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flush_streams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\u001b[0m in \u001b[0;36m_flush_streams\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_flush_streams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;34m\"\"\"flush IO Streams prior to display\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mimport_lock_held\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m                 \u001b[0mevt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m                 \u001b[1;31m# and give a timeout to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    203\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;31m# wake event thread (message content is ignored)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    618\u001b[0m                 )\n\u001b[0;32m    619\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 620\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m     def send_multipart(\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 2e-4, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# standardized profit cubed maximization\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8804' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8804/8804 48:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>-0.004400</td>\n",
       "      <td>-0.008457</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.2591</td>\n",
       "      <td>63.2856</td>\n",
       "      <td>51.9286</td>\n",
       "      <td>51.1299</td>\n",
       "      <td>1.0025</td>\n",
       "      <td>22.2164</td>\n",
       "      <td>49.8151</td>\n",
       "      <td>0.9933</td>\n",
       "      <td>12.966</td>\n",
       "      <td>47.9433</td>\n",
       "      <td>0.9992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>-0.016900</td>\n",
       "      <td>-0.012493</td>\n",
       "      <td>0.1035</td>\n",
       "      <td>0.0873</td>\n",
       "      <td>84.2256</td>\n",
       "      <td>80.4435</td>\n",
       "      <td>50.9021</td>\n",
       "      <td>0.9719</td>\n",
       "      <td>8.5415</td>\n",
       "      <td>49.4006</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>5.068</td>\n",
       "      <td>47.0990</td>\n",
       "      <td>0.9617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>-0.027300</td>\n",
       "      <td>-0.016957</td>\n",
       "      <td>0.1793</td>\n",
       "      <td>0.1669</td>\n",
       "      <td>89.3524</td>\n",
       "      <td>86.6679</td>\n",
       "      <td>51.1046</td>\n",
       "      <td>0.9773</td>\n",
       "      <td>5.8591</td>\n",
       "      <td>47.8404</td>\n",
       "      <td>0.9796</td>\n",
       "      <td>3.3765</td>\n",
       "      <td>45.4821</td>\n",
       "      <td>0.9885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>-0.025400</td>\n",
       "      <td>-0.022955</td>\n",
       "      <td>0.2256</td>\n",
       "      <td>0.1577</td>\n",
       "      <td>90.7531</td>\n",
       "      <td>88.6046</td>\n",
       "      <td>51.4073</td>\n",
       "      <td>0.9772</td>\n",
       "      <td>4.6918</td>\n",
       "      <td>47.9376</td>\n",
       "      <td>0.9835</td>\n",
       "      <td>2.8143</td>\n",
       "      <td>45.4539</td>\n",
       "      <td>0.9551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>-0.032200</td>\n",
       "      <td>-0.030543</td>\n",
       "      <td>0.2912</td>\n",
       "      <td>0.2144</td>\n",
       "      <td>84.1624</td>\n",
       "      <td>78.9521</td>\n",
       "      <td>52.0954</td>\n",
       "      <td>0.9719</td>\n",
       "      <td>9.1092</td>\n",
       "      <td>48.8147</td>\n",
       "      <td>0.9747</td>\n",
       "      <td>5.7258</td>\n",
       "      <td>47.3859</td>\n",
       "      <td>1.0095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>-0.030300</td>\n",
       "      <td>-0.028068</td>\n",
       "      <td>0.1895</td>\n",
       "      <td>0.1256</td>\n",
       "      <td>88.1164</td>\n",
       "      <td>85.1748</td>\n",
       "      <td>51.7029</td>\n",
       "      <td>0.9589</td>\n",
       "      <td>5.2686</td>\n",
       "      <td>50.3642</td>\n",
       "      <td>0.9938</td>\n",
       "      <td>3.6784</td>\n",
       "      <td>47.6685</td>\n",
       "      <td>0.9816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>-0.033900</td>\n",
       "      <td>-0.024841</td>\n",
       "      <td>0.1932</td>\n",
       "      <td>0.1389</td>\n",
       "      <td>87.2684</td>\n",
       "      <td>83.8439</td>\n",
       "      <td>51.5302</td>\n",
       "      <td>0.9656</td>\n",
       "      <td>5.9197</td>\n",
       "      <td>50.4738</td>\n",
       "      <td>0.971</td>\n",
       "      <td>4.3721</td>\n",
       "      <td>48.166</td>\n",
       "      <td>0.9761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>-0.031000</td>\n",
       "      <td>-0.019058</td>\n",
       "      <td>0.0520</td>\n",
       "      <td>0.044</td>\n",
       "      <td>90.399</td>\n",
       "      <td>88.0222</td>\n",
       "      <td>51.1189</td>\n",
       "      <td>0.9502</td>\n",
       "      <td>4.3092</td>\n",
       "      <td>50.7997</td>\n",
       "      <td>0.9736</td>\n",
       "      <td>2.9659</td>\n",
       "      <td>48.6781</td>\n",
       "      <td>0.9866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>-0.031300</td>\n",
       "      <td>-0.026003</td>\n",
       "      <td>0.1194</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>89.4349</td>\n",
       "      <td>86.9488</td>\n",
       "      <td>51.5228</td>\n",
       "      <td>0.9492</td>\n",
       "      <td>4.4882</td>\n",
       "      <td>51.5589</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>3.0541</td>\n",
       "      <td>47.9344</td>\n",
       "      <td>1.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>-0.033400</td>\n",
       "      <td>-0.029101</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.1568</td>\n",
       "      <td>88.6873</td>\n",
       "      <td>86.0723</td>\n",
       "      <td>51.7348</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>4.9223</td>\n",
       "      <td>50.3294</td>\n",
       "      <td>0.9842</td>\n",
       "      <td>3.1248</td>\n",
       "      <td>48.7787</td>\n",
       "      <td>0.9968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>-0.034500</td>\n",
       "      <td>-0.017054</td>\n",
       "      <td>0.0392</td>\n",
       "      <td>0.0264</td>\n",
       "      <td>89.6917</td>\n",
       "      <td>87.2923</td>\n",
       "      <td>51.0218</td>\n",
       "      <td>0.952</td>\n",
       "      <td>4.3660</td>\n",
       "      <td>50.406</td>\n",
       "      <td>1.0081</td>\n",
       "      <td>2.8324</td>\n",
       "      <td>48.7815</td>\n",
       "      <td>0.9429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>-0.035100</td>\n",
       "      <td>-0.031140</td>\n",
       "      <td>0.2557</td>\n",
       "      <td>0.2074</td>\n",
       "      <td>90.3036</td>\n",
       "      <td>87.9963</td>\n",
       "      <td>51.8315</td>\n",
       "      <td>0.9685</td>\n",
       "      <td>4.2295</td>\n",
       "      <td>49.417</td>\n",
       "      <td>0.9596</td>\n",
       "      <td>2.7303</td>\n",
       "      <td>48.5011</td>\n",
       "      <td>0.9617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>-0.035600</td>\n",
       "      <td>-0.023028</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>90.3534</td>\n",
       "      <td>88.0612</td>\n",
       "      <td>51.3777</td>\n",
       "      <td>0.9358</td>\n",
       "      <td>4.3427</td>\n",
       "      <td>49.9928</td>\n",
       "      <td>0.9307</td>\n",
       "      <td>2.7156</td>\n",
       "      <td>47.7538</td>\n",
       "      <td>0.9800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>-0.035000</td>\n",
       "      <td>-0.028873</td>\n",
       "      <td>0.1553</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>92.1225</td>\n",
       "      <td>90.3321</td>\n",
       "      <td>51.6194</td>\n",
       "      <td>0.9533</td>\n",
       "      <td>3.5261</td>\n",
       "      <td>50.8856</td>\n",
       "      <td>0.9452</td>\n",
       "      <td>2.1431</td>\n",
       "      <td>49.7008</td>\n",
       "      <td>0.9742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>-0.039000</td>\n",
       "      <td>-0.031795</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>0.1527</td>\n",
       "      <td>90.6744</td>\n",
       "      <td>88.5059</td>\n",
       "      <td>51.8282</td>\n",
       "      <td>0.9551</td>\n",
       "      <td>4.3165</td>\n",
       "      <td>50.6800</td>\n",
       "      <td>0.9736</td>\n",
       "      <td>2.5571</td>\n",
       "      <td>49.1804</td>\n",
       "      <td>0.9433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>-0.036700</td>\n",
       "      <td>-0.027911</td>\n",
       "      <td>0.1243</td>\n",
       "      <td>0.1036</td>\n",
       "      <td>90.6007</td>\n",
       "      <td>88.3968</td>\n",
       "      <td>51.6104</td>\n",
       "      <td>0.9468</td>\n",
       "      <td>4.2515</td>\n",
       "      <td>50.3556</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>2.6155</td>\n",
       "      <td>49.3542</td>\n",
       "      <td>1.0024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>-0.040600</td>\n",
       "      <td>-0.024298</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0119</td>\n",
       "      <td>91.8046</td>\n",
       "      <td>89.982</td>\n",
       "      <td>51.3778</td>\n",
       "      <td>0.9337</td>\n",
       "      <td>3.6576</td>\n",
       "      <td>50.4789</td>\n",
       "      <td>0.9457</td>\n",
       "      <td>2.1531</td>\n",
       "      <td>49.5714</td>\n",
       "      <td>0.9433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>-0.038100</td>\n",
       "      <td>-0.026767</td>\n",
       "      <td>0.1052</td>\n",
       "      <td>0.077</td>\n",
       "      <td>91.3393</td>\n",
       "      <td>89.3664</td>\n",
       "      <td>51.5217</td>\n",
       "      <td>0.9466</td>\n",
       "      <td>3.9184</td>\n",
       "      <td>50.7810</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>2.3745</td>\n",
       "      <td>48.3576</td>\n",
       "      <td>0.9897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>-0.040400</td>\n",
       "      <td>-0.033500</td>\n",
       "      <td>0.2296</td>\n",
       "      <td>0.1763</td>\n",
       "      <td>90.9466</td>\n",
       "      <td>88.8084</td>\n",
       "      <td>51.9572</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>4.1008</td>\n",
       "      <td>50.3331</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>2.629</td>\n",
       "      <td>45.3932</td>\n",
       "      <td>1.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>-0.035800</td>\n",
       "      <td>-0.034485</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.1179</td>\n",
       "      <td>92.4682</td>\n",
       "      <td>90.7277</td>\n",
       "      <td>51.9343</td>\n",
       "      <td>0.9432</td>\n",
       "      <td>3.6475</td>\n",
       "      <td>50.3588</td>\n",
       "      <td>0.9536</td>\n",
       "      <td>2.0461</td>\n",
       "      <td>49.8038</td>\n",
       "      <td>0.9698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>-0.041400</td>\n",
       "      <td>-0.032697</td>\n",
       "      <td>0.1270</td>\n",
       "      <td>0.0921</td>\n",
       "      <td>91.3066</td>\n",
       "      <td>89.3311</td>\n",
       "      <td>51.8359</td>\n",
       "      <td>0.9394</td>\n",
       "      <td>4.0335</td>\n",
       "      <td>51.1155</td>\n",
       "      <td>0.957</td>\n",
       "      <td>2.3250</td>\n",
       "      <td>51.036</td>\n",
       "      <td>0.9572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>-0.039500</td>\n",
       "      <td>-0.029708</td>\n",
       "      <td>0.1329</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>91.3796</td>\n",
       "      <td>89.4041</td>\n",
       "      <td>51.6897</td>\n",
       "      <td>0.9464</td>\n",
       "      <td>4.0437</td>\n",
       "      <td>50.2875</td>\n",
       "      <td>0.9615</td>\n",
       "      <td>2.3036</td>\n",
       "      <td>50.4164</td>\n",
       "      <td>0.9872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>-0.040000</td>\n",
       "      <td>-0.028285</td>\n",
       "      <td>0.1176</td>\n",
       "      <td>0.0942</td>\n",
       "      <td>89.6609</td>\n",
       "      <td>87.2356</td>\n",
       "      <td>51.6148</td>\n",
       "      <td>0.9464</td>\n",
       "      <td>4.9699</td>\n",
       "      <td>51.2032</td>\n",
       "      <td>0.9629</td>\n",
       "      <td>2.8542</td>\n",
       "      <td>50.2429</td>\n",
       "      <td>0.9526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>-0.039900</td>\n",
       "      <td>-0.031963</td>\n",
       "      <td>0.1839</td>\n",
       "      <td>0.1508</td>\n",
       "      <td>91.3793</td>\n",
       "      <td>89.4565</td>\n",
       "      <td>51.7862</td>\n",
       "      <td>0.9529</td>\n",
       "      <td>4.0197</td>\n",
       "      <td>51.0570</td>\n",
       "      <td>0.9835</td>\n",
       "      <td>2.2848</td>\n",
       "      <td>50.9538</td>\n",
       "      <td>0.9842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>-0.040700</td>\n",
       "      <td>-0.031106</td>\n",
       "      <td>0.1347</td>\n",
       "      <td>0.0961</td>\n",
       "      <td>90.6948</td>\n",
       "      <td>88.5744</td>\n",
       "      <td>51.7622</td>\n",
       "      <td>0.9437</td>\n",
       "      <td>4.3595</td>\n",
       "      <td>50.6972</td>\n",
       "      <td>0.9823</td>\n",
       "      <td>2.4849</td>\n",
       "      <td>50.8623</td>\n",
       "      <td>0.9933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>-0.043000</td>\n",
       "      <td>-0.026828</td>\n",
       "      <td>0.0785</td>\n",
       "      <td>0.0471</td>\n",
       "      <td>91.4990</td>\n",
       "      <td>89.536</td>\n",
       "      <td>51.5052</td>\n",
       "      <td>0.9415</td>\n",
       "      <td>4.1102</td>\n",
       "      <td>50.8346</td>\n",
       "      <td>0.9630</td>\n",
       "      <td>2.312</td>\n",
       "      <td>50.2909</td>\n",
       "      <td>0.9593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>-0.040800</td>\n",
       "      <td>-0.031659</td>\n",
       "      <td>0.1522</td>\n",
       "      <td>0.1151</td>\n",
       "      <td>91.2526</td>\n",
       "      <td>89.2294</td>\n",
       "      <td>51.7878</td>\n",
       "      <td>0.9466</td>\n",
       "      <td>4.2217</td>\n",
       "      <td>51.1584</td>\n",
       "      <td>0.9548</td>\n",
       "      <td>2.3304</td>\n",
       "      <td>50.5213</td>\n",
       "      <td>0.9558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>-0.041300</td>\n",
       "      <td>-0.036530</td>\n",
       "      <td>0.2523</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>90.3826</td>\n",
       "      <td>88.0926</td>\n",
       "      <td>52.0943</td>\n",
       "      <td>0.9566</td>\n",
       "      <td>4.7598</td>\n",
       "      <td>50.7273</td>\n",
       "      <td>0.9765</td>\n",
       "      <td>2.6979</td>\n",
       "      <td>50.3826</td>\n",
       "      <td>0.9794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>-0.039100</td>\n",
       "      <td>-0.034071</td>\n",
       "      <td>0.1922</td>\n",
       "      <td>0.1404</td>\n",
       "      <td>90.8625</td>\n",
       "      <td>88.766</td>\n",
       "      <td>51.9279</td>\n",
       "      <td>0.9497</td>\n",
       "      <td>4.4518</td>\n",
       "      <td>50.7835</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>2.5041</td>\n",
       "      <td>50.3685</td>\n",
       "      <td>0.9669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>-0.041000</td>\n",
       "      <td>-0.029454</td>\n",
       "      <td>0.1447</td>\n",
       "      <td>0.0938</td>\n",
       "      <td>90.4710</td>\n",
       "      <td>88.2617</td>\n",
       "      <td>51.6767</td>\n",
       "      <td>0.9494</td>\n",
       "      <td>4.6551</td>\n",
       "      <td>50.9654</td>\n",
       "      <td>0.9720</td>\n",
       "      <td>2.5999</td>\n",
       "      <td>50.365</td>\n",
       "      <td>1.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>-0.040600</td>\n",
       "      <td>-0.028291</td>\n",
       "      <td>0.1239</td>\n",
       "      <td>0.0904</td>\n",
       "      <td>91.255</td>\n",
       "      <td>89.2527</td>\n",
       "      <td>51.5945</td>\n",
       "      <td>0.9473</td>\n",
       "      <td>4.2747</td>\n",
       "      <td>50.4122</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>2.3476</td>\n",
       "      <td>50.2176</td>\n",
       "      <td>0.9984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>-0.043400</td>\n",
       "      <td>-0.034656</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>0.138</td>\n",
       "      <td>91.1402</td>\n",
       "      <td>89.1368</td>\n",
       "      <td>51.9532</td>\n",
       "      <td>0.9493</td>\n",
       "      <td>4.3104</td>\n",
       "      <td>50.6483</td>\n",
       "      <td>1.0034</td>\n",
       "      <td>2.3845</td>\n",
       "      <td>50.3323</td>\n",
       "      <td>0.9647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>-0.041700</td>\n",
       "      <td>-0.034684</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.1455</td>\n",
       "      <td>90.9905</td>\n",
       "      <td>88.9156</td>\n",
       "      <td>51.9632</td>\n",
       "      <td>0.9495</td>\n",
       "      <td>4.4386</td>\n",
       "      <td>50.8058</td>\n",
       "      <td>0.9762</td>\n",
       "      <td>2.4492</td>\n",
       "      <td>49.917</td>\n",
       "      <td>0.9890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>-0.049900</td>\n",
       "      <td>-0.034690</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>0.1234</td>\n",
       "      <td>91.4952</td>\n",
       "      <td>89.6014</td>\n",
       "      <td>51.9273</td>\n",
       "      <td>0.9460</td>\n",
       "      <td>4.121</td>\n",
       "      <td>51.5586</td>\n",
       "      <td>0.9588</td>\n",
       "      <td>2.2598</td>\n",
       "      <td>50.6183</td>\n",
       "      <td>0.9855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>-0.043400</td>\n",
       "      <td>-0.032373</td>\n",
       "      <td>0.1363</td>\n",
       "      <td>0.0952</td>\n",
       "      <td>91.1033</td>\n",
       "      <td>89.0559</td>\n",
       "      <td>51.8106</td>\n",
       "      <td>0.9419</td>\n",
       "      <td>4.3872</td>\n",
       "      <td>51.3797</td>\n",
       "      <td>0.9708</td>\n",
       "      <td>2.4242</td>\n",
       "      <td>50.471</td>\n",
       "      <td>0.9689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>-0.045100</td>\n",
       "      <td>-0.033020</td>\n",
       "      <td>0.1471</td>\n",
       "      <td>0.1041</td>\n",
       "      <td>91.1679</td>\n",
       "      <td>89.1397</td>\n",
       "      <td>51.8678</td>\n",
       "      <td>0.9423</td>\n",
       "      <td>4.3706</td>\n",
       "      <td>50.7777</td>\n",
       "      <td>0.9783</td>\n",
       "      <td>2.4251</td>\n",
       "      <td>50.0473</td>\n",
       "      <td>0.9778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>-0.042500</td>\n",
       "      <td>-0.032663</td>\n",
       "      <td>0.1428</td>\n",
       "      <td>0.1032</td>\n",
       "      <td>90.8375</td>\n",
       "      <td>88.7372</td>\n",
       "      <td>51.8474</td>\n",
       "      <td>0.9425</td>\n",
       "      <td>4.5277</td>\n",
       "      <td>50.9696</td>\n",
       "      <td>0.9712</td>\n",
       "      <td>2.5047</td>\n",
       "      <td>49.9396</td>\n",
       "      <td>0.9745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>-0.041600</td>\n",
       "      <td>-0.033044</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>0.1082</td>\n",
       "      <td>90.5023</td>\n",
       "      <td>88.3208</td>\n",
       "      <td>51.8698</td>\n",
       "      <td>0.9433</td>\n",
       "      <td>4.6955</td>\n",
       "      <td>50.8683</td>\n",
       "      <td>0.9494</td>\n",
       "      <td>2.6269</td>\n",
       "      <td>50.7244</td>\n",
       "      <td>0.9778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>-0.043800</td>\n",
       "      <td>-0.031185</td>\n",
       "      <td>0.1187</td>\n",
       "      <td>0.0849</td>\n",
       "      <td>90.8567</td>\n",
       "      <td>88.7552</td>\n",
       "      <td>51.7609</td>\n",
       "      <td>0.9405</td>\n",
       "      <td>4.5166</td>\n",
       "      <td>50.9696</td>\n",
       "      <td>0.9780</td>\n",
       "      <td>2.4836</td>\n",
       "      <td>49.8384</td>\n",
       "      <td>0.9802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>-0.042100</td>\n",
       "      <td>-0.030922</td>\n",
       "      <td>0.1167</td>\n",
       "      <td>0.0828</td>\n",
       "      <td>90.8726</td>\n",
       "      <td>88.7763</td>\n",
       "      <td>51.7449</td>\n",
       "      <td>0.9404</td>\n",
       "      <td>4.5113</td>\n",
       "      <td>51.0274</td>\n",
       "      <td>0.9915</td>\n",
       "      <td>2.4884</td>\n",
       "      <td>49.8303</td>\n",
       "      <td>0.9719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>-0.045700</td>\n",
       "      <td>-0.030680</td>\n",
       "      <td>0.1125</td>\n",
       "      <td>0.0793</td>\n",
       "      <td>90.9374</td>\n",
       "      <td>88.8479</td>\n",
       "      <td>51.7326</td>\n",
       "      <td>0.9401</td>\n",
       "      <td>4.4896</td>\n",
       "      <td>50.8582</td>\n",
       "      <td>0.9864</td>\n",
       "      <td>2.4759</td>\n",
       "      <td>50.0316</td>\n",
       "      <td>0.9691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>-0.045800</td>\n",
       "      <td>-0.031429</td>\n",
       "      <td>0.1236</td>\n",
       "      <td>0.0875</td>\n",
       "      <td>90.9192</td>\n",
       "      <td>88.8376</td>\n",
       "      <td>51.7743</td>\n",
       "      <td>0.9407</td>\n",
       "      <td>4.513</td>\n",
       "      <td>50.8699</td>\n",
       "      <td>0.9909</td>\n",
       "      <td>2.4491</td>\n",
       "      <td>50.0043</td>\n",
       "      <td>0.9777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>-0.043800</td>\n",
       "      <td>-0.031280</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>0.0848</td>\n",
       "      <td>90.9406</td>\n",
       "      <td>88.8596</td>\n",
       "      <td>51.7671</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>4.4930</td>\n",
       "      <td>50.8192</td>\n",
       "      <td>0.9971</td>\n",
       "      <td>2.4605</td>\n",
       "      <td>50.0784</td>\n",
       "      <td>0.9650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>-0.042800</td>\n",
       "      <td>-0.031279</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.0849</td>\n",
       "      <td>90.9441</td>\n",
       "      <td>88.8643</td>\n",
       "      <td>51.7672</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>4.4915</td>\n",
       "      <td>50.8369</td>\n",
       "      <td>0.9947</td>\n",
       "      <td>2.4606</td>\n",
       "      <td>50.0720</td>\n",
       "      <td>0.9644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8804, training_loss=-0.03765361434502907, metrics={'train_runtime': 2925.8357, 'train_samples_per_second': 12.035, 'train_steps_per_second': 3.009, 'total_flos': 0.0, 'train_loss': -0.03765361434502907, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 2e-4, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# RL loss\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1201' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1201/8804 05:23 < 34:09, 3.71 it/s, Epoch 0.14/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>-0.002100</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-bb1c47517fd0>:15: RuntimeWarning: Mean of empty slice.\n",
      "  'full trade accuracy': (soft_profit[abs_trade >= .7] > 0).mean() * 100,\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:16: RuntimeWarning: Mean of empty slice.\n",
      "  'full trade g/l': soft_profit[(abs_trade >= .7) & (soft_profit > 0)].mean()\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:17: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade >= .7) & (soft_profit < 0)].mean(),\n",
      "<ipython-input-2-bb1c47517fd0>:20: RuntimeWarning: Mean of empty slice.\n",
      "  'medium trade accuracy': (soft_profit[(abs_trade < .7) & (abs_trade >= .4)] > 0).mean() * 100,\n",
      "<ipython-input-2-bb1c47517fd0>:21: RuntimeWarning: Mean of empty slice.\n",
      "  'medium trade g/l': soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit > 0)].mean()\n",
      "<ipython-input-2-bb1c47517fd0>:22: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit < 0)].mean(),\n",
      "<ipython-input-2-bb1c47517fd0>:25: RuntimeWarning: Mean of empty slice.\n",
      "  'small trade accuracy': (soft_profit[(abs_trade < .4) & (abs_trade >= .2)] > 0).mean() * 100,\n",
      "<ipython-input-2-bb1c47517fd0>:26: RuntimeWarning: Mean of empty slice.\n",
      "  'small trade g/l': soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit > 0)].mean()\n",
      "<ipython-input-2-bb1c47517fd0>:27: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit < 0)].mean(),\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-578e75ab8f1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# capture rate maximization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1850\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1852\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1853\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1854\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2113\u001b[0m                     )\n\u001b[0;32m   2114\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2115\u001b[1;33m                 \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2116\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2809\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2810\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2811\u001b[1;33m         output = eval_loop(\n\u001b[0m\u001b[0;32m   2812\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2813\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Evaluation\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   2977\u001b[0m         \u001b[0mobserved_num_examples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2978\u001b[0m         \u001b[1;31m# Main evaluation loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2979\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2980\u001b[0m             \u001b[1;31m# Update the observed num examples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2981\u001b[0m             \u001b[0mobserved_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_batch_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    679\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa: F811\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[1;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2356\u001b[1;33m         return self._getitem(\n\u001b[0m\u001b[0;32m   2357\u001b[0m             \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2358\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[1;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[0;32m   2338\u001b[0m         \u001b[0mformat_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformat_kwargs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mformat_kwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2339\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2340\u001b[1;33m         \u001b[0mpa_subtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2341\u001b[0m         formatted_output = format_table(\n\u001b[0;32m   2342\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mquery_table\u001b[1;34m(table, key, indices)\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;31m# Query the main table\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m         \u001b[0mpa_subtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_query_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    467\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_query_table_with_indices_mapping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36m_query_table\u001b[1;34m(table, key)\u001b[0m\n\u001b[0;32m     80\u001b[0m     \"\"\"\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfast_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\table.py\u001b[0m in \u001b[0;36mfast_slice\u001b[1;34m(self, offset, length)\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mbatches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlength\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_offsets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[0mbatches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moffset\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_offsets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_schema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 2e-4, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# capture rate maximization\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2175' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2175/8804 12:09 < 37:05, 2.98 it/s, Epoch 0.25/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.499500</td>\n",
       "      <td>0.489494</td>\n",
       "      <td>0.1534</td>\n",
       "      <td>0.2027</td>\n",
       "      <td>72.85</td>\n",
       "      <td>65.769</td>\n",
       "      <td>50.0193</td>\n",
       "      <td>1.008</td>\n",
       "      <td>18.6264</td>\n",
       "      <td>49.4785</td>\n",
       "      <td>1.025</td>\n",
       "      <td>7.8575</td>\n",
       "      <td>49.6211</td>\n",
       "      <td>0.9973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.480600</td>\n",
       "      <td>0.501306</td>\n",
       "      <td>-0.0139</td>\n",
       "      <td>-0.0154</td>\n",
       "      <td>93.4643</td>\n",
       "      <td>92.2364</td>\n",
       "      <td>49.6699</td>\n",
       "      <td>0.9737</td>\n",
       "      <td>3.7304</td>\n",
       "      <td>49.1587</td>\n",
       "      <td>1.0083</td>\n",
       "      <td>1.8124</td>\n",
       "      <td>48.9127</td>\n",
       "      <td>1.0122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.469800</td>\n",
       "      <td>0.478019</td>\n",
       "      <td>0.2312</td>\n",
       "      <td>0.1973</td>\n",
       "      <td>95.9213</td>\n",
       "      <td>95.1104</td>\n",
       "      <td>50.4450</td>\n",
       "      <td>0.9963</td>\n",
       "      <td>2.1879</td>\n",
       "      <td>49.1255</td>\n",
       "      <td>0.9847</td>\n",
       "      <td>1.1039</td>\n",
       "      <td>48.0400</td>\n",
       "      <td>0.9647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.471800</td>\n",
       "      <td>0.483449</td>\n",
       "      <td>0.1888</td>\n",
       "      <td>0.2235</td>\n",
       "      <td>95.9085</td>\n",
       "      <td>95.1358</td>\n",
       "      <td>50.3231</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>2.2032</td>\n",
       "      <td>49.399</td>\n",
       "      <td>1.0041</td>\n",
       "      <td>1.0824</td>\n",
       "      <td>50.1589</td>\n",
       "      <td>1.0239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.463100</td>\n",
       "      <td>0.476317</td>\n",
       "      <td>0.2214</td>\n",
       "      <td>0.1897</td>\n",
       "      <td>96.4928</td>\n",
       "      <td>95.7666</td>\n",
       "      <td>50.4603</td>\n",
       "      <td>0.9934</td>\n",
       "      <td>1.8779</td>\n",
       "      <td>48.623</td>\n",
       "      <td>0.9775</td>\n",
       "      <td>0.9217</td>\n",
       "      <td>49.1685</td>\n",
       "      <td>0.9927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.468800</td>\n",
       "      <td>0.495510</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>97.2626</td>\n",
       "      <td>96.7489</td>\n",
       "      <td>50.0071</td>\n",
       "      <td>0.9655</td>\n",
       "      <td>1.3549</td>\n",
       "      <td>49.2227</td>\n",
       "      <td>0.9601</td>\n",
       "      <td>0.6828</td>\n",
       "      <td>48.9768</td>\n",
       "      <td>1.0041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.461000</td>\n",
       "      <td>0.493331</td>\n",
       "      <td>0.0486</td>\n",
       "      <td>0.0503</td>\n",
       "      <td>97.3426</td>\n",
       "      <td>96.8219</td>\n",
       "      <td>50.2092</td>\n",
       "      <td>0.9661</td>\n",
       "      <td>1.3210</td>\n",
       "      <td>48.5279</td>\n",
       "      <td>1.0549</td>\n",
       "      <td>0.6511</td>\n",
       "      <td>47.1172</td>\n",
       "      <td>1.0361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.468600</td>\n",
       "      <td>0.492745</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>97.3108</td>\n",
       "      <td>96.7954</td>\n",
       "      <td>50.2333</td>\n",
       "      <td>0.9549</td>\n",
       "      <td>1.3311</td>\n",
       "      <td>50.0705</td>\n",
       "      <td>0.9720</td>\n",
       "      <td>0.6578</td>\n",
       "      <td>49.0569</td>\n",
       "      <td>1.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.471600</td>\n",
       "      <td>0.488367</td>\n",
       "      <td>0.0472</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>97.1133</td>\n",
       "      <td>96.5368</td>\n",
       "      <td>50.3070</td>\n",
       "      <td>0.9608</td>\n",
       "      <td>1.4689</td>\n",
       "      <td>49.3647</td>\n",
       "      <td>1.0104</td>\n",
       "      <td>0.7415</td>\n",
       "      <td>50.1055</td>\n",
       "      <td>1.0070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.464200</td>\n",
       "      <td>0.506962</td>\n",
       "      <td>-0.1320</td>\n",
       "      <td>-0.1208</td>\n",
       "      <td>97.3123</td>\n",
       "      <td>96.7868</td>\n",
       "      <td>49.6159</td>\n",
       "      <td>0.9509</td>\n",
       "      <td>1.3410</td>\n",
       "      <td>49.0942</td>\n",
       "      <td>0.9981</td>\n",
       "      <td>0.6648</td>\n",
       "      <td>47.7886</td>\n",
       "      <td>1.0018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-68800826f4b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# standardized trade loss maximization, losses - 1 (summed per day per timeframe)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1773\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1774\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1775\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1776\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1777\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2522\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2523\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2525\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2553\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2554\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2555\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2556\u001b[0m         \u001b[1;31m# Save past state if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2557\u001b[0m         \u001b[1;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\trader_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, ohlcv, labels, future, std_future)\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0membed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_embed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mohlcv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\trader_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, mod)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgconv_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_gconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmod\u001b[0m \u001b[1;31m# residual\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\gconv_standalone.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, u, return_kernel)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m         \u001b[0mk_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrfft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (C H L)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 450\u001b[1;33m         \u001b[0mu_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrfft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (B H L)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    451\u001b[0m         \u001b[1;31m# k_f.unsqueeze(-4) * u_f.unsqueeze(-3) # (B C H L)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m         \u001b[0my_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bhl,chl->bchl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 2e-4, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# standardized trade loss maximization, losses - 1 (summed per day per timeframe)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6991' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6991/8804 35:05 < 09:06, 3.32 it/s, Epoch 0.79/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>-0.000561</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.1062</td>\n",
       "      <td>40.0419</td>\n",
       "      <td>15.1137</td>\n",
       "      <td>51.3757</td>\n",
       "      <td>0.9849</td>\n",
       "      <td>32.3397</td>\n",
       "      <td>49.257</td>\n",
       "      <td>0.9928</td>\n",
       "      <td>25.9386</td>\n",
       "      <td>48.8298</td>\n",
       "      <td>1.0049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>-0.004500</td>\n",
       "      <td>0.009183</td>\n",
       "      <td>-0.0564</td>\n",
       "      <td>-0.0588</td>\n",
       "      <td>69.7121</td>\n",
       "      <td>60.2542</td>\n",
       "      <td>50.0799</td>\n",
       "      <td>0.9482</td>\n",
       "      <td>19.7376</td>\n",
       "      <td>48.986</td>\n",
       "      <td>0.9822</td>\n",
       "      <td>10.1397</td>\n",
       "      <td>49.0714</td>\n",
       "      <td>1.0086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>-0.010100</td>\n",
       "      <td>-0.005791</td>\n",
       "      <td>0.2084</td>\n",
       "      <td>0.2407</td>\n",
       "      <td>84.1349</td>\n",
       "      <td>80.3954</td>\n",
       "      <td>50.6436</td>\n",
       "      <td>0.993</td>\n",
       "      <td>9.7433</td>\n",
       "      <td>49.4168</td>\n",
       "      <td>0.9832</td>\n",
       "      <td>4.8717</td>\n",
       "      <td>49.4093</td>\n",
       "      <td>0.9952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>-0.008500</td>\n",
       "      <td>-0.002012</td>\n",
       "      <td>0.2024</td>\n",
       "      <td>0.2274</td>\n",
       "      <td>82.3789</td>\n",
       "      <td>77.9813</td>\n",
       "      <td>50.5956</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>10.4353</td>\n",
       "      <td>49.6698</td>\n",
       "      <td>1.0035</td>\n",
       "      <td>5.6621</td>\n",
       "      <td>50.0055</td>\n",
       "      <td>1.0232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>-0.013500</td>\n",
       "      <td>-0.011989</td>\n",
       "      <td>0.2923</td>\n",
       "      <td>0.2359</td>\n",
       "      <td>77.3891</td>\n",
       "      <td>70.1068</td>\n",
       "      <td>51.1735</td>\n",
       "      <td>1.0041</td>\n",
       "      <td>13.3131</td>\n",
       "      <td>49.5442</td>\n",
       "      <td>1.0132</td>\n",
       "      <td>8.0890</td>\n",
       "      <td>49.3780</td>\n",
       "      <td>1.0145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>-0.012800</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.1110</td>\n",
       "      <td>0.1131</td>\n",
       "      <td>83.8843</td>\n",
       "      <td>79.7948</td>\n",
       "      <td>50.5471</td>\n",
       "      <td>0.968</td>\n",
       "      <td>9.9245</td>\n",
       "      <td>50.0305</td>\n",
       "      <td>0.9902</td>\n",
       "      <td>5.0486</td>\n",
       "      <td>49.9174</td>\n",
       "      <td>0.9896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>-0.015000</td>\n",
       "      <td>-0.000335</td>\n",
       "      <td>0.1599</td>\n",
       "      <td>0.1618</td>\n",
       "      <td>87.232</td>\n",
       "      <td>83.7858</td>\n",
       "      <td>50.7102</td>\n",
       "      <td>0.9755</td>\n",
       "      <td>7.7097</td>\n",
       "      <td>49.2568</td>\n",
       "      <td>1.0095</td>\n",
       "      <td>4.1012</td>\n",
       "      <td>48.8889</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>-0.010600</td>\n",
       "      <td>0.008533</td>\n",
       "      <td>-0.0530</td>\n",
       "      <td>-0.0587</td>\n",
       "      <td>78.5205</td>\n",
       "      <td>71.9053</td>\n",
       "      <td>50.0211</td>\n",
       "      <td>0.9539</td>\n",
       "      <td>11.8166</td>\n",
       "      <td>48.5617</td>\n",
       "      <td>0.9911</td>\n",
       "      <td>7.6855</td>\n",
       "      <td>48.6724</td>\n",
       "      <td>1.0088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>-0.013500</td>\n",
       "      <td>0.011873</td>\n",
       "      <td>-0.1175</td>\n",
       "      <td>-0.1103</td>\n",
       "      <td>76.6533</td>\n",
       "      <td>69.2196</td>\n",
       "      <td>49.9854</td>\n",
       "      <td>0.9307</td>\n",
       "      <td>12.8330</td>\n",
       "      <td>49.4865</td>\n",
       "      <td>0.9854</td>\n",
       "      <td>8.454</td>\n",
       "      <td>48.9473</td>\n",
       "      <td>0.9941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>-0.013700</td>\n",
       "      <td>0.008589</td>\n",
       "      <td>-0.0501</td>\n",
       "      <td>-0.0521</td>\n",
       "      <td>80.883</td>\n",
       "      <td>75.0791</td>\n",
       "      <td>50.1121</td>\n",
       "      <td>0.9449</td>\n",
       "      <td>10.6079</td>\n",
       "      <td>49.425</td>\n",
       "      <td>1.0221</td>\n",
       "      <td>6.7614</td>\n",
       "      <td>48.7331</td>\n",
       "      <td>1.0072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>-0.015100</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>82.6932</td>\n",
       "      <td>77.4804</td>\n",
       "      <td>50.3712</td>\n",
       "      <td>0.9521</td>\n",
       "      <td>9.9226</td>\n",
       "      <td>49.3863</td>\n",
       "      <td>0.9824</td>\n",
       "      <td>6.0074</td>\n",
       "      <td>49.0757</td>\n",
       "      <td>0.9878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>-0.011500</td>\n",
       "      <td>-0.009812</td>\n",
       "      <td>0.2219</td>\n",
       "      <td>0.2466</td>\n",
       "      <td>82.4876</td>\n",
       "      <td>77.2765</td>\n",
       "      <td>50.7659</td>\n",
       "      <td>0.9918</td>\n",
       "      <td>9.8444</td>\n",
       "      <td>49.5276</td>\n",
       "      <td>1.0128</td>\n",
       "      <td>6.0949</td>\n",
       "      <td>49.1899</td>\n",
       "      <td>0.9931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>-0.015200</td>\n",
       "      <td>0.010828</td>\n",
       "      <td>-0.0671</td>\n",
       "      <td>-0.0584</td>\n",
       "      <td>82.3409</td>\n",
       "      <td>76.8501</td>\n",
       "      <td>50.1850</td>\n",
       "      <td>0.9384</td>\n",
       "      <td>9.8529</td>\n",
       "      <td>49.1211</td>\n",
       "      <td>0.9639</td>\n",
       "      <td>6.2825</td>\n",
       "      <td>49.0108</td>\n",
       "      <td>0.9906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>-0.016400</td>\n",
       "      <td>-0.001537</td>\n",
       "      <td>0.0916</td>\n",
       "      <td>0.1212</td>\n",
       "      <td>83.1907</td>\n",
       "      <td>78.1618</td>\n",
       "      <td>50.6305</td>\n",
       "      <td>0.9605</td>\n",
       "      <td>9.0580</td>\n",
       "      <td>49.9908</td>\n",
       "      <td>0.9689</td>\n",
       "      <td>5.8934</td>\n",
       "      <td>50.0584</td>\n",
       "      <td>0.9741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>-0.017500</td>\n",
       "      <td>-0.000304</td>\n",
       "      <td>0.0837</td>\n",
       "      <td>0.0943</td>\n",
       "      <td>79.8562</td>\n",
       "      <td>73.7698</td>\n",
       "      <td>50.8038</td>\n",
       "      <td>0.95</td>\n",
       "      <td>10.6198</td>\n",
       "      <td>49.8458</td>\n",
       "      <td>0.9997</td>\n",
       "      <td>7.2613</td>\n",
       "      <td>49.5714</td>\n",
       "      <td>0.9794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>-0.019800</td>\n",
       "      <td>-0.001603</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>84.1320</td>\n",
       "      <td>79.4182</td>\n",
       "      <td>50.2936</td>\n",
       "      <td>0.9712</td>\n",
       "      <td>8.9242</td>\n",
       "      <td>49.6372</td>\n",
       "      <td>0.9840</td>\n",
       "      <td>5.5102</td>\n",
       "      <td>49.2629</td>\n",
       "      <td>0.9918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>-0.020000</td>\n",
       "      <td>0.004532</td>\n",
       "      <td>0.0173</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>80.3264</td>\n",
       "      <td>74.2776</td>\n",
       "      <td>50.6787</td>\n",
       "      <td>0.9352</td>\n",
       "      <td>10.8437</td>\n",
       "      <td>49.8216</td>\n",
       "      <td>1.</td>\n",
       "      <td>7.0474</td>\n",
       "      <td>49.7448</td>\n",
       "      <td>0.9978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>-0.019500</td>\n",
       "      <td>0.002558</td>\n",
       "      <td>0.0142</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>78.6984</td>\n",
       "      <td>71.9194</td>\n",
       "      <td>50.5537</td>\n",
       "      <td>0.9383</td>\n",
       "      <td>12.2207</td>\n",
       "      <td>50.1873</td>\n",
       "      <td>0.9797</td>\n",
       "      <td>7.6508</td>\n",
       "      <td>50.0204</td>\n",
       "      <td>0.9908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>-0.019600</td>\n",
       "      <td>-0.009098</td>\n",
       "      <td>0.2174</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>81.1913</td>\n",
       "      <td>75.4645</td>\n",
       "      <td>51.3145</td>\n",
       "      <td>0.9654</td>\n",
       "      <td>10.1786</td>\n",
       "      <td>49.8827</td>\n",
       "      <td>1.0087</td>\n",
       "      <td>6.7391</td>\n",
       "      <td>49.7269</td>\n",
       "      <td>1.0077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>-0.015400</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.0366</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>81.247</td>\n",
       "      <td>75.5649</td>\n",
       "      <td>50.6394</td>\n",
       "      <td>0.9462</td>\n",
       "      <td>10.1097</td>\n",
       "      <td>49.6782</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>6.7056</td>\n",
       "      <td>49.2466</td>\n",
       "      <td>0.9966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>-0.024600</td>\n",
       "      <td>-0.005565</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>0.0901</td>\n",
       "      <td>80.8147</td>\n",
       "      <td>74.5783</td>\n",
       "      <td>51.0327</td>\n",
       "      <td>0.953</td>\n",
       "      <td>11.1496</td>\n",
       "      <td>50.0416</td>\n",
       "      <td>0.9802</td>\n",
       "      <td>6.8789</td>\n",
       "      <td>49.7552</td>\n",
       "      <td>0.9812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>-0.017800</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.0237</td>\n",
       "      <td>0.0257</td>\n",
       "      <td>79.5192</td>\n",
       "      <td>73.1905</td>\n",
       "      <td>50.3973</td>\n",
       "      <td>0.9514</td>\n",
       "      <td>11.0793</td>\n",
       "      <td>49.8405</td>\n",
       "      <td>0.9786</td>\n",
       "      <td>7.3984</td>\n",
       "      <td>49.6096</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>-0.022200</td>\n",
       "      <td>-0.003023</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.069</td>\n",
       "      <td>77.1588</td>\n",
       "      <td>69.7322</td>\n",
       "      <td>50.6113</td>\n",
       "      <td>0.9570</td>\n",
       "      <td>13.6016</td>\n",
       "      <td>49.8126</td>\n",
       "      <td>0.9822</td>\n",
       "      <td>8.1166</td>\n",
       "      <td>49.7296</td>\n",
       "      <td>0.9756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>-0.019000</td>\n",
       "      <td>-0.005895</td>\n",
       "      <td>0.1526</td>\n",
       "      <td>0.1499</td>\n",
       "      <td>80.7867</td>\n",
       "      <td>74.8180</td>\n",
       "      <td>50.7072</td>\n",
       "      <td>0.9695</td>\n",
       "      <td>11.1076</td>\n",
       "      <td>50.2572</td>\n",
       "      <td>1.0103</td>\n",
       "      <td>6.7631</td>\n",
       "      <td>50.1364</td>\n",
       "      <td>1.0182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>-0.018200</td>\n",
       "      <td>-0.000646</td>\n",
       "      <td>0.0866</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>79.0525</td>\n",
       "      <td>72.4298</td>\n",
       "      <td>50.8221</td>\n",
       "      <td>0.9500</td>\n",
       "      <td>11.7236</td>\n",
       "      <td>49.9146</td>\n",
       "      <td>0.9927</td>\n",
       "      <td>7.5691</td>\n",
       "      <td>49.6301</td>\n",
       "      <td>1.0033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>-0.019100</td>\n",
       "      <td>0.003762</td>\n",
       "      <td>0.0501</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>81.2786</td>\n",
       "      <td>75.4992</td>\n",
       "      <td>50.6577</td>\n",
       "      <td>0.9456</td>\n",
       "      <td>10.4833</td>\n",
       "      <td>49.6872</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>6.6542</td>\n",
       "      <td>49.497</td>\n",
       "      <td>0.9946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>-0.021300</td>\n",
       "      <td>-0.002440</td>\n",
       "      <td>0.0947</td>\n",
       "      <td>0.0987</td>\n",
       "      <td>80.0956</td>\n",
       "      <td>73.9491</td>\n",
       "      <td>50.7850</td>\n",
       "      <td>0.9539</td>\n",
       "      <td>11.2511</td>\n",
       "      <td>50.0802</td>\n",
       "      <td>0.9818</td>\n",
       "      <td>7.0339</td>\n",
       "      <td>49.7450</td>\n",
       "      <td>0.9978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>-0.019300</td>\n",
       "      <td>-0.006098</td>\n",
       "      <td>0.1687</td>\n",
       "      <td>0.1713</td>\n",
       "      <td>81.9721</td>\n",
       "      <td>76.4572</td>\n",
       "      <td>50.8837</td>\n",
       "      <td>0.9689</td>\n",
       "      <td>10.0352</td>\n",
       "      <td>49.9896</td>\n",
       "      <td>1.0033</td>\n",
       "      <td>6.3798</td>\n",
       "      <td>50.1806</td>\n",
       "      <td>0.9858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>-0.019100</td>\n",
       "      <td>-0.000990</td>\n",
       "      <td>0.0786</td>\n",
       "      <td>0.0905</td>\n",
       "      <td>79.9555</td>\n",
       "      <td>73.7386</td>\n",
       "      <td>50.5374</td>\n",
       "      <td>0.9623</td>\n",
       "      <td>11.064</td>\n",
       "      <td>49.9255</td>\n",
       "      <td>0.9704</td>\n",
       "      <td>7.1710</td>\n",
       "      <td>49.5907</td>\n",
       "      <td>0.9639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>-0.020600</td>\n",
       "      <td>-0.003007</td>\n",
       "      <td>0.1141</td>\n",
       "      <td>0.1245</td>\n",
       "      <td>77.2238</td>\n",
       "      <td>69.8476</td>\n",
       "      <td>50.8354</td>\n",
       "      <td>0.9552</td>\n",
       "      <td>12.8087</td>\n",
       "      <td>50.3684</td>\n",
       "      <td>0.9906</td>\n",
       "      <td>8.3403</td>\n",
       "      <td>49.5968</td>\n",
       "      <td>1.0099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>-0.021600</td>\n",
       "      <td>-0.003241</td>\n",
       "      <td>0.1093</td>\n",
       "      <td>0.1233</td>\n",
       "      <td>76.7656</td>\n",
       "      <td>69.3012</td>\n",
       "      <td>50.7834</td>\n",
       "      <td>0.9593</td>\n",
       "      <td>12.8299</td>\n",
       "      <td>49.8943</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>8.5210</td>\n",
       "      <td>49.9021</td>\n",
       "      <td>0.9869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>-0.023100</td>\n",
       "      <td>-0.004352</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>0.1377</td>\n",
       "      <td>78.7156</td>\n",
       "      <td>71.948</td>\n",
       "      <td>50.8526</td>\n",
       "      <td>0.9631</td>\n",
       "      <td>11.9663</td>\n",
       "      <td>50.0349</td>\n",
       "      <td>0.9774</td>\n",
       "      <td>7.7066</td>\n",
       "      <td>49.1672</td>\n",
       "      <td>0.9867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>-0.022300</td>\n",
       "      <td>-0.001580</td>\n",
       "      <td>0.0957</td>\n",
       "      <td>0.105</td>\n",
       "      <td>78.2258</td>\n",
       "      <td>71.2947</td>\n",
       "      <td>50.8233</td>\n",
       "      <td>0.9543</td>\n",
       "      <td>12.2250</td>\n",
       "      <td>49.7463</td>\n",
       "      <td>0.9857</td>\n",
       "      <td>7.8554</td>\n",
       "      <td>49.6947</td>\n",
       "      <td>0.9775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>-0.028900</td>\n",
       "      <td>-0.002377</td>\n",
       "      <td>0.1008</td>\n",
       "      <td>0.1056</td>\n",
       "      <td>78.9602</td>\n",
       "      <td>72.2417</td>\n",
       "      <td>50.7880</td>\n",
       "      <td>0.9556</td>\n",
       "      <td>12.0640</td>\n",
       "      <td>50.0143</td>\n",
       "      <td>0.9842</td>\n",
       "      <td>7.5824</td>\n",
       "      <td>49.8556</td>\n",
       "      <td>0.9938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-5581c4bf191a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# standardized trade loss maximization, 1.05x losses (summed per day per timeframe)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1818\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1819\u001b[0m                             \u001b[1;31m# Revert to normal clipping otherwise, handling Apex or full precision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1820\u001b[1;33m                             nn.utils.clip_grad_norm_(\n\u001b[0m\u001b[0;32m   1821\u001b[0m                                 \u001b[0mamp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaster_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_apex\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m                                 \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type, error_if_nonfinite)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnorms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0merror_if_nonfinite\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_or\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         raise RuntimeError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnorms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0merror_if_nonfinite\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_or\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_norm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         raise RuntimeError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m             \u001b[0m_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# noqa: C416 TODO: rewrite as list(range(m))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1451\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m     \u001b[1;31m# TODO: when https://github.com/pytorch/pytorch/issues/33782 is fixed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 2e-4, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# standardized trade loss maximization, 1.05x losses (summed per day per timeframe)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3497' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3497/8804 17:24 < 26:26, 3.35 it/s, Epoch 0.40/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.005342</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.113</td>\n",
       "      <td>12.6436</td>\n",
       "      <td>0.034</td>\n",
       "      <td>42.9448</td>\n",
       "      <td>1.1017</td>\n",
       "      <td>1.1619</td>\n",
       "      <td>50.8211</td>\n",
       "      <td>1.0044</td>\n",
       "      <td>19.5327</td>\n",
       "      <td>50.7378</td>\n",
       "      <td>1.0194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.005200</td>\n",
       "      <td>0.005246</td>\n",
       "      <td>-0.0141</td>\n",
       "      <td>-0.0715</td>\n",
       "      <td>7.4402</td>\n",
       "      <td>0.0959</td>\n",
       "      <td>42.9348</td>\n",
       "      <td>1.0847</td>\n",
       "      <td>0.6865</td>\n",
       "      <td>49.1495</td>\n",
       "      <td>0.9063</td>\n",
       "      <td>4.5672</td>\n",
       "      <td>49.9304</td>\n",
       "      <td>0.9343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.003861</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>0.2529</td>\n",
       "      <td>9.0444</td>\n",
       "      <td>0.0789</td>\n",
       "      <td>54.6896</td>\n",
       "      <td>1.1408</td>\n",
       "      <td>1.2652</td>\n",
       "      <td>55.258</td>\n",
       "      <td>0.9678</td>\n",
       "      <td>7.5670</td>\n",
       "      <td>52.3102</td>\n",
       "      <td>1.0326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.2782</td>\n",
       "      <td>6.4293</td>\n",
       "      <td>0.0315</td>\n",
       "      <td>49.0066</td>\n",
       "      <td>0.8438</td>\n",
       "      <td>0.2949</td>\n",
       "      <td>54.5615</td>\n",
       "      <td>1.1017</td>\n",
       "      <td>3.7770</td>\n",
       "      <td>54.9402</td>\n",
       "      <td>1.0663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.2915</td>\n",
       "      <td>9.5173</td>\n",
       "      <td>0.3122</td>\n",
       "      <td>63.1931</td>\n",
       "      <td>0.8873</td>\n",
       "      <td>3.9717</td>\n",
       "      <td>56.9467</td>\n",
       "      <td>1.122</td>\n",
       "      <td>9.5062</td>\n",
       "      <td>52.1883</td>\n",
       "      <td>0.9550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>-0.000500</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>0.0768</td>\n",
       "      <td>0.1894</td>\n",
       "      <td>16.4891</td>\n",
       "      <td>8.6074</td>\n",
       "      <td>50.7075</td>\n",
       "      <td>1.0064</td>\n",
       "      <td>5.2725</td>\n",
       "      <td>51.9727</td>\n",
       "      <td>0.9780</td>\n",
       "      <td>6.5536</td>\n",
       "      <td>52.1686</td>\n",
       "      <td>0.9778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>-0.000900</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.1693</td>\n",
       "      <td>19.0793</td>\n",
       "      <td>12.4620</td>\n",
       "      <td>50.2217</td>\n",
       "      <td>0.9308</td>\n",
       "      <td>4.8461</td>\n",
       "      <td>47.9602</td>\n",
       "      <td>1.0147</td>\n",
       "      <td>4.7104</td>\n",
       "      <td>50.9507</td>\n",
       "      <td>1.0921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>-0.000700</td>\n",
       "      <td>0.005342</td>\n",
       "      <td>-0.0187</td>\n",
       "      <td>-0.0661</td>\n",
       "      <td>13.7492</td>\n",
       "      <td>6.9533</td>\n",
       "      <td>50.4656</td>\n",
       "      <td>0.8624</td>\n",
       "      <td>4.8705</td>\n",
       "      <td>46.3627</td>\n",
       "      <td>0.9685</td>\n",
       "      <td>4.548</td>\n",
       "      <td>46.6103</td>\n",
       "      <td>0.9616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>-0.004000</td>\n",
       "      <td>0.010139</td>\n",
       "      <td>-0.0265</td>\n",
       "      <td>-0.0514</td>\n",
       "      <td>23.8753</td>\n",
       "      <td>16.5352</td>\n",
       "      <td>51.1322</td>\n",
       "      <td>0.8677</td>\n",
       "      <td>6.4567</td>\n",
       "      <td>48.5950</td>\n",
       "      <td>0.9168</td>\n",
       "      <td>6.5874</td>\n",
       "      <td>47.3788</td>\n",
       "      <td>0.8984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>-0.002900</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.1193</td>\n",
       "      <td>15.144</td>\n",
       "      <td>10.4475</td>\n",
       "      <td>50.8034</td>\n",
       "      <td>0.9174</td>\n",
       "      <td>2.8211</td>\n",
       "      <td>48.0207</td>\n",
       "      <td>0.9747</td>\n",
       "      <td>2.7535</td>\n",
       "      <td>49.6308</td>\n",
       "      <td>0.9808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>-0.004000</td>\n",
       "      <td>0.005119</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>16.5925</td>\n",
       "      <td>10.5102</td>\n",
       "      <td>50.3080</td>\n",
       "      <td>0.8864</td>\n",
       "      <td>3.6668</td>\n",
       "      <td>48.1971</td>\n",
       "      <td>0.9759</td>\n",
       "      <td>3.4014</td>\n",
       "      <td>48.8489</td>\n",
       "      <td>0.9529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>-0.004100</td>\n",
       "      <td>0.005250</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>17.5709</td>\n",
       "      <td>13.522</td>\n",
       "      <td>46.8326</td>\n",
       "      <td>0.9580</td>\n",
       "      <td>3.0870</td>\n",
       "      <td>47.7775</td>\n",
       "      <td>1.0265</td>\n",
       "      <td>2.2938</td>\n",
       "      <td>48.9954</td>\n",
       "      <td>0.9972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>-0.006200</td>\n",
       "      <td>0.005762</td>\n",
       "      <td>-0.0148</td>\n",
       "      <td>-0.0432</td>\n",
       "      <td>18.2483</td>\n",
       "      <td>13.3163</td>\n",
       "      <td>50.6174</td>\n",
       "      <td>0.9059</td>\n",
       "      <td>2.9881</td>\n",
       "      <td>47.5591</td>\n",
       "      <td>0.9172</td>\n",
       "      <td>4.2944</td>\n",
       "      <td>47.4469</td>\n",
       "      <td>0.8498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>-0.005400</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.0516</td>\n",
       "      <td>0.1798</td>\n",
       "      <td>16.6730</td>\n",
       "      <td>12.3098</td>\n",
       "      <td>50.5099</td>\n",
       "      <td>0.9265</td>\n",
       "      <td>2.6164</td>\n",
       "      <td>49.3584</td>\n",
       "      <td>0.9330</td>\n",
       "      <td>2.7957</td>\n",
       "      <td>50.5147</td>\n",
       "      <td>1.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>-0.004300</td>\n",
       "      <td>0.001817</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.1111</td>\n",
       "      <td>15.3596</td>\n",
       "      <td>11.1583</td>\n",
       "      <td>51.2826</td>\n",
       "      <td>0.937</td>\n",
       "      <td>3.2303</td>\n",
       "      <td>47.1917</td>\n",
       "      <td>0.9364</td>\n",
       "      <td>2.5306</td>\n",
       "      <td>48.1808</td>\n",
       "      <td>0.9599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>-0.004800</td>\n",
       "      <td>0.003833</td>\n",
       "      <td>0.0161</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>18.8513</td>\n",
       "      <td>14.1023</td>\n",
       "      <td>50.1002</td>\n",
       "      <td>0.8913</td>\n",
       "      <td>3.5334</td>\n",
       "      <td>49.1722</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>3.1256</td>\n",
       "      <td>50.7406</td>\n",
       "      <td>0.9819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>-0.005300</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.0882</td>\n",
       "      <td>13.8254</td>\n",
       "      <td>8.8138</td>\n",
       "      <td>51.0174</td>\n",
       "      <td>0.8582</td>\n",
       "      <td>3.5496</td>\n",
       "      <td>49.8707</td>\n",
       "      <td>0.9855</td>\n",
       "      <td>3.4194</td>\n",
       "      <td>49.9192</td>\n",
       "      <td>0.9349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a57060df6ca5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# standardized trade loss maximization, 1.1x losses (summed per day per timeframe)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1773\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1774\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1775\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1776\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1777\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2522\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2523\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2525\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2553\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2554\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2555\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2556\u001b[0m         \u001b[1;31m# Save past state if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2557\u001b[0m         \u001b[1;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\trader_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, ohlcv, labels, future, std_future)\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0membed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_embed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mohlcv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\trader_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, mod)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgconv_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_gconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmod\u001b[0m \u001b[1;31m# residual\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\gconv_standalone.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, u, return_kernel)\u001b[0m\n\u001b[0;32m    427\u001b[0m                 torch.arange(k.size(-1), device=k.device)+1).view(1, 1, -1))\n\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_norm_initialized\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             self.kernel_norm_initialized = torch.tensor(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 2e-4, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# standardized trade loss maximization, 1.2x losses (summed per day per timeframe)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1663' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1663/8804 07:58 < 34:16, 3.47 it/s, Epoch 0.19/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>0.018404</td>\n",
       "      <td>-0.0299</td>\n",
       "      <td>-0.1903</td>\n",
       "      <td>4.0203</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>53.6232</td>\n",
       "      <td>0.7830</td>\n",
       "      <td>0.0339</td>\n",
       "      <td>44.9231</td>\n",
       "      <td>1.0468</td>\n",
       "      <td>0.6459</td>\n",
       "      <td>46.2544</td>\n",
       "      <td>0.9501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.009077</td>\n",
       "      <td>0.0192</td>\n",
       "      <td>0.2175</td>\n",
       "      <td>2.8573</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>50.</td>\n",
       "      <td>0.5438</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>43.8017</td>\n",
       "      <td>1.1218</td>\n",
       "      <td>0.061</td>\n",
       "      <td>47.1795</td>\n",
       "      <td>1.0305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.006674</td>\n",
       "      <td>-0.0097</td>\n",
       "      <td>-0.1138</td>\n",
       "      <td>1.4791</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>40.</td>\n",
       "      <td>0.4595</td>\n",
       "      <td>0.0142</td>\n",
       "      <td>49.2647</td>\n",
       "      <td>0.8068</td>\n",
       "      <td>0.1219</td>\n",
       "      <td>46.2789</td>\n",
       "      <td>0.9796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.004292</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.2055</td>\n",
       "      <td>1.3719</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>43.3333</td>\n",
       "      <td>0.4994</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>45.9854</td>\n",
       "      <td>0.9505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.004046</td>\n",
       "      <td>-0.0048</td>\n",
       "      <td>-0.1074</td>\n",
       "      <td>0.8908</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>41.6667</td>\n",
       "      <td>0.6552</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>48.6692</td>\n",
       "      <td>0.9396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.003600</td>\n",
       "      <td>0.002982</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.7442</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>33.3333</td>\n",
       "      <td>0.2523</td>\n",
       "      <td>0.014</td>\n",
       "      <td>49.2537</td>\n",
       "      <td>1.0525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.002077</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0699</td>\n",
       "      <td>0.5410</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>33.3333</td>\n",
       "      <td>0.2757</td>\n",
       "      <td>0.0109</td>\n",
       "      <td>46.6667</td>\n",
       "      <td>1.4051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.003579</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.1118</td>\n",
       "      <td>1.0215</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>55.5556</td>\n",
       "      <td>0.8917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-bb1c47517fd0>:16: RuntimeWarning: Mean of empty slice.\n",
      "  'full trade g/l': soft_profit[(abs_trade >= .7) & (soft_profit > 0)].mean()\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:15: RuntimeWarning: Mean of empty slice.\n",
      "  'full trade accuracy': (soft_profit[abs_trade >= .7] > 0).mean() * 100,\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:17: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade >= .7) & (soft_profit < 0)].mean(),\n",
      "<ipython-input-2-bb1c47517fd0>:21: RuntimeWarning: Mean of empty slice.\n",
      "  'medium trade g/l': soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit > 0)].mean()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a53b3ed884fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# standardized trade loss maximization, 2x losses (summed per day per timeframe)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1775\u001b[0m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1776\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1777\u001b[1;33m                 if (\n\u001b[0m\u001b[0;32m   1778\u001b[0m                     \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1779\u001b[0m                     \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 2e-4, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# standardized trade loss maximization, 2x losses (summed per day per timeframe)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='690' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 690/8804 03:06 < 36:41, 3.69 it/s, Epoch 0.08/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>-0.006200</td>\n",
       "      <td>-0.012604</td>\n",
       "      <td>0.0927</td>\n",
       "      <td>0.1987</td>\n",
       "      <td>74.2320</td>\n",
       "      <td>67.8283</td>\n",
       "      <td>49.8989</td>\n",
       "      <td>1.0032</td>\n",
       "      <td>17.0112</td>\n",
       "      <td>48.6061</td>\n",
       "      <td>1.0048</td>\n",
       "      <td>7.6935</td>\n",
       "      <td>48.8914</td>\n",
       "      <td>0.9780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>-0.020200</td>\n",
       "      <td>-0.004220</td>\n",
       "      <td>-0.0526</td>\n",
       "      <td>-0.0572</td>\n",
       "      <td>93.7773</td>\n",
       "      <td>92.5527</td>\n",
       "      <td>49.5381</td>\n",
       "      <td>0.9697</td>\n",
       "      <td>3.5503</td>\n",
       "      <td>48.9794</td>\n",
       "      <td>1.0179</td>\n",
       "      <td>1.7317</td>\n",
       "      <td>48.7958</td>\n",
       "      <td>1.0198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>-0.025800</td>\n",
       "      <td>-0.020145</td>\n",
       "      <td>0.2237</td>\n",
       "      <td>0.2727</td>\n",
       "      <td>96.1169</td>\n",
       "      <td>95.3575</td>\n",
       "      <td>50.3482</td>\n",
       "      <td>0.9988</td>\n",
       "      <td>2.0785</td>\n",
       "      <td>48.3797</td>\n",
       "      <td>0.9676</td>\n",
       "      <td>1.0335</td>\n",
       "      <td>50.1816</td>\n",
       "      <td>0.9438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b01c985200b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# standardized trade loss maximization (summed per day per timeframe)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1773\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1774\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1775\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1776\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1777\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2522\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2523\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2525\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2553\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2554\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2555\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2556\u001b[0m         \u001b[1;31m# Save past state if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2557\u001b[0m         \u001b[1;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\trader_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, ohlcv, labels, future, std_future)\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[0membed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_embed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mohlcv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\trader_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, mod)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgconv_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_gconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmod\u001b[0m \u001b[1;31m# residual\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\gconv_standalone.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, u, return_kernel)\u001b[0m\n\u001b[0;32m    447\u001b[0m                 \u001b[1;33m+\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m         \u001b[0mk_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrfft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (C H L)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m         \u001b[0mu_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrfft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (B H L)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[1;31m# k_f.unsqueeze(-4) * u_f.unsqueeze(-3) # (B C H L)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 2e-4, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# standardized trade loss maximization (summed per day per timeframe)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8804' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8804/8804 51:51, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.971200</td>\n",
       "      <td>1.938299</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>6.8669</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>8.0119</td>\n",
       "      <td>0.7236</td>\n",
       "      <td>0.3214</td>\n",
       "      <td>10.1882</td>\n",
       "      <td>0.9241</td>\n",
       "      <td>2.1833</td>\n",
       "      <td>45.1836</td>\n",
       "      <td>0.9737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.945700</td>\n",
       "      <td>1.936464</td>\n",
       "      <td>0.0185</td>\n",
       "      <td>0.1949</td>\n",
       "      <td>4.8734</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>23.5294</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.1606</td>\n",
       "      <td>10.4545</td>\n",
       "      <td>1.1331</td>\n",
       "      <td>0.6882</td>\n",
       "      <td>32.9091</td>\n",
       "      <td>0.9401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.937400</td>\n",
       "      <td>1.936441</td>\n",
       "      <td>-0.0308</td>\n",
       "      <td>-0.0786</td>\n",
       "      <td>7.6039</td>\n",
       "      <td>0.4951</td>\n",
       "      <td>5.5181</td>\n",
       "      <td>0.9349</td>\n",
       "      <td>0.7362</td>\n",
       "      <td>31.2606</td>\n",
       "      <td>0.9599</td>\n",
       "      <td>4.4061</td>\n",
       "      <td>48.7457</td>\n",
       "      <td>1.0110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.937800</td>\n",
       "      <td>1.930317</td>\n",
       "      <td>0.0413</td>\n",
       "      <td>0.3174</td>\n",
       "      <td>7.0063</td>\n",
       "      <td>0.5007</td>\n",
       "      <td>0.1666</td>\n",
       "      <td>0.3687</td>\n",
       "      <td>0.2564</td>\n",
       "      <td>14.3148</td>\n",
       "      <td>0.7189</td>\n",
       "      <td>3.0365</td>\n",
       "      <td>51.2208</td>\n",
       "      <td>1.0803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.938300</td>\n",
       "      <td>1.931115</td>\n",
       "      <td>0.0534</td>\n",
       "      <td>0.4053</td>\n",
       "      <td>7.6606</td>\n",
       "      <td>0.6123</td>\n",
       "      <td>1.0388</td>\n",
       "      <td>0.3198</td>\n",
       "      <td>0.3052</td>\n",
       "      <td>12.8801</td>\n",
       "      <td>1.0533</td>\n",
       "      <td>3.2027</td>\n",
       "      <td>55.175</td>\n",
       "      <td>0.9678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.937600</td>\n",
       "      <td>1.933995</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>7.9434</td>\n",
       "      <td>0.0188</td>\n",
       "      <td>30.5556</td>\n",
       "      <td>0.3103</td>\n",
       "      <td>0.4271</td>\n",
       "      <td>36.9141</td>\n",
       "      <td>0.8283</td>\n",
       "      <td>5.9493</td>\n",
       "      <td>49.8878</td>\n",
       "      <td>0.9435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.936300</td>\n",
       "      <td>1.928331</td>\n",
       "      <td>0.0293</td>\n",
       "      <td>0.1675</td>\n",
       "      <td>7.8618</td>\n",
       "      <td>0.5949</td>\n",
       "      <td>1.8054</td>\n",
       "      <td>1.3369</td>\n",
       "      <td>0.4046</td>\n",
       "      <td>39.7680</td>\n",
       "      <td>0.8404</td>\n",
       "      <td>4.4896</td>\n",
       "      <td>53.615</td>\n",
       "      <td>0.9444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.925600</td>\n",
       "      <td>1.926658</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>6.1704</td>\n",
       "      <td>0.7796</td>\n",
       "      <td>5.3096</td>\n",
       "      <td>0.4415</td>\n",
       "      <td>0.3110</td>\n",
       "      <td>34.2608</td>\n",
       "      <td>0.7668</td>\n",
       "      <td>1.6242</td>\n",
       "      <td>56.9686</td>\n",
       "      <td>0.8991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.923400</td>\n",
       "      <td>1.925360</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.1165</td>\n",
       "      <td>5.8019</td>\n",
       "      <td>0.9428</td>\n",
       "      <td>0.7078</td>\n",
       "      <td>0.5284</td>\n",
       "      <td>0.1572</td>\n",
       "      <td>60.6101</td>\n",
       "      <td>0.7372</td>\n",
       "      <td>1.3406</td>\n",
       "      <td>60.4418</td>\n",
       "      <td>0.7325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.925600</td>\n",
       "      <td>1.924275</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>6.1106</td>\n",
       "      <td>0.9062</td>\n",
       "      <td>2.5314</td>\n",
       "      <td>1.1026</td>\n",
       "      <td>0.2692</td>\n",
       "      <td>47.4826</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>1.093</td>\n",
       "      <td>59.1872</td>\n",
       "      <td>0.7205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.930100</td>\n",
       "      <td>1.924826</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>6.9957</td>\n",
       "      <td>0.9367</td>\n",
       "      <td>3.5066</td>\n",
       "      <td>0.5112</td>\n",
       "      <td>0.278</td>\n",
       "      <td>67.4419</td>\n",
       "      <td>0.8378</td>\n",
       "      <td>2.1732</td>\n",
       "      <td>58.5788</td>\n",
       "      <td>0.8939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.928800</td>\n",
       "      <td>1.924266</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.2241</td>\n",
       "      <td>6.8147</td>\n",
       "      <td>0.9682</td>\n",
       "      <td>1.0985</td>\n",
       "      <td>0.7005</td>\n",
       "      <td>0.5181</td>\n",
       "      <td>70.4971</td>\n",
       "      <td>1.0262</td>\n",
       "      <td>3.1274</td>\n",
       "      <td>57.6868</td>\n",
       "      <td>0.9527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.924600</td>\n",
       "      <td>1.924117</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>6.0298</td>\n",
       "      <td>0.9392</td>\n",
       "      <td>4.1190</td>\n",
       "      <td>1.9531</td>\n",
       "      <td>0.3844</td>\n",
       "      <td>68.6737</td>\n",
       "      <td>0.9198</td>\n",
       "      <td>1.8192</td>\n",
       "      <td>62.8991</td>\n",
       "      <td>0.8356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.921300</td>\n",
       "      <td>1.922035</td>\n",
       "      <td>0.0314</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>8.002</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>7.7287</td>\n",
       "      <td>0.3897</td>\n",
       "      <td>0.7071</td>\n",
       "      <td>68.6919</td>\n",
       "      <td>1.0589</td>\n",
       "      <td>4.1967</td>\n",
       "      <td>56.0251</td>\n",
       "      <td>0.9934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.927300</td>\n",
       "      <td>1.924516</td>\n",
       "      <td>0.0314</td>\n",
       "      <td>0.2542</td>\n",
       "      <td>6.6443</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>2.1712</td>\n",
       "      <td>0.7687</td>\n",
       "      <td>0.2068</td>\n",
       "      <td>70.9531</td>\n",
       "      <td>0.7403</td>\n",
       "      <td>1.3519</td>\n",
       "      <td>64.3193</td>\n",
       "      <td>0.8141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.928900</td>\n",
       "      <td>1.920455</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>7.5775</td>\n",
       "      <td>0.6910</td>\n",
       "      <td>2.0522</td>\n",
       "      <td>0.4305</td>\n",
       "      <td>0.6372</td>\n",
       "      <td>47.2754</td>\n",
       "      <td>1.0705</td>\n",
       "      <td>3.1362</td>\n",
       "      <td>58.061</td>\n",
       "      <td>0.9871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.929500</td>\n",
       "      <td>1.930904</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.1314</td>\n",
       "      <td>8.0143</td>\n",
       "      <td>1.0379</td>\n",
       "      <td>6.4798</td>\n",
       "      <td>1.3719</td>\n",
       "      <td>0.7517</td>\n",
       "      <td>64.6414</td>\n",
       "      <td>1.4254</td>\n",
       "      <td>2.5336</td>\n",
       "      <td>54.6588</td>\n",
       "      <td>0.9143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.931200</td>\n",
       "      <td>1.920969</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>6.2744</td>\n",
       "      <td>0.8393</td>\n",
       "      <td>1.6648</td>\n",
       "      <td>1.4859</td>\n",
       "      <td>0.5252</td>\n",
       "      <td>61.4056</td>\n",
       "      <td>1.5351</td>\n",
       "      <td>2.5569</td>\n",
       "      <td>60.9861</td>\n",
       "      <td>1.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.922200</td>\n",
       "      <td>1.920938</td>\n",
       "      <td>0.0321</td>\n",
       "      <td>0.3516</td>\n",
       "      <td>6.6386</td>\n",
       "      <td>0.8791</td>\n",
       "      <td>2.5857</td>\n",
       "      <td>1.2836</td>\n",
       "      <td>0.3134</td>\n",
       "      <td>62.3087</td>\n",
       "      <td>0.9319</td>\n",
       "      <td>2.3189</td>\n",
       "      <td>56.7561</td>\n",
       "      <td>0.9695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.923500</td>\n",
       "      <td>1.919747</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>0.2624</td>\n",
       "      <td>7.5773</td>\n",
       "      <td>1.0380</td>\n",
       "      <td>8.2772</td>\n",
       "      <td>0.8165</td>\n",
       "      <td>1.5826</td>\n",
       "      <td>64.9163</td>\n",
       "      <td>1.2895</td>\n",
       "      <td>3.8029</td>\n",
       "      <td>56.8013</td>\n",
       "      <td>0.9171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.923500</td>\n",
       "      <td>1.918723</td>\n",
       "      <td>0.0396</td>\n",
       "      <td>0.2939</td>\n",
       "      <td>8.7992</td>\n",
       "      <td>1.0731</td>\n",
       "      <td>12.7101</td>\n",
       "      <td>2.1883</td>\n",
       "      <td>2.0152</td>\n",
       "      <td>61.5771</td>\n",
       "      <td>1.2538</td>\n",
       "      <td>4.3011</td>\n",
       "      <td>54.9274</td>\n",
       "      <td>0.9384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.926400</td>\n",
       "      <td>1.915088</td>\n",
       "      <td>0.0466</td>\n",
       "      <td>0.4482</td>\n",
       "      <td>6.7191</td>\n",
       "      <td>0.9129</td>\n",
       "      <td>7.55</td>\n",
       "      <td>0.8113</td>\n",
       "      <td>0.5480</td>\n",
       "      <td>63.0327</td>\n",
       "      <td>1.2114</td>\n",
       "      <td>2.5895</td>\n",
       "      <td>62.644</td>\n",
       "      <td>0.9810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.925400</td>\n",
       "      <td>1.919408</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.3399</td>\n",
       "      <td>6.4818</td>\n",
       "      <td>0.9113</td>\n",
       "      <td>6.1785</td>\n",
       "      <td>6.1036</td>\n",
       "      <td>0.8496</td>\n",
       "      <td>65.9180</td>\n",
       "      <td>1.8065</td>\n",
       "      <td>2.7678</td>\n",
       "      <td>59.4824</td>\n",
       "      <td>1.0447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.920800</td>\n",
       "      <td>1.920035</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.2715</td>\n",
       "      <td>7.0301</td>\n",
       "      <td>0.9756</td>\n",
       "      <td>2.5118</td>\n",
       "      <td>0.4607</td>\n",
       "      <td>0.6787</td>\n",
       "      <td>69.8110</td>\n",
       "      <td>1.1951</td>\n",
       "      <td>2.7940</td>\n",
       "      <td>59.0946</td>\n",
       "      <td>0.9477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.917200</td>\n",
       "      <td>1.918396</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>0.4052</td>\n",
       "      <td>7.6491</td>\n",
       "      <td>1.0179</td>\n",
       "      <td>4.2307</td>\n",
       "      <td>1.9885</td>\n",
       "      <td>1.0956</td>\n",
       "      <td>63.9954</td>\n",
       "      <td>1.102</td>\n",
       "      <td>3.4787</td>\n",
       "      <td>59.0762</td>\n",
       "      <td>1.0184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.924300</td>\n",
       "      <td>1.920318</td>\n",
       "      <td>0.0315</td>\n",
       "      <td>0.3496</td>\n",
       "      <td>6.8452</td>\n",
       "      <td>0.8794</td>\n",
       "      <td>3.5452</td>\n",
       "      <td>5.7180</td>\n",
       "      <td>0.5681</td>\n",
       "      <td>65.4369</td>\n",
       "      <td>1.4845</td>\n",
       "      <td>2.6268</td>\n",
       "      <td>62.7818</td>\n",
       "      <td>1.0385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.915100</td>\n",
       "      <td>1.917119</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.2985</td>\n",
       "      <td>6.7815</td>\n",
       "      <td>0.9902</td>\n",
       "      <td>5.0126</td>\n",
       "      <td>1.3577</td>\n",
       "      <td>0.8363</td>\n",
       "      <td>68.9900</td>\n",
       "      <td>1.3089</td>\n",
       "      <td>2.8808</td>\n",
       "      <td>61.235</td>\n",
       "      <td>1.0716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.920700</td>\n",
       "      <td>1.916194</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>0.3407</td>\n",
       "      <td>7.0542</td>\n",
       "      <td>0.9659</td>\n",
       "      <td>6.2831</td>\n",
       "      <td>4.2146</td>\n",
       "      <td>1.2646</td>\n",
       "      <td>65.4518</td>\n",
       "      <td>1.3505</td>\n",
       "      <td>3.125</td>\n",
       "      <td>59.2759</td>\n",
       "      <td>0.9909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.912900</td>\n",
       "      <td>1.915759</td>\n",
       "      <td>0.0412</td>\n",
       "      <td>0.3786</td>\n",
       "      <td>7.4298</td>\n",
       "      <td>1.1593</td>\n",
       "      <td>14.8138</td>\n",
       "      <td>1.7097</td>\n",
       "      <td>1.7401</td>\n",
       "      <td>63.2191</td>\n",
       "      <td>1.3109</td>\n",
       "      <td>3.2561</td>\n",
       "      <td>57.7321</td>\n",
       "      <td>0.9878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.913500</td>\n",
       "      <td>1.918054</td>\n",
       "      <td>0.0356</td>\n",
       "      <td>0.4037</td>\n",
       "      <td>6.4901</td>\n",
       "      <td>0.9907</td>\n",
       "      <td>6.1572</td>\n",
       "      <td>1.6486</td>\n",
       "      <td>0.5368</td>\n",
       "      <td>68.7063</td>\n",
       "      <td>1.1830</td>\n",
       "      <td>2.3277</td>\n",
       "      <td>62.5157</td>\n",
       "      <td>1.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.924900</td>\n",
       "      <td>1.917004</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.3917</td>\n",
       "      <td>7.7259</td>\n",
       "      <td>1.1379</td>\n",
       "      <td>12.563</td>\n",
       "      <td>3.5478</td>\n",
       "      <td>1.8474</td>\n",
       "      <td>62.4259</td>\n",
       "      <td>1.1557</td>\n",
       "      <td>3.8256</td>\n",
       "      <td>57.0007</td>\n",
       "      <td>0.9985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.913700</td>\n",
       "      <td>1.917363</td>\n",
       "      <td>0.0479</td>\n",
       "      <td>0.4195</td>\n",
       "      <td>7.9762</td>\n",
       "      <td>1.2523</td>\n",
       "      <td>19.6586</td>\n",
       "      <td>2.4961</td>\n",
       "      <td>2.2702</td>\n",
       "      <td>62.1349</td>\n",
       "      <td>1.1067</td>\n",
       "      <td>3.7489</td>\n",
       "      <td>56.407</td>\n",
       "      <td>1.0142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.922800</td>\n",
       "      <td>1.916410</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>0.4299</td>\n",
       "      <td>6.9143</td>\n",
       "      <td>0.9821</td>\n",
       "      <td>8.5572</td>\n",
       "      <td>3.4932</td>\n",
       "      <td>0.7174</td>\n",
       "      <td>65.7413</td>\n",
       "      <td>1.0589</td>\n",
       "      <td>2.8136</td>\n",
       "      <td>61.5735</td>\n",
       "      <td>1.0243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.914500</td>\n",
       "      <td>1.915213</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>0.4975</td>\n",
       "      <td>8.1919</td>\n",
       "      <td>1.2466</td>\n",
       "      <td>20.0920</td>\n",
       "      <td>2.6832</td>\n",
       "      <td>1.7054</td>\n",
       "      <td>67.5879</td>\n",
       "      <td>1.2101</td>\n",
       "      <td>3.6060</td>\n",
       "      <td>58.0025</td>\n",
       "      <td>0.9916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.916800</td>\n",
       "      <td>1.913084</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>0.4748</td>\n",
       "      <td>7.1165</td>\n",
       "      <td>1.0241</td>\n",
       "      <td>9.9267</td>\n",
       "      <td>6.1163</td>\n",
       "      <td>1.1642</td>\n",
       "      <td>68.9207</td>\n",
       "      <td>1.2911</td>\n",
       "      <td>3.1006</td>\n",
       "      <td>60.3612</td>\n",
       "      <td>1.0510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.916100</td>\n",
       "      <td>1.915229</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.4069</td>\n",
       "      <td>7.6816</td>\n",
       "      <td>1.0365</td>\n",
       "      <td>7.6861</td>\n",
       "      <td>4.0110</td>\n",
       "      <td>1.2985</td>\n",
       "      <td>65.9841</td>\n",
       "      <td>1.1396</td>\n",
       "      <td>3.5564</td>\n",
       "      <td>59.0905</td>\n",
       "      <td>0.9644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.915800</td>\n",
       "      <td>1.913691</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>0.5383</td>\n",
       "      <td>7.6426</td>\n",
       "      <td>1.1226</td>\n",
       "      <td>11.7221</td>\n",
       "      <td>3.7347</td>\n",
       "      <td>1.4793</td>\n",
       "      <td>68.0059</td>\n",
       "      <td>1.3570</td>\n",
       "      <td>3.369</td>\n",
       "      <td>60.065</td>\n",
       "      <td>1.0534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.916900</td>\n",
       "      <td>1.914586</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.4738</td>\n",
       "      <td>7.8011</td>\n",
       "      <td>1.0944</td>\n",
       "      <td>10.928</td>\n",
       "      <td>3.8579</td>\n",
       "      <td>1.4107</td>\n",
       "      <td>68.3051</td>\n",
       "      <td>1.4007</td>\n",
       "      <td>3.2955</td>\n",
       "      <td>60.5632</td>\n",
       "      <td>1.0504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.914600</td>\n",
       "      <td>1.914276</td>\n",
       "      <td>0.0415</td>\n",
       "      <td>0.4645</td>\n",
       "      <td>7.6067</td>\n",
       "      <td>1.1290</td>\n",
       "      <td>12.9202</td>\n",
       "      <td>4.1589</td>\n",
       "      <td>1.5204</td>\n",
       "      <td>68.3629</td>\n",
       "      <td>1.3295</td>\n",
       "      <td>3.2113</td>\n",
       "      <td>59.9227</td>\n",
       "      <td>1.0537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.914900</td>\n",
       "      <td>1.913932</td>\n",
       "      <td>0.0451</td>\n",
       "      <td>0.446</td>\n",
       "      <td>7.9772</td>\n",
       "      <td>1.1994</td>\n",
       "      <td>17.1694</td>\n",
       "      <td>3.4237</td>\n",
       "      <td>1.7589</td>\n",
       "      <td>67.7752</td>\n",
       "      <td>1.2193</td>\n",
       "      <td>3.4664</td>\n",
       "      <td>59.328</td>\n",
       "      <td>1.0665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.919700</td>\n",
       "      <td>1.914064</td>\n",
       "      <td>0.0402</td>\n",
       "      <td>0.3904</td>\n",
       "      <td>7.9743</td>\n",
       "      <td>1.1603</td>\n",
       "      <td>14.9982</td>\n",
       "      <td>3.7071</td>\n",
       "      <td>1.7194</td>\n",
       "      <td>67.9745</td>\n",
       "      <td>1.2337</td>\n",
       "      <td>3.4850</td>\n",
       "      <td>59.2736</td>\n",
       "      <td>1.0622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>1.915300</td>\n",
       "      <td>1.913892</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.4138</td>\n",
       "      <td>7.9307</td>\n",
       "      <td>1.157</td>\n",
       "      <td>14.3926</td>\n",
       "      <td>3.7559</td>\n",
       "      <td>1.726</td>\n",
       "      <td>67.8185</td>\n",
       "      <td>1.2266</td>\n",
       "      <td>3.5064</td>\n",
       "      <td>59.4207</td>\n",
       "      <td>1.0668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>1.922300</td>\n",
       "      <td>1.913925</td>\n",
       "      <td>0.0402</td>\n",
       "      <td>0.3937</td>\n",
       "      <td>7.9087</td>\n",
       "      <td>1.1397</td>\n",
       "      <td>13.3394</td>\n",
       "      <td>3.8388</td>\n",
       "      <td>1.6849</td>\n",
       "      <td>67.8198</td>\n",
       "      <td>1.2368</td>\n",
       "      <td>3.4492</td>\n",
       "      <td>59.4939</td>\n",
       "      <td>1.0665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>1.911800</td>\n",
       "      <td>1.913912</td>\n",
       "      <td>0.0403</td>\n",
       "      <td>0.3952</td>\n",
       "      <td>7.8951</td>\n",
       "      <td>1.1387</td>\n",
       "      <td>13.2131</td>\n",
       "      <td>3.8508</td>\n",
       "      <td>1.6751</td>\n",
       "      <td>67.8431</td>\n",
       "      <td>1.2394</td>\n",
       "      <td>3.437</td>\n",
       "      <td>59.5322</td>\n",
       "      <td>1.068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8804, training_loss=1.9246718476220512, metrics={'train_runtime': 3114.176, 'train_samples_per_second': 11.307, 'train_steps_per_second': 2.827, 'total_flos': 0.0, 'train_loss': 1.9246718476220512, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 1e-3, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# .1 lq loss loss with conditioned kelly betting\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8804' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8804/8804 45:24, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.593200</td>\n",
       "      <td>1.565462</td>\n",
       "      <td>-0.0052</td>\n",
       "      <td>-0.0160</td>\n",
       "      <td>9.1773</td>\n",
       "      <td>0.3068</td>\n",
       "      <td>1.2576</td>\n",
       "      <td>0.6705</td>\n",
       "      <td>0.7517</td>\n",
       "      <td>14.8564</td>\n",
       "      <td>1.1294</td>\n",
       "      <td>5.5276</td>\n",
       "      <td>49.2492</td>\n",
       "      <td>1.0303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.574200</td>\n",
       "      <td>1.569934</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.2061</td>\n",
       "      <td>6.0315</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>41.8919</td>\n",
       "      <td>0.5927</td>\n",
       "      <td>0.0825</td>\n",
       "      <td>29.2035</td>\n",
       "      <td>1.1755</td>\n",
       "      <td>1.6097</td>\n",
       "      <td>51.7748</td>\n",
       "      <td>0.8715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.567600</td>\n",
       "      <td>1.566871</td>\n",
       "      <td>-0.0342</td>\n",
       "      <td>-0.0738</td>\n",
       "      <td>9.6669</td>\n",
       "      <td>0.9343</td>\n",
       "      <td>7.1540</td>\n",
       "      <td>1.0165</td>\n",
       "      <td>1.044</td>\n",
       "      <td>47.4730</td>\n",
       "      <td>1.0025</td>\n",
       "      <td>7.7179</td>\n",
       "      <td>49.6731</td>\n",
       "      <td>0.9605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.567400</td>\n",
       "      <td>1.561100</td>\n",
       "      <td>0.0556</td>\n",
       "      <td>0.3503</td>\n",
       "      <td>8.5947</td>\n",
       "      <td>0.778</td>\n",
       "      <td>1.0454</td>\n",
       "      <td>0.3387</td>\n",
       "      <td>0.3690</td>\n",
       "      <td>56.9652</td>\n",
       "      <td>1.0685</td>\n",
       "      <td>5.7391</td>\n",
       "      <td>54.6966</td>\n",
       "      <td>1.0848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.569100</td>\n",
       "      <td>1.562409</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>0.3519</td>\n",
       "      <td>9.6096</td>\n",
       "      <td>0.8412</td>\n",
       "      <td>2.5164</td>\n",
       "      <td>0.6122</td>\n",
       "      <td>0.3304</td>\n",
       "      <td>41.2433</td>\n",
       "      <td>0.8561</td>\n",
       "      <td>7.6397</td>\n",
       "      <td>55.9289</td>\n",
       "      <td>1.0411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.567000</td>\n",
       "      <td>1.562328</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>0.1453</td>\n",
       "      <td>11.1512</td>\n",
       "      <td>0.8804</td>\n",
       "      <td>5.5431</td>\n",
       "      <td>0.6172</td>\n",
       "      <td>1.3669</td>\n",
       "      <td>58.3721</td>\n",
       "      <td>0.9539</td>\n",
       "      <td>11.1031</td>\n",
       "      <td>53.5475</td>\n",
       "      <td>0.9098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.567200</td>\n",
       "      <td>1.559883</td>\n",
       "      <td>0.0361</td>\n",
       "      <td>0.1559</td>\n",
       "      <td>9.6481</td>\n",
       "      <td>0.8947</td>\n",
       "      <td>8.379</td>\n",
       "      <td>0.8981</td>\n",
       "      <td>0.8401</td>\n",
       "      <td>57.7138</td>\n",
       "      <td>0.9394</td>\n",
       "      <td>7.2403</td>\n",
       "      <td>52.858</td>\n",
       "      <td>0.9628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.558100</td>\n",
       "      <td>1.559668</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>8.2743</td>\n",
       "      <td>0.8225</td>\n",
       "      <td>4.285</td>\n",
       "      <td>0.8095</td>\n",
       "      <td>0.3677</td>\n",
       "      <td>51.3046</td>\n",
       "      <td>0.6944</td>\n",
       "      <td>4.5630</td>\n",
       "      <td>55.1976</td>\n",
       "      <td>0.9572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.556800</td>\n",
       "      <td>1.558698</td>\n",
       "      <td>0.0158</td>\n",
       "      <td>0.1673</td>\n",
       "      <td>7.5073</td>\n",
       "      <td>1.083</td>\n",
       "      <td>4.6312</td>\n",
       "      <td>0.7134</td>\n",
       "      <td>0.2636</td>\n",
       "      <td>60.9177</td>\n",
       "      <td>0.8475</td>\n",
       "      <td>4.1428</td>\n",
       "      <td>55.7499</td>\n",
       "      <td>0.8732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.558600</td>\n",
       "      <td>1.556506</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0736</td>\n",
       "      <td>7.6986</td>\n",
       "      <td>0.7639</td>\n",
       "      <td>8.6132</td>\n",
       "      <td>0.5088</td>\n",
       "      <td>0.585</td>\n",
       "      <td>52.7807</td>\n",
       "      <td>0.5932</td>\n",
       "      <td>3.4608</td>\n",
       "      <td>56.1464</td>\n",
       "      <td>0.8157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.562200</td>\n",
       "      <td>1.558700</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>8.7956</td>\n",
       "      <td>0.9311</td>\n",
       "      <td>2.8331</td>\n",
       "      <td>0.5695</td>\n",
       "      <td>0.3643</td>\n",
       "      <td>66.7144</td>\n",
       "      <td>0.702</td>\n",
       "      <td>5.078</td>\n",
       "      <td>54.9363</td>\n",
       "      <td>0.9233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.561200</td>\n",
       "      <td>1.558048</td>\n",
       "      <td>0.0234</td>\n",
       "      <td>0.1638</td>\n",
       "      <td>8.0263</td>\n",
       "      <td>0.9935</td>\n",
       "      <td>2.1096</td>\n",
       "      <td>0.7306</td>\n",
       "      <td>0.5620</td>\n",
       "      <td>64.5455</td>\n",
       "      <td>1.0038</td>\n",
       "      <td>3.9671</td>\n",
       "      <td>57.6618</td>\n",
       "      <td>0.8301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.558300</td>\n",
       "      <td>1.557901</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>8.1066</td>\n",
       "      <td>0.9612</td>\n",
       "      <td>4.8384</td>\n",
       "      <td>0.5498</td>\n",
       "      <td>0.9117</td>\n",
       "      <td>63.6093</td>\n",
       "      <td>0.9825</td>\n",
       "      <td>3.6573</td>\n",
       "      <td>57.2202</td>\n",
       "      <td>0.8900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.556200</td>\n",
       "      <td>1.556381</td>\n",
       "      <td>0.0389</td>\n",
       "      <td>0.2079</td>\n",
       "      <td>9.9291</td>\n",
       "      <td>1.0644</td>\n",
       "      <td>12.0984</td>\n",
       "      <td>0.6209</td>\n",
       "      <td>0.9226</td>\n",
       "      <td>65.6307</td>\n",
       "      <td>0.9179</td>\n",
       "      <td>7.2503</td>\n",
       "      <td>54.1714</td>\n",
       "      <td>0.9359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.560400</td>\n",
       "      <td>1.558210</td>\n",
       "      <td>0.0431</td>\n",
       "      <td>0.2277</td>\n",
       "      <td>8.8715</td>\n",
       "      <td>1.0786</td>\n",
       "      <td>7.3763</td>\n",
       "      <td>0.7948</td>\n",
       "      <td>0.5429</td>\n",
       "      <td>73.1707</td>\n",
       "      <td>0.8950</td>\n",
       "      <td>3.4901</td>\n",
       "      <td>56.6281</td>\n",
       "      <td>0.8737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.562200</td>\n",
       "      <td>1.554980</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.0436</td>\n",
       "      <td>9.1607</td>\n",
       "      <td>0.9506</td>\n",
       "      <td>2.6873</td>\n",
       "      <td>0.2457</td>\n",
       "      <td>0.8374</td>\n",
       "      <td>63.3919</td>\n",
       "      <td>1.1438</td>\n",
       "      <td>6.5499</td>\n",
       "      <td>54.7759</td>\n",
       "      <td>0.9188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.562800</td>\n",
       "      <td>1.561564</td>\n",
       "      <td>0.0251</td>\n",
       "      <td>0.1340</td>\n",
       "      <td>9.1807</td>\n",
       "      <td>1.0696</td>\n",
       "      <td>7.5551</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.7868</td>\n",
       "      <td>62.0064</td>\n",
       "      <td>1.1911</td>\n",
       "      <td>4.4416</td>\n",
       "      <td>56.6918</td>\n",
       "      <td>0.9745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.563500</td>\n",
       "      <td>1.554922</td>\n",
       "      <td>0.0379</td>\n",
       "      <td>0.4296</td>\n",
       "      <td>8.0356</td>\n",
       "      <td>0.9344</td>\n",
       "      <td>6.2828</td>\n",
       "      <td>1.458</td>\n",
       "      <td>0.9747</td>\n",
       "      <td>67.7043</td>\n",
       "      <td>1.3733</td>\n",
       "      <td>4.0867</td>\n",
       "      <td>57.4082</td>\n",
       "      <td>0.9537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.556900</td>\n",
       "      <td>1.554755</td>\n",
       "      <td>0.0412</td>\n",
       "      <td>0.3349</td>\n",
       "      <td>8.1927</td>\n",
       "      <td>0.9219</td>\n",
       "      <td>4.728</td>\n",
       "      <td>1.4393</td>\n",
       "      <td>0.7571</td>\n",
       "      <td>59.0828</td>\n",
       "      <td>1.243</td>\n",
       "      <td>3.7423</td>\n",
       "      <td>56.0574</td>\n",
       "      <td>0.9111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.557700</td>\n",
       "      <td>1.554722</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>0.2253</td>\n",
       "      <td>9.9873</td>\n",
       "      <td>1.3564</td>\n",
       "      <td>20.8026</td>\n",
       "      <td>1.6510</td>\n",
       "      <td>2.3028</td>\n",
       "      <td>63.2375</td>\n",
       "      <td>1.115</td>\n",
       "      <td>6.2263</td>\n",
       "      <td>54.5677</td>\n",
       "      <td>0.8872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.557400</td>\n",
       "      <td>1.553433</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.2545</td>\n",
       "      <td>10.6332</td>\n",
       "      <td>1.2449</td>\n",
       "      <td>18.7034</td>\n",
       "      <td>1.7913</td>\n",
       "      <td>2.4033</td>\n",
       "      <td>61.6816</td>\n",
       "      <td>1.1374</td>\n",
       "      <td>6.2386</td>\n",
       "      <td>54.6122</td>\n",
       "      <td>0.9062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.560000</td>\n",
       "      <td>1.550772</td>\n",
       "      <td>0.0568</td>\n",
       "      <td>0.4441</td>\n",
       "      <td>8.5424</td>\n",
       "      <td>1.1074</td>\n",
       "      <td>12.4765</td>\n",
       "      <td>1.1059</td>\n",
       "      <td>0.9249</td>\n",
       "      <td>69.6956</td>\n",
       "      <td>1.1941</td>\n",
       "      <td>4.894</td>\n",
       "      <td>57.6393</td>\n",
       "      <td>0.9235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.559000</td>\n",
       "      <td>1.553870</td>\n",
       "      <td>0.0538</td>\n",
       "      <td>0.3485</td>\n",
       "      <td>8.6300</td>\n",
       "      <td>1.1426</td>\n",
       "      <td>13.6430</td>\n",
       "      <td>2.5424</td>\n",
       "      <td>1.6076</td>\n",
       "      <td>66.0073</td>\n",
       "      <td>1.4203</td>\n",
       "      <td>4.1949</td>\n",
       "      <td>56.971</td>\n",
       "      <td>0.8883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.555200</td>\n",
       "      <td>1.554466</td>\n",
       "      <td>0.0340</td>\n",
       "      <td>0.2861</td>\n",
       "      <td>9.093</td>\n",
       "      <td>1.0658</td>\n",
       "      <td>5.8801</td>\n",
       "      <td>0.5287</td>\n",
       "      <td>1.2294</td>\n",
       "      <td>63.9355</td>\n",
       "      <td>1.1726</td>\n",
       "      <td>5.5032</td>\n",
       "      <td>55.9608</td>\n",
       "      <td>0.9269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.553000</td>\n",
       "      <td>1.553277</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.3749</td>\n",
       "      <td>9.4631</td>\n",
       "      <td>1.1652</td>\n",
       "      <td>10.7562</td>\n",
       "      <td>1.6386</td>\n",
       "      <td>1.6902</td>\n",
       "      <td>62.0605</td>\n",
       "      <td>1.0461</td>\n",
       "      <td>5.8174</td>\n",
       "      <td>56.8927</td>\n",
       "      <td>0.9735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.558200</td>\n",
       "      <td>1.554432</td>\n",
       "      <td>0.0415</td>\n",
       "      <td>0.3724</td>\n",
       "      <td>8.8839</td>\n",
       "      <td>1.0713</td>\n",
       "      <td>8.3706</td>\n",
       "      <td>2.4007</td>\n",
       "      <td>1.1229</td>\n",
       "      <td>66.9979</td>\n",
       "      <td>1.4174</td>\n",
       "      <td>4.8056</td>\n",
       "      <td>58.6205</td>\n",
       "      <td>0.9416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.551300</td>\n",
       "      <td>1.552399</td>\n",
       "      <td>0.0389</td>\n",
       "      <td>0.2838</td>\n",
       "      <td>8.322</td>\n",
       "      <td>1.0741</td>\n",
       "      <td>9.9602</td>\n",
       "      <td>0.7039</td>\n",
       "      <td>1.2083</td>\n",
       "      <td>66.2151</td>\n",
       "      <td>1.1417</td>\n",
       "      <td>4.4515</td>\n",
       "      <td>58.121</td>\n",
       "      <td>0.9343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.555700</td>\n",
       "      <td>1.551598</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>0.3563</td>\n",
       "      <td>8.9126</td>\n",
       "      <td>1.1476</td>\n",
       "      <td>13.2019</td>\n",
       "      <td>2.7615</td>\n",
       "      <td>2.0918</td>\n",
       "      <td>64.2889</td>\n",
       "      <td>1.2109</td>\n",
       "      <td>4.3270</td>\n",
       "      <td>57.1015</td>\n",
       "      <td>0.9360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.549500</td>\n",
       "      <td>1.551080</td>\n",
       "      <td>0.0504</td>\n",
       "      <td>0.3739</td>\n",
       "      <td>9.3166</td>\n",
       "      <td>1.4762</td>\n",
       "      <td>22.9851</td>\n",
       "      <td>1.6242</td>\n",
       "      <td>2.0152</td>\n",
       "      <td>63.3828</td>\n",
       "      <td>1.1089</td>\n",
       "      <td>5.0018</td>\n",
       "      <td>56.474</td>\n",
       "      <td>0.9757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.549600</td>\n",
       "      <td>1.552717</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>0.3808</td>\n",
       "      <td>8.1214</td>\n",
       "      <td>1.0363</td>\n",
       "      <td>7.5460</td>\n",
       "      <td>2.3109</td>\n",
       "      <td>0.8736</td>\n",
       "      <td>66.8895</td>\n",
       "      <td>1.1841</td>\n",
       "      <td>4.1806</td>\n",
       "      <td>59.3430</td>\n",
       "      <td>1.0134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.559200</td>\n",
       "      <td>1.551882</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.4077</td>\n",
       "      <td>9.4469</td>\n",
       "      <td>1.4002</td>\n",
       "      <td>22.5722</td>\n",
       "      <td>2.1589</td>\n",
       "      <td>2.3412</td>\n",
       "      <td>62.4193</td>\n",
       "      <td>1.0526</td>\n",
       "      <td>5.2541</td>\n",
       "      <td>55.3871</td>\n",
       "      <td>0.9674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.549800</td>\n",
       "      <td>1.552221</td>\n",
       "      <td>0.0632</td>\n",
       "      <td>0.4212</td>\n",
       "      <td>10.0809</td>\n",
       "      <td>1.7732</td>\n",
       "      <td>31.9828</td>\n",
       "      <td>1.6725</td>\n",
       "      <td>2.7333</td>\n",
       "      <td>61.5763</td>\n",
       "      <td>1.0838</td>\n",
       "      <td>5.4213</td>\n",
       "      <td>55.0546</td>\n",
       "      <td>1.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.557600</td>\n",
       "      <td>1.552052</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.4277</td>\n",
       "      <td>8.7620</td>\n",
       "      <td>1.0262</td>\n",
       "      <td>11.9996</td>\n",
       "      <td>1.4731</td>\n",
       "      <td>1.0674</td>\n",
       "      <td>64.2571</td>\n",
       "      <td>1.0777</td>\n",
       "      <td>5.1191</td>\n",
       "      <td>59.0092</td>\n",
       "      <td>1.0051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.550800</td>\n",
       "      <td>1.551077</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.4591</td>\n",
       "      <td>10.2654</td>\n",
       "      <td>1.4607</td>\n",
       "      <td>27.6751</td>\n",
       "      <td>2.3727</td>\n",
       "      <td>2.1092</td>\n",
       "      <td>64.1092</td>\n",
       "      <td>1.1208</td>\n",
       "      <td>6.1583</td>\n",
       "      <td>56.1504</td>\n",
       "      <td>0.9646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.552600</td>\n",
       "      <td>1.549328</td>\n",
       "      <td>0.0586</td>\n",
       "      <td>0.4671</td>\n",
       "      <td>9.1473</td>\n",
       "      <td>1.2245</td>\n",
       "      <td>18.7175</td>\n",
       "      <td>2.7558</td>\n",
       "      <td>1.8467</td>\n",
       "      <td>66.247</td>\n",
       "      <td>1.2251</td>\n",
       "      <td>4.5848</td>\n",
       "      <td>57.7780</td>\n",
       "      <td>1.0319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.552300</td>\n",
       "      <td>1.550724</td>\n",
       "      <td>0.0563</td>\n",
       "      <td>0.4101</td>\n",
       "      <td>9.7612</td>\n",
       "      <td>1.2656</td>\n",
       "      <td>17.3752</td>\n",
       "      <td>1.969</td>\n",
       "      <td>2.0185</td>\n",
       "      <td>63.7876</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>6.2425</td>\n",
       "      <td>57.3512</td>\n",
       "      <td>0.9811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.552000</td>\n",
       "      <td>1.549691</td>\n",
       "      <td>0.0626</td>\n",
       "      <td>0.5636</td>\n",
       "      <td>9.4542</td>\n",
       "      <td>1.3892</td>\n",
       "      <td>22.2848</td>\n",
       "      <td>2.3931</td>\n",
       "      <td>1.9041</td>\n",
       "      <td>66.0479</td>\n",
       "      <td>1.1841</td>\n",
       "      <td>5.5986</td>\n",
       "      <td>58.2068</td>\n",
       "      <td>1.0535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.552800</td>\n",
       "      <td>1.550466</td>\n",
       "      <td>0.0663</td>\n",
       "      <td>0.4449</td>\n",
       "      <td>9.8377</td>\n",
       "      <td>1.3172</td>\n",
       "      <td>20.2106</td>\n",
       "      <td>2.8019</td>\n",
       "      <td>1.8343</td>\n",
       "      <td>66.2005</td>\n",
       "      <td>1.1881</td>\n",
       "      <td>5.9566</td>\n",
       "      <td>58.1959</td>\n",
       "      <td>0.9893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.551100</td>\n",
       "      <td>1.550054</td>\n",
       "      <td>0.0529</td>\n",
       "      <td>0.4650</td>\n",
       "      <td>9.4998</td>\n",
       "      <td>1.3785</td>\n",
       "      <td>22.9274</td>\n",
       "      <td>2.6799</td>\n",
       "      <td>1.9798</td>\n",
       "      <td>66.1769</td>\n",
       "      <td>1.1771</td>\n",
       "      <td>5.3251</td>\n",
       "      <td>58.0517</td>\n",
       "      <td>1.0402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.551200</td>\n",
       "      <td>1.549879</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>9.9005</td>\n",
       "      <td>1.5192</td>\n",
       "      <td>28.0714</td>\n",
       "      <td>2.2547</td>\n",
       "      <td>2.2064</td>\n",
       "      <td>65.0095</td>\n",
       "      <td>1.1598</td>\n",
       "      <td>5.6044</td>\n",
       "      <td>57.8422</td>\n",
       "      <td>1.0252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.555300</td>\n",
       "      <td>1.549922</td>\n",
       "      <td>0.0520</td>\n",
       "      <td>0.4058</td>\n",
       "      <td>9.9325</td>\n",
       "      <td>1.4934</td>\n",
       "      <td>27.1191</td>\n",
       "      <td>2.3351</td>\n",
       "      <td>2.2296</td>\n",
       "      <td>65.2247</td>\n",
       "      <td>1.1668</td>\n",
       "      <td>5.6687</td>\n",
       "      <td>57.5830</td>\n",
       "      <td>1.0379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>1.551400</td>\n",
       "      <td>1.549743</td>\n",
       "      <td>0.0541</td>\n",
       "      <td>0.4261</td>\n",
       "      <td>9.8657</td>\n",
       "      <td>1.4573</td>\n",
       "      <td>25.7012</td>\n",
       "      <td>2.302</td>\n",
       "      <td>2.2222</td>\n",
       "      <td>65.4655</td>\n",
       "      <td>1.1548</td>\n",
       "      <td>5.7332</td>\n",
       "      <td>57.5713</td>\n",
       "      <td>1.0313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>1.557100</td>\n",
       "      <td>1.549741</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.4113</td>\n",
       "      <td>9.8556</td>\n",
       "      <td>1.4430</td>\n",
       "      <td>25.0668</td>\n",
       "      <td>2.3771</td>\n",
       "      <td>2.206</td>\n",
       "      <td>65.485</td>\n",
       "      <td>1.1564</td>\n",
       "      <td>5.668</td>\n",
       "      <td>57.653</td>\n",
       "      <td>1.0353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>1.548600</td>\n",
       "      <td>1.549726</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.4126</td>\n",
       "      <td>9.8320</td>\n",
       "      <td>1.4356</td>\n",
       "      <td>24.8039</td>\n",
       "      <td>2.4147</td>\n",
       "      <td>2.1948</td>\n",
       "      <td>65.4995</td>\n",
       "      <td>1.1552</td>\n",
       "      <td>5.6373</td>\n",
       "      <td>57.6872</td>\n",
       "      <td>1.0372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8804, training_loss=1.5584270546296572, metrics={'train_runtime': 2727.8991, 'train_samples_per_second': 12.908, 'train_steps_per_second': 3.227, 'total_flos': 0.0, 'train_loss': 1.5584270546296572, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 1e-3, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# .3 lq loss loss with conditioned kelly betting\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2673' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2673/8804 12:47 < 29:21, 3.48 it/s, Epoch 0.30/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.441000</td>\n",
       "      <td>1.417281</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>-0.0112</td>\n",
       "      <td>10.3643</td>\n",
       "      <td>0.2588</td>\n",
       "      <td>1.4504</td>\n",
       "      <td>0.9673</td>\n",
       "      <td>0.8915</td>\n",
       "      <td>17.4854</td>\n",
       "      <td>1.0701</td>\n",
       "      <td>9.0346</td>\n",
       "      <td>50.3295</td>\n",
       "      <td>0.9885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.424800</td>\n",
       "      <td>1.420345</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.1526</td>\n",
       "      <td>6.7478</td>\n",
       "      <td>0.0766</td>\n",
       "      <td>11.7007</td>\n",
       "      <td>0.8064</td>\n",
       "      <td>0.1904</td>\n",
       "      <td>28.4775</td>\n",
       "      <td>0.957</td>\n",
       "      <td>3.0918</td>\n",
       "      <td>52.3439</td>\n",
       "      <td>0.8745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.417800</td>\n",
       "      <td>1.416372</td>\n",
       "      <td>-0.0337</td>\n",
       "      <td>-0.0693</td>\n",
       "      <td>10.8385</td>\n",
       "      <td>0.9310</td>\n",
       "      <td>6.0589</td>\n",
       "      <td>1.0693</td>\n",
       "      <td>1.2318</td>\n",
       "      <td>46.7874</td>\n",
       "      <td>1.0441</td>\n",
       "      <td>10.3665</td>\n",
       "      <td>49.7933</td>\n",
       "      <td>0.9430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.418000</td>\n",
       "      <td>1.412284</td>\n",
       "      <td>0.0714</td>\n",
       "      <td>0.3298</td>\n",
       "      <td>10.8007</td>\n",
       "      <td>0.8581</td>\n",
       "      <td>3.4265</td>\n",
       "      <td>0.6053</td>\n",
       "      <td>0.9017</td>\n",
       "      <td>57.7359</td>\n",
       "      <td>1.0390</td>\n",
       "      <td>11.088</td>\n",
       "      <td>54.1528</td>\n",
       "      <td>1.0795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.419600</td>\n",
       "      <td>1.414965</td>\n",
       "      <td>0.0648</td>\n",
       "      <td>0.3546</td>\n",
       "      <td>10.4687</td>\n",
       "      <td>0.7711</td>\n",
       "      <td>2.6504</td>\n",
       "      <td>0.5927</td>\n",
       "      <td>0.534</td>\n",
       "      <td>44.2296</td>\n",
       "      <td>1.0361</td>\n",
       "      <td>10.2434</td>\n",
       "      <td>55.0602</td>\n",
       "      <td>1.0766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.418100</td>\n",
       "      <td>1.413418</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>0.1113</td>\n",
       "      <td>11.9532</td>\n",
       "      <td>0.9943</td>\n",
       "      <td>7.8125</td>\n",
       "      <td>0.7498</td>\n",
       "      <td>1.3606</td>\n",
       "      <td>52.9389</td>\n",
       "      <td>0.9567</td>\n",
       "      <td>13.2977</td>\n",
       "      <td>53.2722</td>\n",
       "      <td>0.9677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.418900</td>\n",
       "      <td>1.411924</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>0.1564</td>\n",
       "      <td>11.0028</td>\n",
       "      <td>0.8897</td>\n",
       "      <td>6.1291</td>\n",
       "      <td>1.2348</td>\n",
       "      <td>1.3616</td>\n",
       "      <td>52.6880</td>\n",
       "      <td>1.0069</td>\n",
       "      <td>10.1392</td>\n",
       "      <td>52.553</td>\n",
       "      <td>0.9374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.410500</td>\n",
       "      <td>1.411573</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>8.9506</td>\n",
       "      <td>0.9112</td>\n",
       "      <td>2.9752</td>\n",
       "      <td>1.2901</td>\n",
       "      <td>0.5596</td>\n",
       "      <td>57.8535</td>\n",
       "      <td>0.8451</td>\n",
       "      <td>6.0902</td>\n",
       "      <td>52.6118</td>\n",
       "      <td>1.0249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.409100</td>\n",
       "      <td>1.409694</td>\n",
       "      <td>0.0231</td>\n",
       "      <td>0.1909</td>\n",
       "      <td>8.4385</td>\n",
       "      <td>1.0662</td>\n",
       "      <td>4.313</td>\n",
       "      <td>0.7475</td>\n",
       "      <td>0.4424</td>\n",
       "      <td>63.0214</td>\n",
       "      <td>0.8395</td>\n",
       "      <td>5.7902</td>\n",
       "      <td>55.1792</td>\n",
       "      <td>0.9003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.404600</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-bb1c47517fd0>:15: RuntimeWarning: Mean of empty slice.\n",
      "  'full trade accuracy': (soft_profit[abs_trade >= .7] > 0).mean() * 100,\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:16: RuntimeWarning: Mean of empty slice.\n",
      "  'full trade g/l': soft_profit[(abs_trade >= .7) & (soft_profit > 0)].mean()\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:17: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade >= .7) & (soft_profit < 0)].mean(),\n",
      "<ipython-input-2-bb1c47517fd0>:20: RuntimeWarning: Mean of empty slice.\n",
      "  'medium trade accuracy': (soft_profit[(abs_trade < .7) & (abs_trade >= .4)] > 0).mean() * 100,\n",
      "<ipython-input-2-bb1c47517fd0>:21: RuntimeWarning: Mean of empty slice.\n",
      "  'medium trade g/l': soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit > 0)].mean()\n",
      "<ipython-input-2-bb1c47517fd0>:22: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit < 0)].mean(),\n",
      "<ipython-input-2-bb1c47517fd0>:25: RuntimeWarning: Mean of empty slice.\n",
      "  'small trade accuracy': (soft_profit[(abs_trade < .4) & (abs_trade >= .2)] > 0).mean() * 100,\n",
      "<ipython-input-2-bb1c47517fd0>:26: RuntimeWarning: Mean of empty slice.\n",
      "  'small trade g/l': soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit > 0)].mean()\n",
      "<ipython-input-2-bb1c47517fd0>:27: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit < 0)].mean(),\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-afa182bdb4f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# .4 lq loss loss with conditioned kelly betting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1747\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1748\u001b[0m             \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1749\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1751\u001b[0m                 \u001b[1;31m# Skip past any already trained steps if resuming training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    679\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# noqa: F811\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[1;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2356\u001b[1;33m         return self._getitem(\n\u001b[0m\u001b[0;32m   2357\u001b[0m             \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2358\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[1;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[0;32m   2339\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2340\u001b[0m         \u001b[0mpa_subtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2341\u001b[1;33m         formatted_output = format_table(\n\u001b[0m\u001b[0;32m   2342\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2343\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mformat_table\u001b[1;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[0mpa_table_to_format\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_names\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m         \u001b[0mformatted_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table_to_format\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mquery_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    518\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformatted_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMutableMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pa_table, query_type)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery_type\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mRowFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mColumnFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchFormat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"row\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 282\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mquery_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"column\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mformat_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPythonFormatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFormatter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mformat_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython_arrow_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m             \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython_features_decoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36mextract_row\u001b[1;34m(self, pa_table)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mPythonArrowExtractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseArrowExtractor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_row\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_pydict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\formatting\\formatting.py\u001b[0m in \u001b[0;36m_unnest\u001b[1;34m(py_dict)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0m_unnest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_dict\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;34m\"\"\"Return the first element of a batch (dict) as a row (dict)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpy_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 1e-3, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# .4 lq loss loss with conditioned kelly betting\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3158' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3158/8804 15:43 < 28:08, 3.34 it/s, Epoch 0.36/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.308900</td>\n",
       "      <td>1.288457</td>\n",
       "      <td>-0.0087</td>\n",
       "      <td>-0.0171</td>\n",
       "      <td>13.3484</td>\n",
       "      <td>1.1761</td>\n",
       "      <td>10.3467</td>\n",
       "      <td>0.8409</td>\n",
       "      <td>2.0290</td>\n",
       "      <td>43.9796</td>\n",
       "      <td>1.205</td>\n",
       "      <td>16.2586</td>\n",
       "      <td>50.3547</td>\n",
       "      <td>0.9370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.294200</td>\n",
       "      <td>1.286498</td>\n",
       "      <td>0.0493</td>\n",
       "      <td>0.2386</td>\n",
       "      <td>10.8332</td>\n",
       "      <td>0.6180</td>\n",
       "      <td>4.3192</td>\n",
       "      <td>1.1229</td>\n",
       "      <td>0.6069</td>\n",
       "      <td>27.8694</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>10.829</td>\n",
       "      <td>52.0375</td>\n",
       "      <td>1.0217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.287100</td>\n",
       "      <td>1.287766</td>\n",
       "      <td>-0.0428</td>\n",
       "      <td>-0.0843</td>\n",
       "      <td>12.9954</td>\n",
       "      <td>1.0505</td>\n",
       "      <td>8.2382</td>\n",
       "      <td>1.0663</td>\n",
       "      <td>2.114</td>\n",
       "      <td>48.3871</td>\n",
       "      <td>1.007</td>\n",
       "      <td>15.2826</td>\n",
       "      <td>50.0525</td>\n",
       "      <td>0.9077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.287500</td>\n",
       "      <td>1.282871</td>\n",
       "      <td>0.0849</td>\n",
       "      <td>0.3318</td>\n",
       "      <td>13.4155</td>\n",
       "      <td>0.9310</td>\n",
       "      <td>5.3981</td>\n",
       "      <td>0.6246</td>\n",
       "      <td>2.3811</td>\n",
       "      <td>52.9515</td>\n",
       "      <td>1.042</td>\n",
       "      <td>17.478</td>\n",
       "      <td>53.9932</td>\n",
       "      <td>1.0976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.289100</td>\n",
       "      <td>1.285466</td>\n",
       "      <td>0.0659</td>\n",
       "      <td>0.3415</td>\n",
       "      <td>12.6937</td>\n",
       "      <td>1.1654</td>\n",
       "      <td>6.9339</td>\n",
       "      <td>0.8039</td>\n",
       "      <td>1.3575</td>\n",
       "      <td>53.5218</td>\n",
       "      <td>0.9592</td>\n",
       "      <td>15.6362</td>\n",
       "      <td>54.2402</td>\n",
       "      <td>1.0352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.288100</td>\n",
       "      <td>1.284596</td>\n",
       "      <td>0.0345</td>\n",
       "      <td>0.1039</td>\n",
       "      <td>14.2645</td>\n",
       "      <td>0.9681</td>\n",
       "      <td>5.6549</td>\n",
       "      <td>0.7730</td>\n",
       "      <td>2.849</td>\n",
       "      <td>52.7724</td>\n",
       "      <td>0.9063</td>\n",
       "      <td>19.7845</td>\n",
       "      <td>52.6739</td>\n",
       "      <td>0.9602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.288300</td>\n",
       "      <td>1.282551</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>13.6302</td>\n",
       "      <td>1.083</td>\n",
       "      <td>14.4907</td>\n",
       "      <td>0.836</td>\n",
       "      <td>2.9373</td>\n",
       "      <td>54.8420</td>\n",
       "      <td>0.948</td>\n",
       "      <td>17.0302</td>\n",
       "      <td>51.9715</td>\n",
       "      <td>0.9146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.281300</td>\n",
       "      <td>1.282241</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0255</td>\n",
       "      <td>10.4031</td>\n",
       "      <td>1.0135</td>\n",
       "      <td>8.7346</td>\n",
       "      <td>0.8041</td>\n",
       "      <td>0.7257</td>\n",
       "      <td>55.7040</td>\n",
       "      <td>0.737</td>\n",
       "      <td>9.0625</td>\n",
       "      <td>53.3142</td>\n",
       "      <td>0.9812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.279700</td>\n",
       "      <td>1.281076</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.1271</td>\n",
       "      <td>10.6885</td>\n",
       "      <td>1.2152</td>\n",
       "      <td>9.7306</td>\n",
       "      <td>0.977</td>\n",
       "      <td>1.0922</td>\n",
       "      <td>60.0286</td>\n",
       "      <td>0.8684</td>\n",
       "      <td>10.2010</td>\n",
       "      <td>53.7156</td>\n",
       "      <td>0.8767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.276500</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-bb1c47517fd0>:15: RuntimeWarning: Mean of empty slice.\n",
      "  'full trade accuracy': (soft_profit[abs_trade >= .7] > 0).mean() * 100,\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:16: RuntimeWarning: Mean of empty slice.\n",
      "  'full trade g/l': soft_profit[(abs_trade >= .7) & (soft_profit > 0)].mean()\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:17: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade >= .7) & (soft_profit < 0)].mean(),\n",
      "<ipython-input-2-bb1c47517fd0>:20: RuntimeWarning: Mean of empty slice.\n",
      "  'medium trade accuracy': (soft_profit[(abs_trade < .7) & (abs_trade >= .4)] > 0).mean() * 100,\n",
      "<ipython-input-2-bb1c47517fd0>:21: RuntimeWarning: Mean of empty slice.\n",
      "  'medium trade g/l': soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit > 0)].mean()\n",
      "<ipython-input-2-bb1c47517fd0>:22: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade < .7) & (abs_trade >= .4) & (soft_profit < 0)].mean(),\n",
      "<ipython-input-2-bb1c47517fd0>:25: RuntimeWarning: Mean of empty slice.\n",
      "  'small trade accuracy': (soft_profit[(abs_trade < .4) & (abs_trade >= .2)] > 0).mean() * 100,\n",
      "<ipython-input-2-bb1c47517fd0>:26: RuntimeWarning: Mean of empty slice.\n",
      "  'small trade g/l': soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit > 0)].mean()\n",
      "<ipython-input-2-bb1c47517fd0>:27: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade < .4) & (abs_trade >= .2) & (soft_profit < 0)].mean(),\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-915bcbadc136>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# .5 lq loss loss with conditioned kelly betting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m         )\n\u001b[1;32m-> 1527\u001b[1;33m         return inner_training_loop(\n\u001b[0m\u001b[0;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1773\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1774\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1775\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1776\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1777\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2522\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2523\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2525\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2553\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2554\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2555\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2556\u001b[0m         \u001b[1;31m# Save past state if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2557\u001b[0m         \u001b[1;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\trader_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, ohlcv, labels, future, std_future)\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0membed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_embed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mohlcv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m             \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\trader_models.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, mod)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgconv_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_gconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmod\u001b[0m \u001b[1;31m# residual\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Trader\\gconv_standalone.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, u, return_kernel)\u001b[0m\n\u001b[0;32m    413\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;34m'cat'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_scales\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m                 kernel = F.interpolate(\n\u001b[0m\u001b[0;32m    416\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m                     \u001b[0mscale_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_scale\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "# SGCONV transformer architecture (fixed linear)! lr of 1e-3, batch size 5,\n",
    "# hidden size 320, 1 head, NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# .5 lq loss loss with conditioned kelly betting\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8804' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8804/8804 41:11, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.774600</td>\n",
       "      <td>1.743430</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0319</td>\n",
       "      <td>8.2202</td>\n",
       "      <td>0.2489</td>\n",
       "      <td>8.672</td>\n",
       "      <td>0.7134</td>\n",
       "      <td>0.8235</td>\n",
       "      <td>20.7774</td>\n",
       "      <td>1.0805</td>\n",
       "      <td>3.5094</td>\n",
       "      <td>46.9055</td>\n",
       "      <td>0.9799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.746000</td>\n",
       "      <td>1.741117</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.1031</td>\n",
       "      <td>5.2076</td>\n",
       "      <td>0.0652</td>\n",
       "      <td>15.2</td>\n",
       "      <td>0.6447</td>\n",
       "      <td>0.1940</td>\n",
       "      <td>13.3262</td>\n",
       "      <td>1.0342</td>\n",
       "      <td>1.0899</td>\n",
       "      <td>45.6998</td>\n",
       "      <td>0.9243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.737100</td>\n",
       "      <td>1.734195</td>\n",
       "      <td>-0.0156</td>\n",
       "      <td>-0.0456</td>\n",
       "      <td>7.2049</td>\n",
       "      <td>0.6348</td>\n",
       "      <td>4.6156</td>\n",
       "      <td>1.0041</td>\n",
       "      <td>0.6087</td>\n",
       "      <td>35.9027</td>\n",
       "      <td>0.983</td>\n",
       "      <td>3.4847</td>\n",
       "      <td>49.9192</td>\n",
       "      <td>0.9904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.737500</td>\n",
       "      <td>1.733007</td>\n",
       "      <td>0.0464</td>\n",
       "      <td>0.3144</td>\n",
       "      <td>7.8553</td>\n",
       "      <td>0.6790</td>\n",
       "      <td>0.5989</td>\n",
       "      <td>0.6585</td>\n",
       "      <td>0.1857</td>\n",
       "      <td>34.082</td>\n",
       "      <td>0.5955</td>\n",
       "      <td>3.8759</td>\n",
       "      <td>53.3238</td>\n",
       "      <td>1.0934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.738700</td>\n",
       "      <td>1.733746</td>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.3606</td>\n",
       "      <td>8.2033</td>\n",
       "      <td>0.5272</td>\n",
       "      <td>2.9074</td>\n",
       "      <td>0.4674</td>\n",
       "      <td>0.2982</td>\n",
       "      <td>30.1399</td>\n",
       "      <td>0.8653</td>\n",
       "      <td>4.5898</td>\n",
       "      <td>53.5554</td>\n",
       "      <td>0.9567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.737600</td>\n",
       "      <td>1.733765</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.1143</td>\n",
       "      <td>9.0699</td>\n",
       "      <td>0.4183</td>\n",
       "      <td>2.5922</td>\n",
       "      <td>0.5522</td>\n",
       "      <td>0.6169</td>\n",
       "      <td>36.5112</td>\n",
       "      <td>0.8279</td>\n",
       "      <td>7.2354</td>\n",
       "      <td>52.2049</td>\n",
       "      <td>0.9226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.736700</td>\n",
       "      <td>1.728844</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>0.1870</td>\n",
       "      <td>8.6041</td>\n",
       "      <td>0.7899</td>\n",
       "      <td>5.7162</td>\n",
       "      <td>0.9917</td>\n",
       "      <td>0.7011</td>\n",
       "      <td>55.8894</td>\n",
       "      <td>0.8718</td>\n",
       "      <td>5.9554</td>\n",
       "      <td>54.6056</td>\n",
       "      <td>0.9454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.727200</td>\n",
       "      <td>1.730293</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0585</td>\n",
       "      <td>7.2468</td>\n",
       "      <td>0.7787</td>\n",
       "      <td>2.1023</td>\n",
       "      <td>0.7182</td>\n",
       "      <td>0.3058</td>\n",
       "      <td>41.1183</td>\n",
       "      <td>0.7263</td>\n",
       "      <td>2.8371</td>\n",
       "      <td>55.3640</td>\n",
       "      <td>0.9787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.725700</td>\n",
       "      <td>1.726693</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>0.1912</td>\n",
       "      <td>6.5713</td>\n",
       "      <td>1.0101</td>\n",
       "      <td>2.9214</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.2721</td>\n",
       "      <td>68.3525</td>\n",
       "      <td>0.7659</td>\n",
       "      <td>2.4764</td>\n",
       "      <td>59.7389</td>\n",
       "      <td>0.8057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.727800</td>\n",
       "      <td>1.726651</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.0717</td>\n",
       "      <td>7.3569</td>\n",
       "      <td>0.9784</td>\n",
       "      <td>7.9825</td>\n",
       "      <td>0.6906</td>\n",
       "      <td>0.4265</td>\n",
       "      <td>60.2445</td>\n",
       "      <td>0.6300</td>\n",
       "      <td>2.9468</td>\n",
       "      <td>57.9314</td>\n",
       "      <td>0.8795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.732000</td>\n",
       "      <td>1.728154</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>7.6214</td>\n",
       "      <td>0.9448</td>\n",
       "      <td>2.9025</td>\n",
       "      <td>0.7505</td>\n",
       "      <td>0.3171</td>\n",
       "      <td>70.1743</td>\n",
       "      <td>0.7307</td>\n",
       "      <td>2.3031</td>\n",
       "      <td>58.2398</td>\n",
       "      <td>0.8651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.730600</td>\n",
       "      <td>1.726162</td>\n",
       "      <td>0.0282</td>\n",
       "      <td>0.2298</td>\n",
       "      <td>7.5961</td>\n",
       "      <td>1.0001</td>\n",
       "      <td>3.9933</td>\n",
       "      <td>0.4625</td>\n",
       "      <td>0.9836</td>\n",
       "      <td>66.3734</td>\n",
       "      <td>1.1284</td>\n",
       "      <td>3.4357</td>\n",
       "      <td>57.8634</td>\n",
       "      <td>0.8387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.727300</td>\n",
       "      <td>1.727023</td>\n",
       "      <td>0.0071</td>\n",
       "      <td>0.0492</td>\n",
       "      <td>6.8837</td>\n",
       "      <td>0.9199</td>\n",
       "      <td>4.3981</td>\n",
       "      <td>0.3611</td>\n",
       "      <td>0.6089</td>\n",
       "      <td>61.5925</td>\n",
       "      <td>0.9834</td>\n",
       "      <td>2.7661</td>\n",
       "      <td>57.8219</td>\n",
       "      <td>0.8888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.724800</td>\n",
       "      <td>1.727894</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>0.2094</td>\n",
       "      <td>9.3607</td>\n",
       "      <td>1.0107</td>\n",
       "      <td>11.4516</td>\n",
       "      <td>0.5751</td>\n",
       "      <td>1.0301</td>\n",
       "      <td>66.2010</td>\n",
       "      <td>1.0896</td>\n",
       "      <td>6.0098</td>\n",
       "      <td>54.1554</td>\n",
       "      <td>0.9635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.729700</td>\n",
       "      <td>1.725824</td>\n",
       "      <td>0.0343</td>\n",
       "      <td>0.2339</td>\n",
       "      <td>7.3131</td>\n",
       "      <td>1.0065</td>\n",
       "      <td>3.9884</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.3003</td>\n",
       "      <td>73.75</td>\n",
       "      <td>0.7857</td>\n",
       "      <td>1.6065</td>\n",
       "      <td>62.4327</td>\n",
       "      <td>0.8914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.731300</td>\n",
       "      <td>1.723138</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0419</td>\n",
       "      <td>8.6709</td>\n",
       "      <td>0.8821</td>\n",
       "      <td>0.6501</td>\n",
       "      <td>0.3557</td>\n",
       "      <td>0.7140</td>\n",
       "      <td>57.8709</td>\n",
       "      <td>1.1256</td>\n",
       "      <td>4.6702</td>\n",
       "      <td>57.7352</td>\n",
       "      <td>0.9648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.732200</td>\n",
       "      <td>1.729765</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>0.1213</td>\n",
       "      <td>8.6441</td>\n",
       "      <td>1.0360</td>\n",
       "      <td>6.2399</td>\n",
       "      <td>1.1464</td>\n",
       "      <td>0.7773</td>\n",
       "      <td>61.4487</td>\n",
       "      <td>1.2074</td>\n",
       "      <td>3.3194</td>\n",
       "      <td>56.2041</td>\n",
       "      <td>0.9455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.733100</td>\n",
       "      <td>1.724669</td>\n",
       "      <td>0.0311</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>6.8269</td>\n",
       "      <td>0.8968</td>\n",
       "      <td>4.7669</td>\n",
       "      <td>0.4084</td>\n",
       "      <td>0.5146</td>\n",
       "      <td>68.6727</td>\n",
       "      <td>1.2405</td>\n",
       "      <td>2.6639</td>\n",
       "      <td>60.1378</td>\n",
       "      <td>1.0366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.725700</td>\n",
       "      <td>1.724297</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.3214</td>\n",
       "      <td>7.3742</td>\n",
       "      <td>0.9151</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>0.7624</td>\n",
       "      <td>0.5653</td>\n",
       "      <td>61.9627</td>\n",
       "      <td>1.2624</td>\n",
       "      <td>2.8914</td>\n",
       "      <td>56.3758</td>\n",
       "      <td>0.9781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.726800</td>\n",
       "      <td>1.722992</td>\n",
       "      <td>0.0379</td>\n",
       "      <td>0.2487</td>\n",
       "      <td>8.9781</td>\n",
       "      <td>1.1191</td>\n",
       "      <td>13.7147</td>\n",
       "      <td>1.2139</td>\n",
       "      <td>1.9738</td>\n",
       "      <td>65.5203</td>\n",
       "      <td>1.2665</td>\n",
       "      <td>5.1431</td>\n",
       "      <td>56.1167</td>\n",
       "      <td>0.8866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.726700</td>\n",
       "      <td>1.723082</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>0.299</td>\n",
       "      <td>9.4819</td>\n",
       "      <td>1.1825</td>\n",
       "      <td>17.5734</td>\n",
       "      <td>1.5179</td>\n",
       "      <td>2.3282</td>\n",
       "      <td>60.7847</td>\n",
       "      <td>1.1693</td>\n",
       "      <td>4.7247</td>\n",
       "      <td>54.2616</td>\n",
       "      <td>0.8887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.729600</td>\n",
       "      <td>1.719373</td>\n",
       "      <td>0.0599</td>\n",
       "      <td>0.4822</td>\n",
       "      <td>7.7916</td>\n",
       "      <td>1.0325</td>\n",
       "      <td>11.5633</td>\n",
       "      <td>1.0259</td>\n",
       "      <td>0.7587</td>\n",
       "      <td>67.9632</td>\n",
       "      <td>1.2165</td>\n",
       "      <td>3.6927</td>\n",
       "      <td>59.3664</td>\n",
       "      <td>1.0119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.728300</td>\n",
       "      <td>1.723229</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>0.3617</td>\n",
       "      <td>7.0155</td>\n",
       "      <td>0.9663</td>\n",
       "      <td>9.2587</td>\n",
       "      <td>1.7575</td>\n",
       "      <td>1.2516</td>\n",
       "      <td>62.7093</td>\n",
       "      <td>1.4896</td>\n",
       "      <td>2.9910</td>\n",
       "      <td>58.6509</td>\n",
       "      <td>0.9953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.724200</td>\n",
       "      <td>1.723300</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>0.3053</td>\n",
       "      <td>8.1741</td>\n",
       "      <td>1.0239</td>\n",
       "      <td>6.1202</td>\n",
       "      <td>0.3976</td>\n",
       "      <td>1.1149</td>\n",
       "      <td>64.4875</td>\n",
       "      <td>1.2407</td>\n",
       "      <td>4.1554</td>\n",
       "      <td>57.3572</td>\n",
       "      <td>0.9475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.721500</td>\n",
       "      <td>1.721905</td>\n",
       "      <td>0.0492</td>\n",
       "      <td>0.4178</td>\n",
       "      <td>8.5119</td>\n",
       "      <td>1.0391</td>\n",
       "      <td>9.0115</td>\n",
       "      <td>1.0662</td>\n",
       "      <td>1.4989</td>\n",
       "      <td>63.1443</td>\n",
       "      <td>1.0711</td>\n",
       "      <td>4.4418</td>\n",
       "      <td>58.1962</td>\n",
       "      <td>0.9760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.727600</td>\n",
       "      <td>1.724501</td>\n",
       "      <td>0.0366</td>\n",
       "      <td>0.3261</td>\n",
       "      <td>8.1479</td>\n",
       "      <td>0.9324</td>\n",
       "      <td>7.4033</td>\n",
       "      <td>1.2655</td>\n",
       "      <td>1.0274</td>\n",
       "      <td>65.9292</td>\n",
       "      <td>1.3392</td>\n",
       "      <td>3.7676</td>\n",
       "      <td>60.0144</td>\n",
       "      <td>0.9752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.719500</td>\n",
       "      <td>1.722151</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>0.2716</td>\n",
       "      <td>7.4374</td>\n",
       "      <td>1.0138</td>\n",
       "      <td>6.4486</td>\n",
       "      <td>0.8740</td>\n",
       "      <td>0.9291</td>\n",
       "      <td>67.8788</td>\n",
       "      <td>1.1614</td>\n",
       "      <td>3.4122</td>\n",
       "      <td>59.6168</td>\n",
       "      <td>0.9540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.724600</td>\n",
       "      <td>1.720417</td>\n",
       "      <td>0.0427</td>\n",
       "      <td>0.3518</td>\n",
       "      <td>8.3021</td>\n",
       "      <td>1.0817</td>\n",
       "      <td>12.2325</td>\n",
       "      <td>3.0764</td>\n",
       "      <td>1.8687</td>\n",
       "      <td>63.6257</td>\n",
       "      <td>1.2369</td>\n",
       "      <td>3.8529</td>\n",
       "      <td>57.5059</td>\n",
       "      <td>0.981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.717700</td>\n",
       "      <td>1.720381</td>\n",
       "      <td>0.0411</td>\n",
       "      <td>0.3746</td>\n",
       "      <td>7.9497</td>\n",
       "      <td>1.1671</td>\n",
       "      <td>13.9909</td>\n",
       "      <td>1.4533</td>\n",
       "      <td>1.6265</td>\n",
       "      <td>63.2541</td>\n",
       "      <td>1.1881</td>\n",
       "      <td>3.6549</td>\n",
       "      <td>57.429</td>\n",
       "      <td>0.9796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.717900</td>\n",
       "      <td>1.720792</td>\n",
       "      <td>0.0426</td>\n",
       "      <td>0.4220</td>\n",
       "      <td>7.2086</td>\n",
       "      <td>0.9881</td>\n",
       "      <td>5.3820</td>\n",
       "      <td>1.6728</td>\n",
       "      <td>0.7027</td>\n",
       "      <td>69.0904</td>\n",
       "      <td>1.3168</td>\n",
       "      <td>3.0287</td>\n",
       "      <td>60.3904</td>\n",
       "      <td>1.0653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.728600</td>\n",
       "      <td>1.721511</td>\n",
       "      <td>0.0496</td>\n",
       "      <td>0.4178</td>\n",
       "      <td>8.4522</td>\n",
       "      <td>1.2461</td>\n",
       "      <td>17.4964</td>\n",
       "      <td>2.2012</td>\n",
       "      <td>2.0481</td>\n",
       "      <td>60.7728</td>\n",
       "      <td>1.1552</td>\n",
       "      <td>4.4586</td>\n",
       "      <td>55.6104</td>\n",
       "      <td>1.0058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.718500</td>\n",
       "      <td>1.721444</td>\n",
       "      <td>0.0561</td>\n",
       "      <td>0.4157</td>\n",
       "      <td>9.0394</td>\n",
       "      <td>1.4152</td>\n",
       "      <td>24.6095</td>\n",
       "      <td>1.8686</td>\n",
       "      <td>2.4726</td>\n",
       "      <td>61.1226</td>\n",
       "      <td>1.1124</td>\n",
       "      <td>4.7008</td>\n",
       "      <td>55.8902</td>\n",
       "      <td>1.0699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.726700</td>\n",
       "      <td>1.721674</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.4431</td>\n",
       "      <td>7.9008</td>\n",
       "      <td>0.9952</td>\n",
       "      <td>9.8910</td>\n",
       "      <td>1.0246</td>\n",
       "      <td>0.9775</td>\n",
       "      <td>64.5653</td>\n",
       "      <td>1.2379</td>\n",
       "      <td>3.8310</td>\n",
       "      <td>59.3969</td>\n",
       "      <td>0.9893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.719200</td>\n",
       "      <td>1.719811</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.4769</td>\n",
       "      <td>9.0478</td>\n",
       "      <td>1.2558</td>\n",
       "      <td>19.6446</td>\n",
       "      <td>1.7057</td>\n",
       "      <td>1.8638</td>\n",
       "      <td>65.7063</td>\n",
       "      <td>1.2809</td>\n",
       "      <td>4.3735</td>\n",
       "      <td>57.8843</td>\n",
       "      <td>1.0092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.721500</td>\n",
       "      <td>1.718663</td>\n",
       "      <td>0.0514</td>\n",
       "      <td>0.4232</td>\n",
       "      <td>8.0429</td>\n",
       "      <td>1.0823</td>\n",
       "      <td>12.4952</td>\n",
       "      <td>1.8147</td>\n",
       "      <td>1.533</td>\n",
       "      <td>65.3517</td>\n",
       "      <td>1.3542</td>\n",
       "      <td>3.5239</td>\n",
       "      <td>59.0099</td>\n",
       "      <td>1.0531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.720900</td>\n",
       "      <td>1.720051</td>\n",
       "      <td>0.0514</td>\n",
       "      <td>0.4278</td>\n",
       "      <td>8.5833</td>\n",
       "      <td>1.1</td>\n",
       "      <td>10.8541</td>\n",
       "      <td>1.8531</td>\n",
       "      <td>1.611</td>\n",
       "      <td>64.0971</td>\n",
       "      <td>1.1879</td>\n",
       "      <td>4.3270</td>\n",
       "      <td>58.9450</td>\n",
       "      <td>0.9848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.720700</td>\n",
       "      <td>1.718958</td>\n",
       "      <td>0.0532</td>\n",
       "      <td>0.5047</td>\n",
       "      <td>8.3610</td>\n",
       "      <td>1.145</td>\n",
       "      <td>13.2957</td>\n",
       "      <td>2.3767</td>\n",
       "      <td>1.5753</td>\n",
       "      <td>66.9579</td>\n",
       "      <td>1.3076</td>\n",
       "      <td>4.1047</td>\n",
       "      <td>59.7851</td>\n",
       "      <td>1.0421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.721700</td>\n",
       "      <td>1.720079</td>\n",
       "      <td>0.0573</td>\n",
       "      <td>0.4321</td>\n",
       "      <td>8.6068</td>\n",
       "      <td>1.1103</td>\n",
       "      <td>11.9553</td>\n",
       "      <td>2.8524</td>\n",
       "      <td>1.5159</td>\n",
       "      <td>66.1439</td>\n",
       "      <td>1.2848</td>\n",
       "      <td>4.204</td>\n",
       "      <td>59.8368</td>\n",
       "      <td>1.0231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.719700</td>\n",
       "      <td>1.719321</td>\n",
       "      <td>0.0465</td>\n",
       "      <td>0.4595</td>\n",
       "      <td>8.4317</td>\n",
       "      <td>1.1578</td>\n",
       "      <td>14.5083</td>\n",
       "      <td>2.2603</td>\n",
       "      <td>1.6801</td>\n",
       "      <td>67.0142</td>\n",
       "      <td>1.3188</td>\n",
       "      <td>4.0399</td>\n",
       "      <td>59.5137</td>\n",
       "      <td>1.0706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.720100</td>\n",
       "      <td>1.719178</td>\n",
       "      <td>0.0503</td>\n",
       "      <td>0.4308</td>\n",
       "      <td>8.7139</td>\n",
       "      <td>1.2498</td>\n",
       "      <td>19.3976</td>\n",
       "      <td>1.7891</td>\n",
       "      <td>1.8656</td>\n",
       "      <td>66.4207</td>\n",
       "      <td>1.2712</td>\n",
       "      <td>4.1313</td>\n",
       "      <td>58.9485</td>\n",
       "      <td>1.0464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.724500</td>\n",
       "      <td>1.719054</td>\n",
       "      <td>0.0459</td>\n",
       "      <td>0.3974</td>\n",
       "      <td>8.7294</td>\n",
       "      <td>1.2223</td>\n",
       "      <td>18.2904</td>\n",
       "      <td>1.7179</td>\n",
       "      <td>1.8627</td>\n",
       "      <td>66.8551</td>\n",
       "      <td>1.2871</td>\n",
       "      <td>4.2216</td>\n",
       "      <td>58.9547</td>\n",
       "      <td>1.0490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>1.720400</td>\n",
       "      <td>1.718951</td>\n",
       "      <td>0.0479</td>\n",
       "      <td>0.4166</td>\n",
       "      <td>8.6856</td>\n",
       "      <td>1.2181</td>\n",
       "      <td>17.5655</td>\n",
       "      <td>1.7905</td>\n",
       "      <td>1.8555</td>\n",
       "      <td>66.5693</td>\n",
       "      <td>1.2576</td>\n",
       "      <td>4.2731</td>\n",
       "      <td>59.1420</td>\n",
       "      <td>1.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>1.726500</td>\n",
       "      <td>1.719004</td>\n",
       "      <td>0.0466</td>\n",
       "      <td>0.4028</td>\n",
       "      <td>8.6781</td>\n",
       "      <td>1.2087</td>\n",
       "      <td>16.9772</td>\n",
       "      <td>1.8360</td>\n",
       "      <td>1.8451</td>\n",
       "      <td>66.6403</td>\n",
       "      <td>1.2638</td>\n",
       "      <td>4.2095</td>\n",
       "      <td>59.1415</td>\n",
       "      <td>1.0626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>1.717300</td>\n",
       "      <td>1.718987</td>\n",
       "      <td>0.0466</td>\n",
       "      <td>0.4045</td>\n",
       "      <td>8.6596</td>\n",
       "      <td>1.2057</td>\n",
       "      <td>16.7517</td>\n",
       "      <td>1.8375</td>\n",
       "      <td>1.8336</td>\n",
       "      <td>66.6875</td>\n",
       "      <td>1.2638</td>\n",
       "      <td>4.1919</td>\n",
       "      <td>59.2010</td>\n",
       "      <td>1.0654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8804, training_loss=1.7278540894856296, metrics={'train_runtime': 2474.8636, 'train_samples_per_second': 14.228, 'train_steps_per_second': 3.557, 'total_flos': 0.0, 'train_loss': 1.7278540894856296, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "\n",
    "# SGCONV transformer architecture! lr of 5e-4, batch size 5 hidden size 320, 1 head\n",
    "# NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# .2 lq loss loss with conditioned kelly betting\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8804' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8804/8804 43:31, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.771900</td>\n",
       "      <td>1.740882</td>\n",
       "      <td>-0.0022</td>\n",
       "      <td>-0.0079</td>\n",
       "      <td>8.6264</td>\n",
       "      <td>0.5304</td>\n",
       "      <td>3.0863</td>\n",
       "      <td>1.0414</td>\n",
       "      <td>0.6078</td>\n",
       "      <td>23.8978</td>\n",
       "      <td>1.0255</td>\n",
       "      <td>4.2838</td>\n",
       "      <td>49.456</td>\n",
       "      <td>1.0147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.746000</td>\n",
       "      <td>1.740744</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.145</td>\n",
       "      <td>5.2622</td>\n",
       "      <td>0.0638</td>\n",
       "      <td>9.9673</td>\n",
       "      <td>0.6842</td>\n",
       "      <td>0.2119</td>\n",
       "      <td>12.5</td>\n",
       "      <td>1.3398</td>\n",
       "      <td>0.8527</td>\n",
       "      <td>43.5803</td>\n",
       "      <td>0.8646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.737700</td>\n",
       "      <td>1.737146</td>\n",
       "      <td>-0.0300</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>8.5557</td>\n",
       "      <td>0.6738</td>\n",
       "      <td>5.1068</td>\n",
       "      <td>1.0455</td>\n",
       "      <td>0.7722</td>\n",
       "      <td>39.8731</td>\n",
       "      <td>0.9465</td>\n",
       "      <td>5.4536</td>\n",
       "      <td>48.8452</td>\n",
       "      <td>0.9918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.738000</td>\n",
       "      <td>1.732830</td>\n",
       "      <td>0.0486</td>\n",
       "      <td>0.339</td>\n",
       "      <td>7.7076</td>\n",
       "      <td>0.6866</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.4253</td>\n",
       "      <td>0.1887</td>\n",
       "      <td>45.0276</td>\n",
       "      <td>0.7292</td>\n",
       "      <td>3.1239</td>\n",
       "      <td>54.9017</td>\n",
       "      <td>1.123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.738900</td>\n",
       "      <td>1.731301</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.3798</td>\n",
       "      <td>8.3071</td>\n",
       "      <td>0.5527</td>\n",
       "      <td>2.6221</td>\n",
       "      <td>0.5230</td>\n",
       "      <td>0.2709</td>\n",
       "      <td>29.5227</td>\n",
       "      <td>0.7241</td>\n",
       "      <td>4.5714</td>\n",
       "      <td>54.3246</td>\n",
       "      <td>1.0201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.737500</td>\n",
       "      <td>1.734015</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.1203</td>\n",
       "      <td>9.7639</td>\n",
       "      <td>0.5622</td>\n",
       "      <td>7.4555</td>\n",
       "      <td>0.3009</td>\n",
       "      <td>0.4964</td>\n",
       "      <td>35.4337</td>\n",
       "      <td>0.755</td>\n",
       "      <td>8.2614</td>\n",
       "      <td>52.5533</td>\n",
       "      <td>0.9393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.737000</td>\n",
       "      <td>1.728971</td>\n",
       "      <td>0.0314</td>\n",
       "      <td>0.1735</td>\n",
       "      <td>8.6972</td>\n",
       "      <td>0.8286</td>\n",
       "      <td>8.3428</td>\n",
       "      <td>0.8351</td>\n",
       "      <td>0.5788</td>\n",
       "      <td>56.2962</td>\n",
       "      <td>0.8118</td>\n",
       "      <td>5.2933</td>\n",
       "      <td>54.1081</td>\n",
       "      <td>0.9387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.727400</td>\n",
       "      <td>1.732957</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>6.8011</td>\n",
       "      <td>0.5790</td>\n",
       "      <td>6.2309</td>\n",
       "      <td>0.7058</td>\n",
       "      <td>0.3005</td>\n",
       "      <td>33.449</td>\n",
       "      <td>0.6485</td>\n",
       "      <td>2.6050</td>\n",
       "      <td>55.7219</td>\n",
       "      <td>0.8956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.726000</td>\n",
       "      <td>1.727111</td>\n",
       "      <td>0.0113</td>\n",
       "      <td>0.1682</td>\n",
       "      <td>6.3725</td>\n",
       "      <td>1.0240</td>\n",
       "      <td>2.7798</td>\n",
       "      <td>0.5429</td>\n",
       "      <td>0.1927</td>\n",
       "      <td>66.342</td>\n",
       "      <td>1.0107</td>\n",
       "      <td>2.0395</td>\n",
       "      <td>59.3763</td>\n",
       "      <td>0.8091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.727900</td>\n",
       "      <td>1.726488</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0505</td>\n",
       "      <td>7.0975</td>\n",
       "      <td>0.9010</td>\n",
       "      <td>6.7932</td>\n",
       "      <td>0.6405</td>\n",
       "      <td>0.4030</td>\n",
       "      <td>50.8926</td>\n",
       "      <td>0.6253</td>\n",
       "      <td>1.7949</td>\n",
       "      <td>58.6267</td>\n",
       "      <td>0.7372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.731800</td>\n",
       "      <td>1.728158</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0459</td>\n",
       "      <td>7.9528</td>\n",
       "      <td>1.0032</td>\n",
       "      <td>3.8769</td>\n",
       "      <td>0.7498</td>\n",
       "      <td>0.4253</td>\n",
       "      <td>71.3655</td>\n",
       "      <td>0.8776</td>\n",
       "      <td>2.7224</td>\n",
       "      <td>59.1099</td>\n",
       "      <td>0.9054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.730500</td>\n",
       "      <td>1.725578</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>0.2802</td>\n",
       "      <td>7.3624</td>\n",
       "      <td>1.01</td>\n",
       "      <td>3.8612</td>\n",
       "      <td>0.3863</td>\n",
       "      <td>0.8398</td>\n",
       "      <td>68.8478</td>\n",
       "      <td>1.2550</td>\n",
       "      <td>3.4085</td>\n",
       "      <td>58.6528</td>\n",
       "      <td>0.8738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.727000</td>\n",
       "      <td>1.727841</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>7.2068</td>\n",
       "      <td>0.9527</td>\n",
       "      <td>5.0235</td>\n",
       "      <td>0.7439</td>\n",
       "      <td>0.9122</td>\n",
       "      <td>63.3402</td>\n",
       "      <td>1.1999</td>\n",
       "      <td>2.9763</td>\n",
       "      <td>56.8946</td>\n",
       "      <td>0.9011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.724700</td>\n",
       "      <td>1.728313</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>0.2356</td>\n",
       "      <td>8.7543</td>\n",
       "      <td>0.9896</td>\n",
       "      <td>9.9147</td>\n",
       "      <td>0.4849</td>\n",
       "      <td>0.9029</td>\n",
       "      <td>67.4327</td>\n",
       "      <td>1.1016</td>\n",
       "      <td>5.0592</td>\n",
       "      <td>54.5280</td>\n",
       "      <td>0.9845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.729800</td>\n",
       "      <td>1.726149</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>0.2331</td>\n",
       "      <td>7.2252</td>\n",
       "      <td>0.9833</td>\n",
       "      <td>2.3224</td>\n",
       "      <td>0.3741</td>\n",
       "      <td>0.2163</td>\n",
       "      <td>75.5063</td>\n",
       "      <td>0.6815</td>\n",
       "      <td>1.3345</td>\n",
       "      <td>63.1271</td>\n",
       "      <td>0.8150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.731300</td>\n",
       "      <td>1.723092</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>8.2795</td>\n",
       "      <td>0.9064</td>\n",
       "      <td>1.3114</td>\n",
       "      <td>0.1605</td>\n",
       "      <td>0.4153</td>\n",
       "      <td>59.0259</td>\n",
       "      <td>1.2883</td>\n",
       "      <td>3.9851</td>\n",
       "      <td>57.7618</td>\n",
       "      <td>0.8912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.732000</td>\n",
       "      <td>1.730078</td>\n",
       "      <td>0.0216</td>\n",
       "      <td>0.1392</td>\n",
       "      <td>8.9568</td>\n",
       "      <td>1.1235</td>\n",
       "      <td>10.4316</td>\n",
       "      <td>1.4987</td>\n",
       "      <td>0.8994</td>\n",
       "      <td>62.0102</td>\n",
       "      <td>1.3026</td>\n",
       "      <td>3.479</td>\n",
       "      <td>56.88</td>\n",
       "      <td>0.9557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.733100</td>\n",
       "      <td>1.724821</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.4271</td>\n",
       "      <td>6.7109</td>\n",
       "      <td>0.8772</td>\n",
       "      <td>4.1602</td>\n",
       "      <td>0.4867</td>\n",
       "      <td>0.6615</td>\n",
       "      <td>65.3058</td>\n",
       "      <td>1.6002</td>\n",
       "      <td>2.7446</td>\n",
       "      <td>60.3032</td>\n",
       "      <td>0.9777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.725900</td>\n",
       "      <td>1.724676</td>\n",
       "      <td>0.0330</td>\n",
       "      <td>0.3051</td>\n",
       "      <td>7.3402</td>\n",
       "      <td>0.9309</td>\n",
       "      <td>3.2706</td>\n",
       "      <td>0.6537</td>\n",
       "      <td>0.4294</td>\n",
       "      <td>63.7931</td>\n",
       "      <td>1.6146</td>\n",
       "      <td>2.8139</td>\n",
       "      <td>57.5150</td>\n",
       "      <td>0.9104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.726800</td>\n",
       "      <td>1.722609</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.2562</td>\n",
       "      <td>8.6825</td>\n",
       "      <td>1.1423</td>\n",
       "      <td>13.2816</td>\n",
       "      <td>1.3873</td>\n",
       "      <td>1.9596</td>\n",
       "      <td>65.9341</td>\n",
       "      <td>1.2282</td>\n",
       "      <td>5.0722</td>\n",
       "      <td>55.9308</td>\n",
       "      <td>0.8928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.726800</td>\n",
       "      <td>1.723070</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>0.2827</td>\n",
       "      <td>9.7023</td>\n",
       "      <td>1.1333</td>\n",
       "      <td>13.9111</td>\n",
       "      <td>1.6011</td>\n",
       "      <td>2.2232</td>\n",
       "      <td>61.8451</td>\n",
       "      <td>1.2078</td>\n",
       "      <td>4.9853</td>\n",
       "      <td>54.5439</td>\n",
       "      <td>0.8841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.729400</td>\n",
       "      <td>1.719200</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.493</td>\n",
       "      <td>7.72</td>\n",
       "      <td>1.0423</td>\n",
       "      <td>11.5546</td>\n",
       "      <td>0.9529</td>\n",
       "      <td>0.7757</td>\n",
       "      <td>68.4097</td>\n",
       "      <td>1.2397</td>\n",
       "      <td>3.6814</td>\n",
       "      <td>61.2021</td>\n",
       "      <td>0.9958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.728700</td>\n",
       "      <td>1.723474</td>\n",
       "      <td>0.0372</td>\n",
       "      <td>0.2931</td>\n",
       "      <td>6.9269</td>\n",
       "      <td>0.9612</td>\n",
       "      <td>6.2378</td>\n",
       "      <td>3.5712</td>\n",
       "      <td>0.8775</td>\n",
       "      <td>65.8032</td>\n",
       "      <td>1.5737</td>\n",
       "      <td>2.5238</td>\n",
       "      <td>59.8703</td>\n",
       "      <td>1.0167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.724200</td>\n",
       "      <td>1.722755</td>\n",
       "      <td>0.0283</td>\n",
       "      <td>0.281</td>\n",
       "      <td>8.0656</td>\n",
       "      <td>1.0444</td>\n",
       "      <td>6.3798</td>\n",
       "      <td>0.7732</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>67.6251</td>\n",
       "      <td>1.1495</td>\n",
       "      <td>4.1104</td>\n",
       "      <td>57.5368</td>\n",
       "      <td>0.8926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.721600</td>\n",
       "      <td>1.722023</td>\n",
       "      <td>0.0426</td>\n",
       "      <td>0.3643</td>\n",
       "      <td>8.2378</td>\n",
       "      <td>1.0419</td>\n",
       "      <td>6.4852</td>\n",
       "      <td>1.0594</td>\n",
       "      <td>1.3563</td>\n",
       "      <td>63.6657</td>\n",
       "      <td>1.0417</td>\n",
       "      <td>4.2425</td>\n",
       "      <td>58.3725</td>\n",
       "      <td>0.9637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.727600</td>\n",
       "      <td>1.724406</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>0.3637</td>\n",
       "      <td>7.6546</td>\n",
       "      <td>0.8578</td>\n",
       "      <td>4.3515</td>\n",
       "      <td>4.1788</td>\n",
       "      <td>0.8947</td>\n",
       "      <td>64.9458</td>\n",
       "      <td>1.3889</td>\n",
       "      <td>3.5254</td>\n",
       "      <td>61.6563</td>\n",
       "      <td>0.9696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.719600</td>\n",
       "      <td>1.722223</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>0.2648</td>\n",
       "      <td>7.3812</td>\n",
       "      <td>1.0089</td>\n",
       "      <td>6.0045</td>\n",
       "      <td>0.2642</td>\n",
       "      <td>0.9755</td>\n",
       "      <td>67.9209</td>\n",
       "      <td>1.1869</td>\n",
       "      <td>3.4578</td>\n",
       "      <td>60.7623</td>\n",
       "      <td>0.9944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.724600</td>\n",
       "      <td>1.720861</td>\n",
       "      <td>0.0414</td>\n",
       "      <td>0.3487</td>\n",
       "      <td>8.2255</td>\n",
       "      <td>1.0989</td>\n",
       "      <td>12.0505</td>\n",
       "      <td>3.2331</td>\n",
       "      <td>1.7269</td>\n",
       "      <td>64.6661</td>\n",
       "      <td>1.2287</td>\n",
       "      <td>3.7951</td>\n",
       "      <td>58.09</td>\n",
       "      <td>0.9515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.717900</td>\n",
       "      <td>1.720462</td>\n",
       "      <td>0.0398</td>\n",
       "      <td>0.3575</td>\n",
       "      <td>7.9198</td>\n",
       "      <td>1.1827</td>\n",
       "      <td>14.5552</td>\n",
       "      <td>2.0926</td>\n",
       "      <td>1.5813</td>\n",
       "      <td>64.306</td>\n",
       "      <td>1.2574</td>\n",
       "      <td>3.6134</td>\n",
       "      <td>58.0972</td>\n",
       "      <td>0.9305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.718000</td>\n",
       "      <td>1.721188</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>0.4141</td>\n",
       "      <td>7.3273</td>\n",
       "      <td>1.0347</td>\n",
       "      <td>7.3264</td>\n",
       "      <td>2.6910</td>\n",
       "      <td>0.8743</td>\n",
       "      <td>69.3977</td>\n",
       "      <td>1.327</td>\n",
       "      <td>3.1828</td>\n",
       "      <td>61.1879</td>\n",
       "      <td>0.9849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.728500</td>\n",
       "      <td>1.721281</td>\n",
       "      <td>0.0509</td>\n",
       "      <td>0.4089</td>\n",
       "      <td>8.2797</td>\n",
       "      <td>1.2539</td>\n",
       "      <td>18.0374</td>\n",
       "      <td>2.9429</td>\n",
       "      <td>1.9510</td>\n",
       "      <td>63.3104</td>\n",
       "      <td>1.2352</td>\n",
       "      <td>4.1304</td>\n",
       "      <td>56.7202</td>\n",
       "      <td>0.9703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.718500</td>\n",
       "      <td>1.721292</td>\n",
       "      <td>0.0544</td>\n",
       "      <td>0.4193</td>\n",
       "      <td>8.939</td>\n",
       "      <td>1.3913</td>\n",
       "      <td>23.7428</td>\n",
       "      <td>2.3275</td>\n",
       "      <td>2.3756</td>\n",
       "      <td>62.8978</td>\n",
       "      <td>1.1662</td>\n",
       "      <td>4.519</td>\n",
       "      <td>55.915</td>\n",
       "      <td>1.0355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.726700</td>\n",
       "      <td>1.721977</td>\n",
       "      <td>0.0419</td>\n",
       "      <td>0.4792</td>\n",
       "      <td>7.4328</td>\n",
       "      <td>0.9676</td>\n",
       "      <td>9.1056</td>\n",
       "      <td>1.4106</td>\n",
       "      <td>0.8977</td>\n",
       "      <td>65.8962</td>\n",
       "      <td>1.2338</td>\n",
       "      <td>3.4642</td>\n",
       "      <td>60.7441</td>\n",
       "      <td>1.0085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.719300</td>\n",
       "      <td>1.720031</td>\n",
       "      <td>0.0619</td>\n",
       "      <td>0.4961</td>\n",
       "      <td>9.0947</td>\n",
       "      <td>1.303</td>\n",
       "      <td>21.759</td>\n",
       "      <td>2.6056</td>\n",
       "      <td>1.9189</td>\n",
       "      <td>66.6848</td>\n",
       "      <td>1.2797</td>\n",
       "      <td>4.4023</td>\n",
       "      <td>57.847</td>\n",
       "      <td>1.0095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.721400</td>\n",
       "      <td>1.718532</td>\n",
       "      <td>0.0517</td>\n",
       "      <td>0.4792</td>\n",
       "      <td>7.9103</td>\n",
       "      <td>1.1041</td>\n",
       "      <td>13.1552</td>\n",
       "      <td>3.5587</td>\n",
       "      <td>1.5196</td>\n",
       "      <td>67.3254</td>\n",
       "      <td>1.3403</td>\n",
       "      <td>3.4157</td>\n",
       "      <td>60.0678</td>\n",
       "      <td>1.0305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.721100</td>\n",
       "      <td>1.720139</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.4519</td>\n",
       "      <td>8.3164</td>\n",
       "      <td>1.1143</td>\n",
       "      <td>11.7807</td>\n",
       "      <td>3.1829</td>\n",
       "      <td>1.5852</td>\n",
       "      <td>65.5529</td>\n",
       "      <td>1.1748</td>\n",
       "      <td>4.0508</td>\n",
       "      <td>59.2834</td>\n",
       "      <td>0.9449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.720900</td>\n",
       "      <td>1.719059</td>\n",
       "      <td>0.0532</td>\n",
       "      <td>0.5503</td>\n",
       "      <td>8.2298</td>\n",
       "      <td>1.2099</td>\n",
       "      <td>15.8321</td>\n",
       "      <td>3.2721</td>\n",
       "      <td>1.6226</td>\n",
       "      <td>67.1101</td>\n",
       "      <td>1.3403</td>\n",
       "      <td>3.9325</td>\n",
       "      <td>59.8717</td>\n",
       "      <td>0.9912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.721800</td>\n",
       "      <td>1.720227</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>0.4856</td>\n",
       "      <td>8.4192</td>\n",
       "      <td>1.1657</td>\n",
       "      <td>14.2576</td>\n",
       "      <td>3.4925</td>\n",
       "      <td>1.5446</td>\n",
       "      <td>66.8804</td>\n",
       "      <td>1.3184</td>\n",
       "      <td>3.9323</td>\n",
       "      <td>60.2169</td>\n",
       "      <td>1.0279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.719800</td>\n",
       "      <td>1.719658</td>\n",
       "      <td>0.0449</td>\n",
       "      <td>0.485</td>\n",
       "      <td>8.1342</td>\n",
       "      <td>1.186</td>\n",
       "      <td>15.7201</td>\n",
       "      <td>3.4062</td>\n",
       "      <td>1.6220</td>\n",
       "      <td>67.2795</td>\n",
       "      <td>1.347</td>\n",
       "      <td>3.6764</td>\n",
       "      <td>60.2445</td>\n",
       "      <td>1.0295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.720100</td>\n",
       "      <td>1.719432</td>\n",
       "      <td>0.0494</td>\n",
       "      <td>0.4755</td>\n",
       "      <td>8.5155</td>\n",
       "      <td>1.303</td>\n",
       "      <td>21.3348</td>\n",
       "      <td>2.5530</td>\n",
       "      <td>1.8672</td>\n",
       "      <td>66.6890</td>\n",
       "      <td>1.2985</td>\n",
       "      <td>3.8429</td>\n",
       "      <td>59.4899</td>\n",
       "      <td>1.0246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.724500</td>\n",
       "      <td>1.719358</td>\n",
       "      <td>0.0451</td>\n",
       "      <td>0.4239</td>\n",
       "      <td>8.5508</td>\n",
       "      <td>1.2699</td>\n",
       "      <td>20.092</td>\n",
       "      <td>2.6372</td>\n",
       "      <td>1.8757</td>\n",
       "      <td>67.1410</td>\n",
       "      <td>1.3049</td>\n",
       "      <td>3.9416</td>\n",
       "      <td>59.3884</td>\n",
       "      <td>0.9997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>1.720600</td>\n",
       "      <td>1.719229</td>\n",
       "      <td>0.0472</td>\n",
       "      <td>0.4515</td>\n",
       "      <td>8.5252</td>\n",
       "      <td>1.2656</td>\n",
       "      <td>19.4266</td>\n",
       "      <td>2.7518</td>\n",
       "      <td>1.8709</td>\n",
       "      <td>67.0847</td>\n",
       "      <td>1.2888</td>\n",
       "      <td>3.9725</td>\n",
       "      <td>59.4493</td>\n",
       "      <td>1.0096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>1.726500</td>\n",
       "      <td>1.719276</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>0.4332</td>\n",
       "      <td>8.5058</td>\n",
       "      <td>1.2505</td>\n",
       "      <td>18.5775</td>\n",
       "      <td>2.7912</td>\n",
       "      <td>1.8472</td>\n",
       "      <td>67.1973</td>\n",
       "      <td>1.2890</td>\n",
       "      <td>3.9425</td>\n",
       "      <td>59.4419</td>\n",
       "      <td>1.0101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>1.717200</td>\n",
       "      <td>1.719259</td>\n",
       "      <td>0.0459</td>\n",
       "      <td>0.4350</td>\n",
       "      <td>8.4919</td>\n",
       "      <td>1.2482</td>\n",
       "      <td>18.4112</td>\n",
       "      <td>2.7847</td>\n",
       "      <td>1.8376</td>\n",
       "      <td>67.1622</td>\n",
       "      <td>1.2905</td>\n",
       "      <td>3.9286</td>\n",
       "      <td>59.5244</td>\n",
       "      <td>1.0118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8804, training_loss=1.7278660092230334, metrics={'train_runtime': 2614.5076, 'train_samples_per_second': 13.468, 'train_steps_per_second': 3.367, 'total_flos': 0.0, 'train_loss': 1.7278660092230334, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "\n",
    "# SGCONV transformer architecture! lr of 1e-3, batch size 5 hidden size 512, 4 heads\n",
    "# NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# .2 lq loss loss with conditioned kelly betting\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8804' max='8804' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8804/8804 43:30, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.228700</td>\n",
       "      <td>2.184106</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>6.8137</td>\n",
       "      <td>0.0311</td>\n",
       "      <td>10.4027</td>\n",
       "      <td>0.3824</td>\n",
       "      <td>0.3919</td>\n",
       "      <td>17.0037</td>\n",
       "      <td>1.1345</td>\n",
       "      <td>1.6726</td>\n",
       "      <td>39.3055</td>\n",
       "      <td>0.9694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.188100</td>\n",
       "      <td>2.182916</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.2016</td>\n",
       "      <td>4.3631</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>37.5</td>\n",
       "      <td>0.4863</td>\n",
       "      <td>0.0435</td>\n",
       "      <td>29.7362</td>\n",
       "      <td>1.0239</td>\n",
       "      <td>0.2938</td>\n",
       "      <td>34.4925</td>\n",
       "      <td>1.0359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.181200</td>\n",
       "      <td>2.175694</td>\n",
       "      <td>-0.0174</td>\n",
       "      <td>-0.0555</td>\n",
       "      <td>6.3118</td>\n",
       "      <td>0.5153</td>\n",
       "      <td>5.686</td>\n",
       "      <td>0.9326</td>\n",
       "      <td>0.5498</td>\n",
       "      <td>24.0470</td>\n",
       "      <td>0.9418</td>\n",
       "      <td>2.3485</td>\n",
       "      <td>49.2519</td>\n",
       "      <td>1.0864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.180100</td>\n",
       "      <td>2.171566</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.2740</td>\n",
       "      <td>5.8136</td>\n",
       "      <td>0.4061</td>\n",
       "      <td>0.2567</td>\n",
       "      <td>0.2682</td>\n",
       "      <td>0.187</td>\n",
       "      <td>3.1233</td>\n",
       "      <td>1.047</td>\n",
       "      <td>1.3893</td>\n",
       "      <td>50.9156</td>\n",
       "      <td>1.1088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.179200</td>\n",
       "      <td>2.171188</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>0.3329</td>\n",
       "      <td>6.7033</td>\n",
       "      <td>0.6491</td>\n",
       "      <td>0.5141</td>\n",
       "      <td>0.4161</td>\n",
       "      <td>0.2133</td>\n",
       "      <td>7.6735</td>\n",
       "      <td>1.3133</td>\n",
       "      <td>1.5694</td>\n",
       "      <td>51.9235</td>\n",
       "      <td>0.9585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.179800</td>\n",
       "      <td>2.181060</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.1331</td>\n",
       "      <td>6.9636</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>55.3846</td>\n",
       "      <td>0.2249</td>\n",
       "      <td>0.1338</td>\n",
       "      <td>66.6407</td>\n",
       "      <td>0.7535</td>\n",
       "      <td>3.5005</td>\n",
       "      <td>52.459</td>\n",
       "      <td>0.9030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.178300</td>\n",
       "      <td>2.169340</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.1307</td>\n",
       "      <td>6.6273</td>\n",
       "      <td>0.1186</td>\n",
       "      <td>3.0783</td>\n",
       "      <td>1.6927</td>\n",
       "      <td>0.2345</td>\n",
       "      <td>21.8764</td>\n",
       "      <td>0.9170</td>\n",
       "      <td>2.5368</td>\n",
       "      <td>52.2463</td>\n",
       "      <td>0.9278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.168000</td>\n",
       "      <td>2.169051</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0388</td>\n",
       "      <td>5.6617</td>\n",
       "      <td>0.5421</td>\n",
       "      <td>0.0962</td>\n",
       "      <td>0.3222</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>10.1338</td>\n",
       "      <td>0.5150</td>\n",
       "      <td>1.3033</td>\n",
       "      <td>54.4924</td>\n",
       "      <td>0.9981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.164200</td>\n",
       "      <td>2.165759</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.1157</td>\n",
       "      <td>4.8945</td>\n",
       "      <td>0.9233</td>\n",
       "      <td>1.2874</td>\n",
       "      <td>0.2826</td>\n",
       "      <td>0.1107</td>\n",
       "      <td>39.9247</td>\n",
       "      <td>0.6778</td>\n",
       "      <td>0.5312</td>\n",
       "      <td>61.6019</td>\n",
       "      <td>0.7903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.165400</td>\n",
       "      <td>2.164024</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0534</td>\n",
       "      <td>6.6606</td>\n",
       "      <td>0.7025</td>\n",
       "      <td>3.5179</td>\n",
       "      <td>0.6966</td>\n",
       "      <td>0.4210</td>\n",
       "      <td>38.9054</td>\n",
       "      <td>0.6059</td>\n",
       "      <td>2.5851</td>\n",
       "      <td>54.6063</td>\n",
       "      <td>0.8835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.171000</td>\n",
       "      <td>2.164007</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>6.172</td>\n",
       "      <td>0.8878</td>\n",
       "      <td>1.3272</td>\n",
       "      <td>0.473</td>\n",
       "      <td>0.2768</td>\n",
       "      <td>66.0640</td>\n",
       "      <td>0.8398</td>\n",
       "      <td>1.3942</td>\n",
       "      <td>63.3012</td>\n",
       "      <td>0.8028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.169900</td>\n",
       "      <td>2.162140</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.2440</td>\n",
       "      <td>6.5052</td>\n",
       "      <td>0.9653</td>\n",
       "      <td>1.6526</td>\n",
       "      <td>0.3290</td>\n",
       "      <td>0.59</td>\n",
       "      <td>70.3782</td>\n",
       "      <td>1.2145</td>\n",
       "      <td>2.7115</td>\n",
       "      <td>59.9254</td>\n",
       "      <td>0.9236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.167100</td>\n",
       "      <td>2.164841</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>5.2535</td>\n",
       "      <td>0.8119</td>\n",
       "      <td>1.7724</td>\n",
       "      <td>4.6043</td>\n",
       "      <td>0.3017</td>\n",
       "      <td>56.7577</td>\n",
       "      <td>1.1934</td>\n",
       "      <td>1.7146</td>\n",
       "      <td>59.1644</td>\n",
       "      <td>0.9703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.161300</td>\n",
       "      <td>2.162941</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.2193</td>\n",
       "      <td>7.5356</td>\n",
       "      <td>0.8314</td>\n",
       "      <td>6.2712</td>\n",
       "      <td>0.3473</td>\n",
       "      <td>0.6344</td>\n",
       "      <td>65.2038</td>\n",
       "      <td>0.8883</td>\n",
       "      <td>3.5874</td>\n",
       "      <td>57.0731</td>\n",
       "      <td>0.9749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.167000</td>\n",
       "      <td>2.162160</td>\n",
       "      <td>0.0278</td>\n",
       "      <td>0.2101</td>\n",
       "      <td>6.289</td>\n",
       "      <td>0.9330</td>\n",
       "      <td>0.6817</td>\n",
       "      <td>1.0949</td>\n",
       "      <td>0.1688</td>\n",
       "      <td>64.1137</td>\n",
       "      <td>0.5496</td>\n",
       "      <td>0.7795</td>\n",
       "      <td>68.0444</td>\n",
       "      <td>0.7593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.168700</td>\n",
       "      <td>2.160807</td>\n",
       "      <td>-0.0013</td>\n",
       "      <td>-0.0109</td>\n",
       "      <td>6.4874</td>\n",
       "      <td>0.2394</td>\n",
       "      <td>0.0436</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.4486</td>\n",
       "      <td>33.1241</td>\n",
       "      <td>0.9611</td>\n",
       "      <td>2.3786</td>\n",
       "      <td>56.821</td>\n",
       "      <td>0.9646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.169100</td>\n",
       "      <td>2.168083</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0752</td>\n",
       "      <td>7.2056</td>\n",
       "      <td>0.9171</td>\n",
       "      <td>1.7055</td>\n",
       "      <td>2.904</td>\n",
       "      <td>0.4098</td>\n",
       "      <td>64.2494</td>\n",
       "      <td>1.5391</td>\n",
       "      <td>1.6232</td>\n",
       "      <td>57.9752</td>\n",
       "      <td>0.9785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.172000</td>\n",
       "      <td>2.160958</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>5.4637</td>\n",
       "      <td>0.7750</td>\n",
       "      <td>0.3632</td>\n",
       "      <td>0.4405</td>\n",
       "      <td>0.2720</td>\n",
       "      <td>54.8869</td>\n",
       "      <td>1.0581</td>\n",
       "      <td>1.643</td>\n",
       "      <td>62.9879</td>\n",
       "      <td>1.0392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.161100</td>\n",
       "      <td>2.159349</td>\n",
       "      <td>0.0242</td>\n",
       "      <td>0.3134</td>\n",
       "      <td>5.9145</td>\n",
       "      <td>0.9113</td>\n",
       "      <td>0.492</td>\n",
       "      <td>2.9888</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>64.7462</td>\n",
       "      <td>1.218</td>\n",
       "      <td>1.7925</td>\n",
       "      <td>57.4952</td>\n",
       "      <td>0.9914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.163600</td>\n",
       "      <td>2.158483</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>0.2224</td>\n",
       "      <td>7.3036</td>\n",
       "      <td>1.0028</td>\n",
       "      <td>10.1175</td>\n",
       "      <td>1.1082</td>\n",
       "      <td>1.7530</td>\n",
       "      <td>62.0212</td>\n",
       "      <td>1.0768</td>\n",
       "      <td>3.8083</td>\n",
       "      <td>56.7916</td>\n",
       "      <td>0.9232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.163200</td>\n",
       "      <td>2.158368</td>\n",
       "      <td>0.0342</td>\n",
       "      <td>0.2712</td>\n",
       "      <td>7.7958</td>\n",
       "      <td>0.7508</td>\n",
       "      <td>10.3194</td>\n",
       "      <td>2.3011</td>\n",
       "      <td>1.8821</td>\n",
       "      <td>53.9058</td>\n",
       "      <td>1.1653</td>\n",
       "      <td>3.6313</td>\n",
       "      <td>55.5562</td>\n",
       "      <td>0.9472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.166200</td>\n",
       "      <td>2.153903</td>\n",
       "      <td>0.0411</td>\n",
       "      <td>0.4028</td>\n",
       "      <td>6.1953</td>\n",
       "      <td>0.9136</td>\n",
       "      <td>4.5195</td>\n",
       "      <td>1.2173</td>\n",
       "      <td>0.4252</td>\n",
       "      <td>64.1736</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>2.0692</td>\n",
       "      <td>62.2405</td>\n",
       "      <td>1.0112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.165900</td>\n",
       "      <td>2.159046</td>\n",
       "      <td>0.0325</td>\n",
       "      <td>0.2892</td>\n",
       "      <td>5.6473</td>\n",
       "      <td>0.8636</td>\n",
       "      <td>3.5619</td>\n",
       "      <td>3.9168</td>\n",
       "      <td>0.7209</td>\n",
       "      <td>60.081</td>\n",
       "      <td>1.6100</td>\n",
       "      <td>2.1107</td>\n",
       "      <td>59.1493</td>\n",
       "      <td>1.0965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.160300</td>\n",
       "      <td>2.158291</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>0.2826</td>\n",
       "      <td>6.3961</td>\n",
       "      <td>0.9190</td>\n",
       "      <td>2.2918</td>\n",
       "      <td>0.6956</td>\n",
       "      <td>0.6221</td>\n",
       "      <td>62.3198</td>\n",
       "      <td>1.0211</td>\n",
       "      <td>2.4096</td>\n",
       "      <td>59.254</td>\n",
       "      <td>0.9461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.157000</td>\n",
       "      <td>2.157156</td>\n",
       "      <td>0.0392</td>\n",
       "      <td>0.3815</td>\n",
       "      <td>7.297</td>\n",
       "      <td>0.9515</td>\n",
       "      <td>2.4329</td>\n",
       "      <td>1.0202</td>\n",
       "      <td>1.1038</td>\n",
       "      <td>62.6866</td>\n",
       "      <td>1.0216</td>\n",
       "      <td>3.3738</td>\n",
       "      <td>59.071</td>\n",
       "      <td>0.9632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.164500</td>\n",
       "      <td>2.158952</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.3427</td>\n",
       "      <td>6.4026</td>\n",
       "      <td>0.7981</td>\n",
       "      <td>3.7889</td>\n",
       "      <td>2.0899</td>\n",
       "      <td>0.6272</td>\n",
       "      <td>65.7855</td>\n",
       "      <td>1.2431</td>\n",
       "      <td>2.4772</td>\n",
       "      <td>64.1158</td>\n",
       "      <td>1.0748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.154400</td>\n",
       "      <td>2.155676</td>\n",
       "      <td>0.0321</td>\n",
       "      <td>0.3011</td>\n",
       "      <td>6.0412</td>\n",
       "      <td>0.9202</td>\n",
       "      <td>1.8357</td>\n",
       "      <td>0.7893</td>\n",
       "      <td>0.4770</td>\n",
       "      <td>67.2568</td>\n",
       "      <td>1.2045</td>\n",
       "      <td>2.4122</td>\n",
       "      <td>63.7763</td>\n",
       "      <td>1.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.158300</td>\n",
       "      <td>2.154635</td>\n",
       "      <td>0.0334</td>\n",
       "      <td>0.3748</td>\n",
       "      <td>6.4684</td>\n",
       "      <td>0.9191</td>\n",
       "      <td>4.0953</td>\n",
       "      <td>4.8234</td>\n",
       "      <td>1.1191</td>\n",
       "      <td>65.9648</td>\n",
       "      <td>1.4912</td>\n",
       "      <td>2.9015</td>\n",
       "      <td>59.4459</td>\n",
       "      <td>0.9718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.153600</td>\n",
       "      <td>2.154628</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.37</td>\n",
       "      <td>6.3645</td>\n",
       "      <td>0.9392</td>\n",
       "      <td>8.0049</td>\n",
       "      <td>1.6047</td>\n",
       "      <td>1.3989</td>\n",
       "      <td>61.3074</td>\n",
       "      <td>1.3118</td>\n",
       "      <td>2.8197</td>\n",
       "      <td>58.6717</td>\n",
       "      <td>0.9664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.153400</td>\n",
       "      <td>2.155897</td>\n",
       "      <td>0.0335</td>\n",
       "      <td>0.4141</td>\n",
       "      <td>5.9818</td>\n",
       "      <td>0.9091</td>\n",
       "      <td>3.6128</td>\n",
       "      <td>1.5234</td>\n",
       "      <td>0.595</td>\n",
       "      <td>66.0533</td>\n",
       "      <td>1.2536</td>\n",
       "      <td>2.0857</td>\n",
       "      <td>62.7356</td>\n",
       "      <td>1.0664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>2.165500</td>\n",
       "      <td>2.156074</td>\n",
       "      <td>0.0342</td>\n",
       "      <td>0.3419</td>\n",
       "      <td>6.7623</td>\n",
       "      <td>1.0036</td>\n",
       "      <td>6.4935</td>\n",
       "      <td>3.9218</td>\n",
       "      <td>1.5116</td>\n",
       "      <td>61.937</td>\n",
       "      <td>1.2403</td>\n",
       "      <td>3.176</td>\n",
       "      <td>56.9323</td>\n",
       "      <td>0.9673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>2.154400</td>\n",
       "      <td>2.155564</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.4147</td>\n",
       "      <td>7.2035</td>\n",
       "      <td>1.0618</td>\n",
       "      <td>11.8236</td>\n",
       "      <td>2.9109</td>\n",
       "      <td>1.9975</td>\n",
       "      <td>61.9982</td>\n",
       "      <td>1.1676</td>\n",
       "      <td>3.3543</td>\n",
       "      <td>57.2601</td>\n",
       "      <td>1.0216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>2.160900</td>\n",
       "      <td>2.155174</td>\n",
       "      <td>0.0336</td>\n",
       "      <td>0.4590</td>\n",
       "      <td>6.2069</td>\n",
       "      <td>0.8868</td>\n",
       "      <td>5.8789</td>\n",
       "      <td>1.0671</td>\n",
       "      <td>0.6497</td>\n",
       "      <td>63.2643</td>\n",
       "      <td>1.2025</td>\n",
       "      <td>2.3705</td>\n",
       "      <td>61.9029</td>\n",
       "      <td>1.0111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>2.153900</td>\n",
       "      <td>2.153545</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.4853</td>\n",
       "      <td>7.3971</td>\n",
       "      <td>1.0869</td>\n",
       "      <td>12.3273</td>\n",
       "      <td>2.4732</td>\n",
       "      <td>1.4163</td>\n",
       "      <td>67.0618</td>\n",
       "      <td>1.3267</td>\n",
       "      <td>3.0564</td>\n",
       "      <td>60.4224</td>\n",
       "      <td>1.0685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.157600</td>\n",
       "      <td>2.151766</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.417</td>\n",
       "      <td>6.4240</td>\n",
       "      <td>0.951</td>\n",
       "      <td>6.6009</td>\n",
       "      <td>5.9526</td>\n",
       "      <td>1.1429</td>\n",
       "      <td>66.2531</td>\n",
       "      <td>1.3881</td>\n",
       "      <td>2.6985</td>\n",
       "      <td>61.2828</td>\n",
       "      <td>1.1037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>2.156900</td>\n",
       "      <td>2.153373</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.4194</td>\n",
       "      <td>6.7527</td>\n",
       "      <td>0.9401</td>\n",
       "      <td>5.2462</td>\n",
       "      <td>6.3312</td>\n",
       "      <td>1.0395</td>\n",
       "      <td>64.3896</td>\n",
       "      <td>1.2867</td>\n",
       "      <td>2.8325</td>\n",
       "      <td>61.0639</td>\n",
       "      <td>1.0092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>2.156800</td>\n",
       "      <td>2.152116</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.4921</td>\n",
       "      <td>6.7514</td>\n",
       "      <td>0.9961</td>\n",
       "      <td>6.1028</td>\n",
       "      <td>4.247</td>\n",
       "      <td>1.1624</td>\n",
       "      <td>67.3125</td>\n",
       "      <td>1.2171</td>\n",
       "      <td>2.7899</td>\n",
       "      <td>61.6086</td>\n",
       "      <td>1.0072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>2.156400</td>\n",
       "      <td>2.153754</td>\n",
       "      <td>0.0475</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>6.9999</td>\n",
       "      <td>0.9475</td>\n",
       "      <td>5.6564</td>\n",
       "      <td>4.9194</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>65.3083</td>\n",
       "      <td>1.3052</td>\n",
       "      <td>2.7568</td>\n",
       "      <td>61.2277</td>\n",
       "      <td>1.0256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>2.153800</td>\n",
       "      <td>2.152851</td>\n",
       "      <td>0.0370</td>\n",
       "      <td>0.4541</td>\n",
       "      <td>6.8109</td>\n",
       "      <td>1.0138</td>\n",
       "      <td>7.6828</td>\n",
       "      <td>4.6264</td>\n",
       "      <td>1.2851</td>\n",
       "      <td>66.8803</td>\n",
       "      <td>1.4234</td>\n",
       "      <td>2.7328</td>\n",
       "      <td>60.8188</td>\n",
       "      <td>1.0465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.154100</td>\n",
       "      <td>2.152435</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>0.4509</td>\n",
       "      <td>7.0512</td>\n",
       "      <td>1.0763</td>\n",
       "      <td>11.9550</td>\n",
       "      <td>3.4811</td>\n",
       "      <td>1.5093</td>\n",
       "      <td>66.9430</td>\n",
       "      <td>1.2983</td>\n",
       "      <td>2.8543</td>\n",
       "      <td>60.8059</td>\n",
       "      <td>1.0368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>2.158800</td>\n",
       "      <td>2.152527</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>7.0326</td>\n",
       "      <td>1.0322</td>\n",
       "      <td>10.4657</td>\n",
       "      <td>3.9345</td>\n",
       "      <td>1.4342</td>\n",
       "      <td>66.9138</td>\n",
       "      <td>1.2896</td>\n",
       "      <td>2.9161</td>\n",
       "      <td>61.0827</td>\n",
       "      <td>1.0324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>2.156700</td>\n",
       "      <td>2.152345</td>\n",
       "      <td>0.0379</td>\n",
       "      <td>0.4265</td>\n",
       "      <td>6.9733</td>\n",
       "      <td>1.0379</td>\n",
       "      <td>9.6645</td>\n",
       "      <td>4.2373</td>\n",
       "      <td>1.4198</td>\n",
       "      <td>67.1049</td>\n",
       "      <td>1.2959</td>\n",
       "      <td>2.9243</td>\n",
       "      <td>60.92</td>\n",
       "      <td>1.0355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>2.164000</td>\n",
       "      <td>2.152441</td>\n",
       "      <td>0.0366</td>\n",
       "      <td>0.4083</td>\n",
       "      <td>6.9505</td>\n",
       "      <td>1.0258</td>\n",
       "      <td>8.7314</td>\n",
       "      <td>4.3035</td>\n",
       "      <td>1.3958</td>\n",
       "      <td>66.8983</td>\n",
       "      <td>1.3179</td>\n",
       "      <td>2.893</td>\n",
       "      <td>61.0921</td>\n",
       "      <td>1.0343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>2.152400</td>\n",
       "      <td>2.152421</td>\n",
       "      <td>0.0366</td>\n",
       "      <td>0.4104</td>\n",
       "      <td>6.9382</td>\n",
       "      <td>1.0265</td>\n",
       "      <td>8.644</td>\n",
       "      <td>4.3045</td>\n",
       "      <td>1.3862</td>\n",
       "      <td>66.9701</td>\n",
       "      <td>1.3216</td>\n",
       "      <td>2.8857</td>\n",
       "      <td>61.0587</td>\n",
       "      <td>1.0341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8804, training_loss=2.1655000732574394, metrics={'train_runtime': 2617.1298, 'train_samples_per_second': 13.455, 'train_steps_per_second': 3.364, 'total_flos': 0.0, 'train_loss': 2.1655000732574394, 'epoch': 1.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oanda data ONLY MAJORS past 2009, overnight labels ignored\n",
    "\n",
    "# SGCONV transformer architecture! lr of 1e-3, batch size 5 hidden size 512, 4 heads\n",
    "# NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# ce loss with conditioned kelly betting\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17746' max='17746' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17746/17746 1:27:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.253900</td>\n",
       "      <td>2.188456</td>\n",
       "      <td>-0.0036</td>\n",
       "      <td>-0.0232</td>\n",
       "      <td>6.2597</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>14.5695</td>\n",
       "      <td>0.6227</td>\n",
       "      <td>0.5867</td>\n",
       "      <td>9.5610</td>\n",
       "      <td>1.0884</td>\n",
       "      <td>2.458</td>\n",
       "      <td>36.2449</td>\n",
       "      <td>1.0998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.203800</td>\n",
       "      <td>2.178138</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>5.6864</td>\n",
       "      <td>0.0872</td>\n",
       "      <td>0.1196</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.5105</td>\n",
       "      <td>2.594</td>\n",
       "      <td>1.1356</td>\n",
       "      <td>1.1123</td>\n",
       "      <td>42.9549</td>\n",
       "      <td>1.1416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.201300</td>\n",
       "      <td>2.179769</td>\n",
       "      <td>0.0234</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>5.9524</td>\n",
       "      <td>0.7283</td>\n",
       "      <td>0.5154</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.231</td>\n",
       "      <td>14.1761</td>\n",
       "      <td>1.4146</td>\n",
       "      <td>0.8273</td>\n",
       "      <td>45.0214</td>\n",
       "      <td>0.9949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.200600</td>\n",
       "      <td>2.198580</td>\n",
       "      <td>-0.0076</td>\n",
       "      <td>-0.0785</td>\n",
       "      <td>7.3671</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>3.6585</td>\n",
       "      <td>0.6763</td>\n",
       "      <td>0.2367</td>\n",
       "      <td>10.1762</td>\n",
       "      <td>0.7261</td>\n",
       "      <td>6.6103</td>\n",
       "      <td>44.472</td>\n",
       "      <td>1.0515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.188900</td>\n",
       "      <td>2.186725</td>\n",
       "      <td>0.0209</td>\n",
       "      <td>0.1637</td>\n",
       "      <td>6.7894</td>\n",
       "      <td>1.1486</td>\n",
       "      <td>4.7476</td>\n",
       "      <td>0.8963</td>\n",
       "      <td>0.2909</td>\n",
       "      <td>42.8674</td>\n",
       "      <td>0.9228</td>\n",
       "      <td>1.1649</td>\n",
       "      <td>55.8897</td>\n",
       "      <td>0.8131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.190600</td>\n",
       "      <td>2.187392</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0.1196</td>\n",
       "      <td>5.0382</td>\n",
       "      <td>0.8751</td>\n",
       "      <td>1.537</td>\n",
       "      <td>1.5506</td>\n",
       "      <td>0.1165</td>\n",
       "      <td>22.1128</td>\n",
       "      <td>1.1316</td>\n",
       "      <td>0.5391</td>\n",
       "      <td>54.236</td>\n",
       "      <td>0.8117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.191000</td>\n",
       "      <td>2.173209</td>\n",
       "      <td>-0.0120</td>\n",
       "      <td>-0.0820</td>\n",
       "      <td>6.8619</td>\n",
       "      <td>0.5578</td>\n",
       "      <td>6.9346</td>\n",
       "      <td>0.4355</td>\n",
       "      <td>0.5691</td>\n",
       "      <td>45.2364</td>\n",
       "      <td>0.6217</td>\n",
       "      <td>3.0424</td>\n",
       "      <td>53.3964</td>\n",
       "      <td>0.876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.197100</td>\n",
       "      <td>2.169390</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.1033</td>\n",
       "      <td>7.0964</td>\n",
       "      <td>0.9566</td>\n",
       "      <td>2.0057</td>\n",
       "      <td>0.9936</td>\n",
       "      <td>0.2105</td>\n",
       "      <td>47.0035</td>\n",
       "      <td>0.7102</td>\n",
       "      <td>2.9752</td>\n",
       "      <td>56.1700</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.183300</td>\n",
       "      <td>2.168299</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.0899</td>\n",
       "      <td>6.7017</td>\n",
       "      <td>0.9428</td>\n",
       "      <td>0.9401</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.2907</td>\n",
       "      <td>54.2324</td>\n",
       "      <td>0.7251</td>\n",
       "      <td>2.3046</td>\n",
       "      <td>54.2530</td>\n",
       "      <td>0.8996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.188000</td>\n",
       "      <td>2.167574</td>\n",
       "      <td>0.0108</td>\n",
       "      <td>0.1385</td>\n",
       "      <td>5.5074</td>\n",
       "      <td>1.0627</td>\n",
       "      <td>4.1797</td>\n",
       "      <td>1.4348</td>\n",
       "      <td>0.3262</td>\n",
       "      <td>60.1662</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>1.7722</td>\n",
       "      <td>59.5258</td>\n",
       "      <td>1.0790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.182800</td>\n",
       "      <td>2.173687</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.2203</td>\n",
       "      <td>6.0097</td>\n",
       "      <td>0.9089</td>\n",
       "      <td>1.0898</td>\n",
       "      <td>0.3195</td>\n",
       "      <td>0.1689</td>\n",
       "      <td>48.2716</td>\n",
       "      <td>0.9001</td>\n",
       "      <td>1.9905</td>\n",
       "      <td>53.164</td>\n",
       "      <td>0.9599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.186700</td>\n",
       "      <td>2.162575</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.1967</td>\n",
       "      <td>7.8951</td>\n",
       "      <td>0.9629</td>\n",
       "      <td>3.5084</td>\n",
       "      <td>1.4702</td>\n",
       "      <td>1.4831</td>\n",
       "      <td>56.707</td>\n",
       "      <td>1.0929</td>\n",
       "      <td>5.2670</td>\n",
       "      <td>53.4536</td>\n",
       "      <td>0.9285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.187800</td>\n",
       "      <td>2.168478</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0534</td>\n",
       "      <td>8.0864</td>\n",
       "      <td>1.0511</td>\n",
       "      <td>3.631</td>\n",
       "      <td>0.881</td>\n",
       "      <td>1.1421</td>\n",
       "      <td>48.2699</td>\n",
       "      <td>0.9185</td>\n",
       "      <td>4.2379</td>\n",
       "      <td>51.4209</td>\n",
       "      <td>0.8912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.182000</td>\n",
       "      <td>2.166930</td>\n",
       "      <td>0.0516</td>\n",
       "      <td>0.2063</td>\n",
       "      <td>7.6202</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.6402</td>\n",
       "      <td>1.2311</td>\n",
       "      <td>0.3112</td>\n",
       "      <td>57.2864</td>\n",
       "      <td>1.3844</td>\n",
       "      <td>2.9694</td>\n",
       "      <td>58.6558</td>\n",
       "      <td>0.8235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.184000</td>\n",
       "      <td>2.169067</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>0.3287</td>\n",
       "      <td>8.9819</td>\n",
       "      <td>1.1076</td>\n",
       "      <td>9.2544</td>\n",
       "      <td>0.8856</td>\n",
       "      <td>1.3346</td>\n",
       "      <td>60.6532</td>\n",
       "      <td>1.0121</td>\n",
       "      <td>5.5851</td>\n",
       "      <td>55.2956</td>\n",
       "      <td>0.8991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.177200</td>\n",
       "      <td>2.170250</td>\n",
       "      <td>0.0176</td>\n",
       "      <td>0.1711</td>\n",
       "      <td>7.0850</td>\n",
       "      <td>0.7824</td>\n",
       "      <td>1.1061</td>\n",
       "      <td>1.6948</td>\n",
       "      <td>1.2116</td>\n",
       "      <td>48.9157</td>\n",
       "      <td>1.2989</td>\n",
       "      <td>3.6111</td>\n",
       "      <td>51.6488</td>\n",
       "      <td>0.903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.179000</td>\n",
       "      <td>2.168007</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.1379</td>\n",
       "      <td>7.1143</td>\n",
       "      <td>0.9580</td>\n",
       "      <td>1.3061</td>\n",
       "      <td>0.3474</td>\n",
       "      <td>1.2002</td>\n",
       "      <td>59.9913</td>\n",
       "      <td>1.429</td>\n",
       "      <td>3.5354</td>\n",
       "      <td>54.8664</td>\n",
       "      <td>0.8607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.179900</td>\n",
       "      <td>2.166504</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.1109</td>\n",
       "      <td>6.4020</td>\n",
       "      <td>0.9759</td>\n",
       "      <td>2.2545</td>\n",
       "      <td>2.2363</td>\n",
       "      <td>0.4917</td>\n",
       "      <td>54.2833</td>\n",
       "      <td>1.1517</td>\n",
       "      <td>2.0467</td>\n",
       "      <td>54.0068</td>\n",
       "      <td>1.0301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.184200</td>\n",
       "      <td>2.167235</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>7.9248</td>\n",
       "      <td>0.8604</td>\n",
       "      <td>1.6723</td>\n",
       "      <td>1.1782</td>\n",
       "      <td>1.208</td>\n",
       "      <td>51.8774</td>\n",
       "      <td>1.1217</td>\n",
       "      <td>4.2381</td>\n",
       "      <td>52.4837</td>\n",
       "      <td>0.9553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.180700</td>\n",
       "      <td>2.170062</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.1279</td>\n",
       "      <td>8.1763</td>\n",
       "      <td>1.0969</td>\n",
       "      <td>3.1084</td>\n",
       "      <td>0.9027</td>\n",
       "      <td>1.3787</td>\n",
       "      <td>60.6640</td>\n",
       "      <td>1.1714</td>\n",
       "      <td>4.5478</td>\n",
       "      <td>53.7636</td>\n",
       "      <td>0.9493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.179000</td>\n",
       "      <td>2.167562</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0884</td>\n",
       "      <td>7.9370</td>\n",
       "      <td>0.9107</td>\n",
       "      <td>3.4005</td>\n",
       "      <td>0.8057</td>\n",
       "      <td>1.6911</td>\n",
       "      <td>55.8269</td>\n",
       "      <td>1.1294</td>\n",
       "      <td>4.221</td>\n",
       "      <td>54.1316</td>\n",
       "      <td>0.8598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.180400</td>\n",
       "      <td>2.169405</td>\n",
       "      <td>0.0142</td>\n",
       "      <td>0.1182</td>\n",
       "      <td>8.0546</td>\n",
       "      <td>0.8331</td>\n",
       "      <td>1.3141</td>\n",
       "      <td>1.7988</td>\n",
       "      <td>0.9800</td>\n",
       "      <td>53.7291</td>\n",
       "      <td>0.9610</td>\n",
       "      <td>5.1297</td>\n",
       "      <td>53.2767</td>\n",
       "      <td>0.8817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.180800</td>\n",
       "      <td>2.163085</td>\n",
       "      <td>0.0311</td>\n",
       "      <td>0.3819</td>\n",
       "      <td>6.8481</td>\n",
       "      <td>1.0577</td>\n",
       "      <td>6.6936</td>\n",
       "      <td>1.2854</td>\n",
       "      <td>1.4876</td>\n",
       "      <td>62.3817</td>\n",
       "      <td>1.3992</td>\n",
       "      <td>3.4398</td>\n",
       "      <td>57.6738</td>\n",
       "      <td>0.9833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.180800</td>\n",
       "      <td>2.162595</td>\n",
       "      <td>0.0334</td>\n",
       "      <td>0.3398</td>\n",
       "      <td>7.5256</td>\n",
       "      <td>1.1129</td>\n",
       "      <td>12.0678</td>\n",
       "      <td>1.8132</td>\n",
       "      <td>1.4414</td>\n",
       "      <td>63.1004</td>\n",
       "      <td>1.2317</td>\n",
       "      <td>3.7737</td>\n",
       "      <td>57.8348</td>\n",
       "      <td>0.9107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.181000</td>\n",
       "      <td>2.164686</td>\n",
       "      <td>0.0460</td>\n",
       "      <td>0.1867</td>\n",
       "      <td>9.6198</td>\n",
       "      <td>1.0781</td>\n",
       "      <td>6.8962</td>\n",
       "      <td>3.1171</td>\n",
       "      <td>1.6505</td>\n",
       "      <td>66.2076</td>\n",
       "      <td>1.2863</td>\n",
       "      <td>5.5110</td>\n",
       "      <td>55.7319</td>\n",
       "      <td>0.8522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.175600</td>\n",
       "      <td>2.163231</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>0.1789</td>\n",
       "      <td>7.8434</td>\n",
       "      <td>1.0176</td>\n",
       "      <td>6.4966</td>\n",
       "      <td>0.3325</td>\n",
       "      <td>1.4631</td>\n",
       "      <td>59.7420</td>\n",
       "      <td>1.1677</td>\n",
       "      <td>3.8603</td>\n",
       "      <td>56.1855</td>\n",
       "      <td>0.9184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.178000</td>\n",
       "      <td>2.162626</td>\n",
       "      <td>0.0253</td>\n",
       "      <td>0.1958</td>\n",
       "      <td>8.9674</td>\n",
       "      <td>1.1202</td>\n",
       "      <td>8.7685</td>\n",
       "      <td>1.2602</td>\n",
       "      <td>1.7481</td>\n",
       "      <td>63.5550</td>\n",
       "      <td>1.2697</td>\n",
       "      <td>5.4956</td>\n",
       "      <td>54.5280</td>\n",
       "      <td>0.9516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.182800</td>\n",
       "      <td>2.163358</td>\n",
       "      <td>0.0243</td>\n",
       "      <td>0.154</td>\n",
       "      <td>7.9394</td>\n",
       "      <td>0.9681</td>\n",
       "      <td>0.8509</td>\n",
       "      <td>3.4923</td>\n",
       "      <td>0.7905</td>\n",
       "      <td>61.0737</td>\n",
       "      <td>1.0912</td>\n",
       "      <td>4.2851</td>\n",
       "      <td>55.4726</td>\n",
       "      <td>0.8488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.177800</td>\n",
       "      <td>2.161380</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.1971</td>\n",
       "      <td>7.5176</td>\n",
       "      <td>1.0275</td>\n",
       "      <td>2.943</td>\n",
       "      <td>0.4694</td>\n",
       "      <td>0.6909</td>\n",
       "      <td>64.7751</td>\n",
       "      <td>0.9653</td>\n",
       "      <td>3.4621</td>\n",
       "      <td>57.7689</td>\n",
       "      <td>0.8693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.180900</td>\n",
       "      <td>2.159762</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>0.2194</td>\n",
       "      <td>6.1583</td>\n",
       "      <td>1.0279</td>\n",
       "      <td>6.5023</td>\n",
       "      <td>0.4864</td>\n",
       "      <td>0.7811</td>\n",
       "      <td>67.1072</td>\n",
       "      <td>1.1423</td>\n",
       "      <td>2.5038</td>\n",
       "      <td>59.1746</td>\n",
       "      <td>0.9044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>2.179100</td>\n",
       "      <td>2.163557</td>\n",
       "      <td>0.0542</td>\n",
       "      <td>0.4642</td>\n",
       "      <td>7.3748</td>\n",
       "      <td>1.0516</td>\n",
       "      <td>4.5315</td>\n",
       "      <td>2.4176</td>\n",
       "      <td>1.3054</td>\n",
       "      <td>66.4190</td>\n",
       "      <td>1.2884</td>\n",
       "      <td>3.495</td>\n",
       "      <td>59.5769</td>\n",
       "      <td>0.9702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>2.178000</td>\n",
       "      <td>2.163322</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.2443</td>\n",
       "      <td>7.8986</td>\n",
       "      <td>1.0858</td>\n",
       "      <td>9.0944</td>\n",
       "      <td>2.3993</td>\n",
       "      <td>1.603</td>\n",
       "      <td>66.8054</td>\n",
       "      <td>1.6008</td>\n",
       "      <td>3.6686</td>\n",
       "      <td>58.4544</td>\n",
       "      <td>0.9926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>2.175200</td>\n",
       "      <td>2.158898</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.3422</td>\n",
       "      <td>6.7914</td>\n",
       "      <td>0.7487</td>\n",
       "      <td>0.4457</td>\n",
       "      <td>1.4070</td>\n",
       "      <td>0.8751</td>\n",
       "      <td>51.2093</td>\n",
       "      <td>1.3173</td>\n",
       "      <td>3.4874</td>\n",
       "      <td>58.7454</td>\n",
       "      <td>1.0591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>2.176400</td>\n",
       "      <td>2.155293</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.2334</td>\n",
       "      <td>8.4864</td>\n",
       "      <td>1.03</td>\n",
       "      <td>7.3699</td>\n",
       "      <td>2.2796</td>\n",
       "      <td>1.9761</td>\n",
       "      <td>65.4179</td>\n",
       "      <td>1.3464</td>\n",
       "      <td>3.9143</td>\n",
       "      <td>56.252</td>\n",
       "      <td>0.8836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.176000</td>\n",
       "      <td>2.161501</td>\n",
       "      <td>0.0473</td>\n",
       "      <td>0.3315</td>\n",
       "      <td>8.8726</td>\n",
       "      <td>0.9366</td>\n",
       "      <td>8.2832</td>\n",
       "      <td>1.3490</td>\n",
       "      <td>1.83</td>\n",
       "      <td>57.6581</td>\n",
       "      <td>1.1869</td>\n",
       "      <td>4.978</td>\n",
       "      <td>54.5778</td>\n",
       "      <td>0.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>2.176900</td>\n",
       "      <td>2.162210</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.4247</td>\n",
       "      <td>8.2981</td>\n",
       "      <td>0.905</td>\n",
       "      <td>9.8398</td>\n",
       "      <td>1.4878</td>\n",
       "      <td>1.5074</td>\n",
       "      <td>64.5431</td>\n",
       "      <td>1.2257</td>\n",
       "      <td>4.1081</td>\n",
       "      <td>56.1881</td>\n",
       "      <td>0.9317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>2.183700</td>\n",
       "      <td>2.161466</td>\n",
       "      <td>0.0380</td>\n",
       "      <td>0.3982</td>\n",
       "      <td>7.7109</td>\n",
       "      <td>1.0067</td>\n",
       "      <td>3.4697</td>\n",
       "      <td>1.3709</td>\n",
       "      <td>1.144</td>\n",
       "      <td>66.5391</td>\n",
       "      <td>1.1614</td>\n",
       "      <td>3.7578</td>\n",
       "      <td>57.9178</td>\n",
       "      <td>0.9138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>2.180300</td>\n",
       "      <td>2.160738</td>\n",
       "      <td>0.0592</td>\n",
       "      <td>0.3855</td>\n",
       "      <td>8.4973</td>\n",
       "      <td>1.2524</td>\n",
       "      <td>17.9918</td>\n",
       "      <td>2.6933</td>\n",
       "      <td>2.2297</td>\n",
       "      <td>64.1461</td>\n",
       "      <td>1.2307</td>\n",
       "      <td>3.9505</td>\n",
       "      <td>56.7398</td>\n",
       "      <td>0.9864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>2.172400</td>\n",
       "      <td>2.160551</td>\n",
       "      <td>0.0273</td>\n",
       "      <td>0.3005</td>\n",
       "      <td>7.6662</td>\n",
       "      <td>1.2045</td>\n",
       "      <td>10.0416</td>\n",
       "      <td>2.8867</td>\n",
       "      <td>1.4288</td>\n",
       "      <td>68.8681</td>\n",
       "      <td>1.5769</td>\n",
       "      <td>3.2587</td>\n",
       "      <td>58.2907</td>\n",
       "      <td>0.9536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.173300</td>\n",
       "      <td>2.157615</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>0.3863</td>\n",
       "      <td>7.0894</td>\n",
       "      <td>0.9555</td>\n",
       "      <td>10.1921</td>\n",
       "      <td>2.3493</td>\n",
       "      <td>1.6519</td>\n",
       "      <td>61.8609</td>\n",
       "      <td>1.2902</td>\n",
       "      <td>3.8226</td>\n",
       "      <td>58.257</td>\n",
       "      <td>0.9233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>2.173200</td>\n",
       "      <td>2.158741</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.4140</td>\n",
       "      <td>7.1847</td>\n",
       "      <td>1.0970</td>\n",
       "      <td>15.5403</td>\n",
       "      <td>2.9258</td>\n",
       "      <td>2.0485</td>\n",
       "      <td>62.8678</td>\n",
       "      <td>1.1634</td>\n",
       "      <td>3.2313</td>\n",
       "      <td>58.1819</td>\n",
       "      <td>0.9551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>2.179200</td>\n",
       "      <td>2.162123</td>\n",
       "      <td>0.0443</td>\n",
       "      <td>0.4557</td>\n",
       "      <td>6.8446</td>\n",
       "      <td>1.0759</td>\n",
       "      <td>7.5402</td>\n",
       "      <td>2.1835</td>\n",
       "      <td>1.342</td>\n",
       "      <td>64.8640</td>\n",
       "      <td>1.259</td>\n",
       "      <td>2.8652</td>\n",
       "      <td>59.2729</td>\n",
       "      <td>0.991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>2.179500</td>\n",
       "      <td>2.159885</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.4928</td>\n",
       "      <td>7.2448</td>\n",
       "      <td>1.1743</td>\n",
       "      <td>15.9297</td>\n",
       "      <td>2.1401</td>\n",
       "      <td>1.8807</td>\n",
       "      <td>62.1445</td>\n",
       "      <td>1.1651</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>57.4052</td>\n",
       "      <td>0.9050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>2.179500</td>\n",
       "      <td>2.157681</td>\n",
       "      <td>0.0435</td>\n",
       "      <td>0.4223</td>\n",
       "      <td>7.8523</td>\n",
       "      <td>1.2770</td>\n",
       "      <td>15.0567</td>\n",
       "      <td>1.9578</td>\n",
       "      <td>2.2082</td>\n",
       "      <td>62.7538</td>\n",
       "      <td>1.0884</td>\n",
       "      <td>3.7589</td>\n",
       "      <td>56.8865</td>\n",
       "      <td>0.9534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.171400</td>\n",
       "      <td>2.156536</td>\n",
       "      <td>0.0512</td>\n",
       "      <td>0.4283</td>\n",
       "      <td>7.5680</td>\n",
       "      <td>1.3558</td>\n",
       "      <td>23.2177</td>\n",
       "      <td>2.4576</td>\n",
       "      <td>1.994</td>\n",
       "      <td>63.4576</td>\n",
       "      <td>1.0975</td>\n",
       "      <td>3.4671</td>\n",
       "      <td>58.0855</td>\n",
       "      <td>0.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>2.175200</td>\n",
       "      <td>2.163562</td>\n",
       "      <td>0.0440</td>\n",
       "      <td>0.3749</td>\n",
       "      <td>8.3814</td>\n",
       "      <td>1.2067</td>\n",
       "      <td>21.4033</td>\n",
       "      <td>2.3791</td>\n",
       "      <td>2.5937</td>\n",
       "      <td>63.0352</td>\n",
       "      <td>1.1383</td>\n",
       "      <td>3.9742</td>\n",
       "      <td>57.5510</td>\n",
       "      <td>0.9923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>2.174600</td>\n",
       "      <td>2.163250</td>\n",
       "      <td>0.0321</td>\n",
       "      <td>0.1637</td>\n",
       "      <td>8.1667</td>\n",
       "      <td>1.2624</td>\n",
       "      <td>17.1141</td>\n",
       "      <td>1.5944</td>\n",
       "      <td>1.4730</td>\n",
       "      <td>56.9265</td>\n",
       "      <td>1.1734</td>\n",
       "      <td>3.5833</td>\n",
       "      <td>56.4208</td>\n",
       "      <td>0.9279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>2.176000</td>\n",
       "      <td>2.159311</td>\n",
       "      <td>0.0632</td>\n",
       "      <td>0.4967</td>\n",
       "      <td>8.5517</td>\n",
       "      <td>1.1245</td>\n",
       "      <td>17.0716</td>\n",
       "      <td>0.9962</td>\n",
       "      <td>2.2629</td>\n",
       "      <td>62.0265</td>\n",
       "      <td>1.1088</td>\n",
       "      <td>4.2537</td>\n",
       "      <td>55.7274</td>\n",
       "      <td>0.9748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>2.175100</td>\n",
       "      <td>2.155588</td>\n",
       "      <td>0.0373</td>\n",
       "      <td>0.4124</td>\n",
       "      <td>6.8725</td>\n",
       "      <td>1.1272</td>\n",
       "      <td>6.6142</td>\n",
       "      <td>1.1075</td>\n",
       "      <td>1.3202</td>\n",
       "      <td>67.7356</td>\n",
       "      <td>1.3254</td>\n",
       "      <td>2.7061</td>\n",
       "      <td>60.9987</td>\n",
       "      <td>0.9905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.174500</td>\n",
       "      <td>2.155469</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.303</td>\n",
       "      <td>8.2360</td>\n",
       "      <td>1.1476</td>\n",
       "      <td>11.3938</td>\n",
       "      <td>2.438</td>\n",
       "      <td>1.5472</td>\n",
       "      <td>64.7122</td>\n",
       "      <td>1.1427</td>\n",
       "      <td>3.4681</td>\n",
       "      <td>59.1732</td>\n",
       "      <td>1.0155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>2.176600</td>\n",
       "      <td>2.156529</td>\n",
       "      <td>0.0356</td>\n",
       "      <td>0.4201</td>\n",
       "      <td>7.0136</td>\n",
       "      <td>1.0597</td>\n",
       "      <td>5.2445</td>\n",
       "      <td>4.3083</td>\n",
       "      <td>1.2293</td>\n",
       "      <td>68.6148</td>\n",
       "      <td>1.3651</td>\n",
       "      <td>3.0024</td>\n",
       "      <td>61.94</td>\n",
       "      <td>1.0256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>2.174300</td>\n",
       "      <td>2.156956</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>0.2720</td>\n",
       "      <td>8.3424</td>\n",
       "      <td>1.4159</td>\n",
       "      <td>18.2193</td>\n",
       "      <td>2.2477</td>\n",
       "      <td>2.0624</td>\n",
       "      <td>60.8019</td>\n",
       "      <td>1.1056</td>\n",
       "      <td>3.8755</td>\n",
       "      <td>56.4572</td>\n",
       "      <td>0.9643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>2.176300</td>\n",
       "      <td>2.160277</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>0.2406</td>\n",
       "      <td>7.994</td>\n",
       "      <td>1.162</td>\n",
       "      <td>10.6694</td>\n",
       "      <td>2.3456</td>\n",
       "      <td>1.7825</td>\n",
       "      <td>65.7327</td>\n",
       "      <td>1.3381</td>\n",
       "      <td>3.7151</td>\n",
       "      <td>58.5422</td>\n",
       "      <td>0.9983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>2.174100</td>\n",
       "      <td>2.155124</td>\n",
       "      <td>0.0469</td>\n",
       "      <td>0.3952</td>\n",
       "      <td>8.2366</td>\n",
       "      <td>1.2173</td>\n",
       "      <td>13.4573</td>\n",
       "      <td>2.8407</td>\n",
       "      <td>2.0991</td>\n",
       "      <td>64.7509</td>\n",
       "      <td>1.3736</td>\n",
       "      <td>3.8693</td>\n",
       "      <td>58.0037</td>\n",
       "      <td>0.9732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.179700</td>\n",
       "      <td>2.154406</td>\n",
       "      <td>0.0451</td>\n",
       "      <td>0.417</td>\n",
       "      <td>7.2846</td>\n",
       "      <td>1.1139</td>\n",
       "      <td>7.7974</td>\n",
       "      <td>3.3614</td>\n",
       "      <td>1.4388</td>\n",
       "      <td>67.4034</td>\n",
       "      <td>1.3617</td>\n",
       "      <td>3.4698</td>\n",
       "      <td>59.2632</td>\n",
       "      <td>0.9670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>2.170500</td>\n",
       "      <td>2.155181</td>\n",
       "      <td>0.0430</td>\n",
       "      <td>0.4241</td>\n",
       "      <td>7.4655</td>\n",
       "      <td>1.1120</td>\n",
       "      <td>11.3080</td>\n",
       "      <td>3.3362</td>\n",
       "      <td>1.6585</td>\n",
       "      <td>68.0309</td>\n",
       "      <td>1.4281</td>\n",
       "      <td>3.3341</td>\n",
       "      <td>59.3495</td>\n",
       "      <td>0.9979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>2.174200</td>\n",
       "      <td>2.155228</td>\n",
       "      <td>0.0441</td>\n",
       "      <td>0.5239</td>\n",
       "      <td>7.125</td>\n",
       "      <td>1.2315</td>\n",
       "      <td>18.0594</td>\n",
       "      <td>3.1529</td>\n",
       "      <td>2.1034</td>\n",
       "      <td>66.4485</td>\n",
       "      <td>1.4041</td>\n",
       "      <td>3.3396</td>\n",
       "      <td>59.2606</td>\n",
       "      <td>0.9773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>2.169400</td>\n",
       "      <td>2.155183</td>\n",
       "      <td>0.0460</td>\n",
       "      <td>0.4261</td>\n",
       "      <td>7.4337</td>\n",
       "      <td>1.1957</td>\n",
       "      <td>11.9473</td>\n",
       "      <td>3.0056</td>\n",
       "      <td>1.5736</td>\n",
       "      <td>68.3321</td>\n",
       "      <td>1.4259</td>\n",
       "      <td>3.1197</td>\n",
       "      <td>60.5268</td>\n",
       "      <td>1.0137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>2.174400</td>\n",
       "      <td>2.159400</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.3345</td>\n",
       "      <td>7.1694</td>\n",
       "      <td>1.0075</td>\n",
       "      <td>13.7653</td>\n",
       "      <td>2.3555</td>\n",
       "      <td>1.7405</td>\n",
       "      <td>63.8150</td>\n",
       "      <td>1.2115</td>\n",
       "      <td>3.1924</td>\n",
       "      <td>58.8352</td>\n",
       "      <td>0.9301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.171100</td>\n",
       "      <td>2.154883</td>\n",
       "      <td>0.0518</td>\n",
       "      <td>0.3547</td>\n",
       "      <td>8.1554</td>\n",
       "      <td>1.1343</td>\n",
       "      <td>8.2368</td>\n",
       "      <td>3.5873</td>\n",
       "      <td>1.7539</td>\n",
       "      <td>67.9389</td>\n",
       "      <td>1.4501</td>\n",
       "      <td>3.9484</td>\n",
       "      <td>58.3569</td>\n",
       "      <td>0.945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>2.174700</td>\n",
       "      <td>2.154706</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.3741</td>\n",
       "      <td>7.3953</td>\n",
       "      <td>1.0948</td>\n",
       "      <td>8.6857</td>\n",
       "      <td>3.6613</td>\n",
       "      <td>1.7229</td>\n",
       "      <td>66.6041</td>\n",
       "      <td>1.3573</td>\n",
       "      <td>3.2405</td>\n",
       "      <td>59.5791</td>\n",
       "      <td>0.9791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>2.175400</td>\n",
       "      <td>2.154919</td>\n",
       "      <td>0.0432</td>\n",
       "      <td>0.402</td>\n",
       "      <td>8.0789</td>\n",
       "      <td>1.2063</td>\n",
       "      <td>12.0494</td>\n",
       "      <td>3.6707</td>\n",
       "      <td>1.7644</td>\n",
       "      <td>67.0469</td>\n",
       "      <td>1.2204</td>\n",
       "      <td>3.6227</td>\n",
       "      <td>59.7185</td>\n",
       "      <td>1.0201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>2.172000</td>\n",
       "      <td>2.154749</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.4208</td>\n",
       "      <td>7.8455</td>\n",
       "      <td>1.188</td>\n",
       "      <td>13.7716</td>\n",
       "      <td>3.5122</td>\n",
       "      <td>1.9151</td>\n",
       "      <td>67.7356</td>\n",
       "      <td>1.4514</td>\n",
       "      <td>3.2106</td>\n",
       "      <td>59.735</td>\n",
       "      <td>1.0191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>2.172400</td>\n",
       "      <td>2.158855</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>0.2944</td>\n",
       "      <td>7.9940</td>\n",
       "      <td>1.3314</td>\n",
       "      <td>16.5792</td>\n",
       "      <td>2.5678</td>\n",
       "      <td>1.8503</td>\n",
       "      <td>65.1789</td>\n",
       "      <td>1.4246</td>\n",
       "      <td>3.1667</td>\n",
       "      <td>59.1801</td>\n",
       "      <td>0.9507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.169300</td>\n",
       "      <td>2.154804</td>\n",
       "      <td>0.0568</td>\n",
       "      <td>0.4037</td>\n",
       "      <td>8.1724</td>\n",
       "      <td>1.1921</td>\n",
       "      <td>13.9596</td>\n",
       "      <td>2.6055</td>\n",
       "      <td>2.2428</td>\n",
       "      <td>65.4424</td>\n",
       "      <td>1.2625</td>\n",
       "      <td>3.5349</td>\n",
       "      <td>59.3021</td>\n",
       "      <td>1.0361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>2.177700</td>\n",
       "      <td>2.155500</td>\n",
       "      <td>0.0553</td>\n",
       "      <td>0.4572</td>\n",
       "      <td>8.0502</td>\n",
       "      <td>1.6261</td>\n",
       "      <td>29.1504</td>\n",
       "      <td>2.2454</td>\n",
       "      <td>2.6204</td>\n",
       "      <td>63.0218</td>\n",
       "      <td>1.1821</td>\n",
       "      <td>3.1496</td>\n",
       "      <td>56.9721</td>\n",
       "      <td>0.9544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>2.164600</td>\n",
       "      <td>2.153647</td>\n",
       "      <td>0.0576</td>\n",
       "      <td>0.5566</td>\n",
       "      <td>7.8412</td>\n",
       "      <td>1.2800</td>\n",
       "      <td>19.4444</td>\n",
       "      <td>2.3026</td>\n",
       "      <td>2.1584</td>\n",
       "      <td>67.7295</td>\n",
       "      <td>1.2962</td>\n",
       "      <td>3.2620</td>\n",
       "      <td>58.6210</td>\n",
       "      <td>0.9877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>2.174200</td>\n",
       "      <td>2.156243</td>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.6212</td>\n",
       "      <td>7.9195</td>\n",
       "      <td>1.3224</td>\n",
       "      <td>19.6657</td>\n",
       "      <td>2.6865</td>\n",
       "      <td>2.1465</td>\n",
       "      <td>65.7486</td>\n",
       "      <td>1.3866</td>\n",
       "      <td>3.2612</td>\n",
       "      <td>59.9022</td>\n",
       "      <td>1.0269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>2.173000</td>\n",
       "      <td>2.153805</td>\n",
       "      <td>0.0460</td>\n",
       "      <td>0.5029</td>\n",
       "      <td>7.5812</td>\n",
       "      <td>1.2128</td>\n",
       "      <td>12.2861</td>\n",
       "      <td>2.9758</td>\n",
       "      <td>1.7503</td>\n",
       "      <td>66.2993</td>\n",
       "      <td>1.3818</td>\n",
       "      <td>3.1699</td>\n",
       "      <td>60.7348</td>\n",
       "      <td>1.0763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.166600</td>\n",
       "      <td>2.153442</td>\n",
       "      <td>0.0441</td>\n",
       "      <td>0.4968</td>\n",
       "      <td>7.4402</td>\n",
       "      <td>1.1178</td>\n",
       "      <td>9.8694</td>\n",
       "      <td>3.4494</td>\n",
       "      <td>1.7961</td>\n",
       "      <td>67.2453</td>\n",
       "      <td>1.394</td>\n",
       "      <td>3.2491</td>\n",
       "      <td>59.8748</td>\n",
       "      <td>1.0301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>2.170700</td>\n",
       "      <td>2.153655</td>\n",
       "      <td>0.0454</td>\n",
       "      <td>0.5119</td>\n",
       "      <td>7.6578</td>\n",
       "      <td>1.2745</td>\n",
       "      <td>16.1417</td>\n",
       "      <td>2.8111</td>\n",
       "      <td>2.1966</td>\n",
       "      <td>67.8819</td>\n",
       "      <td>1.2697</td>\n",
       "      <td>3.1842</td>\n",
       "      <td>58.9266</td>\n",
       "      <td>1.0261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>2.175300</td>\n",
       "      <td>2.154399</td>\n",
       "      <td>0.0547</td>\n",
       "      <td>0.5479</td>\n",
       "      <td>8.197</td>\n",
       "      <td>1.3443</td>\n",
       "      <td>19.3841</td>\n",
       "      <td>2.8166</td>\n",
       "      <td>2.3415</td>\n",
       "      <td>66.4678</td>\n",
       "      <td>1.2241</td>\n",
       "      <td>3.4555</td>\n",
       "      <td>58.5818</td>\n",
       "      <td>1.0252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>2.161100</td>\n",
       "      <td>2.153786</td>\n",
       "      <td>0.0467</td>\n",
       "      <td>0.4781</td>\n",
       "      <td>7.7065</td>\n",
       "      <td>1.2303</td>\n",
       "      <td>14.5775</td>\n",
       "      <td>3.2128</td>\n",
       "      <td>2.0996</td>\n",
       "      <td>66.8951</td>\n",
       "      <td>1.2663</td>\n",
       "      <td>3.2989</td>\n",
       "      <td>60.0765</td>\n",
       "      <td>1.0103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>2.177000</td>\n",
       "      <td>2.154824</td>\n",
       "      <td>0.0414</td>\n",
       "      <td>0.5123</td>\n",
       "      <td>7.5473</td>\n",
       "      <td>1.2291</td>\n",
       "      <td>14.3536</td>\n",
       "      <td>2.8220</td>\n",
       "      <td>2.0703</td>\n",
       "      <td>66.1798</td>\n",
       "      <td>1.2163</td>\n",
       "      <td>3.1932</td>\n",
       "      <td>59.9236</td>\n",
       "      <td>1.0147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.166000</td>\n",
       "      <td>2.154325</td>\n",
       "      <td>0.0496</td>\n",
       "      <td>0.5294</td>\n",
       "      <td>8.044</td>\n",
       "      <td>1.4131</td>\n",
       "      <td>23.3692</td>\n",
       "      <td>2.3677</td>\n",
       "      <td>2.6139</td>\n",
       "      <td>65.2346</td>\n",
       "      <td>1.1691</td>\n",
       "      <td>3.332</td>\n",
       "      <td>57.8814</td>\n",
       "      <td>0.9924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>2.176200</td>\n",
       "      <td>2.153042</td>\n",
       "      <td>0.0532</td>\n",
       "      <td>0.5688</td>\n",
       "      <td>7.6642</td>\n",
       "      <td>1.2510</td>\n",
       "      <td>15.0608</td>\n",
       "      <td>3.0326</td>\n",
       "      <td>2.2423</td>\n",
       "      <td>65.7382</td>\n",
       "      <td>1.2285</td>\n",
       "      <td>3.264</td>\n",
       "      <td>59.9911</td>\n",
       "      <td>1.0245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>2.169000</td>\n",
       "      <td>2.152288</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.5504</td>\n",
       "      <td>7.9773</td>\n",
       "      <td>1.3682</td>\n",
       "      <td>20.6523</td>\n",
       "      <td>2.7829</td>\n",
       "      <td>2.4168</td>\n",
       "      <td>65.7045</td>\n",
       "      <td>1.2253</td>\n",
       "      <td>3.3149</td>\n",
       "      <td>58.7493</td>\n",
       "      <td>0.9855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>2.168400</td>\n",
       "      <td>2.152277</td>\n",
       "      <td>0.0515</td>\n",
       "      <td>0.5296</td>\n",
       "      <td>7.99</td>\n",
       "      <td>1.3495</td>\n",
       "      <td>19.9505</td>\n",
       "      <td>2.8374</td>\n",
       "      <td>2.4202</td>\n",
       "      <td>65.5164</td>\n",
       "      <td>1.1829</td>\n",
       "      <td>3.2868</td>\n",
       "      <td>59.3014</td>\n",
       "      <td>1.0144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>2.165200</td>\n",
       "      <td>2.152453</td>\n",
       "      <td>0.0497</td>\n",
       "      <td>0.5093</td>\n",
       "      <td>7.8679</td>\n",
       "      <td>1.3885</td>\n",
       "      <td>21.4103</td>\n",
       "      <td>2.7841</td>\n",
       "      <td>2.4337</td>\n",
       "      <td>65.7455</td>\n",
       "      <td>1.2135</td>\n",
       "      <td>3.1906</td>\n",
       "      <td>59.283</td>\n",
       "      <td>1.0206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>2.152827</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.4685</td>\n",
       "      <td>7.7906</td>\n",
       "      <td>1.2584</td>\n",
       "      <td>15.3285</td>\n",
       "      <td>3.1733</td>\n",
       "      <td>2.2118</td>\n",
       "      <td>66.0428</td>\n",
       "      <td>1.2329</td>\n",
       "      <td>3.2782</td>\n",
       "      <td>59.7188</td>\n",
       "      <td>1.0249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>2.172300</td>\n",
       "      <td>2.152894</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.4772</td>\n",
       "      <td>7.9494</td>\n",
       "      <td>1.3601</td>\n",
       "      <td>20.6762</td>\n",
       "      <td>3.0156</td>\n",
       "      <td>2.3520</td>\n",
       "      <td>65.4387</td>\n",
       "      <td>1.2289</td>\n",
       "      <td>3.291</td>\n",
       "      <td>59.1978</td>\n",
       "      <td>1.0031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>2.168800</td>\n",
       "      <td>2.153398</td>\n",
       "      <td>0.0434</td>\n",
       "      <td>0.4501</td>\n",
       "      <td>8.0135</td>\n",
       "      <td>1.3555</td>\n",
       "      <td>20.7308</td>\n",
       "      <td>2.9115</td>\n",
       "      <td>2.4324</td>\n",
       "      <td>65.1663</td>\n",
       "      <td>1.2068</td>\n",
       "      <td>3.3026</td>\n",
       "      <td>59.044</td>\n",
       "      <td>1.0006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>2.173600</td>\n",
       "      <td>2.152939</td>\n",
       "      <td>0.0441</td>\n",
       "      <td>0.46</td>\n",
       "      <td>7.9127</td>\n",
       "      <td>1.3260</td>\n",
       "      <td>19.242</td>\n",
       "      <td>2.9929</td>\n",
       "      <td>2.4253</td>\n",
       "      <td>65.5546</td>\n",
       "      <td>1.2006</td>\n",
       "      <td>3.3211</td>\n",
       "      <td>59.1473</td>\n",
       "      <td>1.0098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>2.165500</td>\n",
       "      <td>2.152992</td>\n",
       "      <td>0.0445</td>\n",
       "      <td>0.4634</td>\n",
       "      <td>7.9397</td>\n",
       "      <td>1.3380</td>\n",
       "      <td>19.4514</td>\n",
       "      <td>2.9623</td>\n",
       "      <td>2.4119</td>\n",
       "      <td>65.5743</td>\n",
       "      <td>1.2016</td>\n",
       "      <td>3.3177</td>\n",
       "      <td>59.1332</td>\n",
       "      <td>1.0023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>2.172000</td>\n",
       "      <td>2.153089</td>\n",
       "      <td>0.0448</td>\n",
       "      <td>0.4677</td>\n",
       "      <td>7.9510</td>\n",
       "      <td>1.3495</td>\n",
       "      <td>20.1051</td>\n",
       "      <td>2.9045</td>\n",
       "      <td>2.4243</td>\n",
       "      <td>65.0839</td>\n",
       "      <td>1.1941</td>\n",
       "      <td>3.2873</td>\n",
       "      <td>59.1556</td>\n",
       "      <td>1.0099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>2.165700</td>\n",
       "      <td>2.152925</td>\n",
       "      <td>0.0452</td>\n",
       "      <td>0.4683</td>\n",
       "      <td>7.9746</td>\n",
       "      <td>1.3630</td>\n",
       "      <td>20.3947</td>\n",
       "      <td>2.8231</td>\n",
       "      <td>2.4369</td>\n",
       "      <td>65.1149</td>\n",
       "      <td>1.1933</td>\n",
       "      <td>3.3029</td>\n",
       "      <td>59.0163</td>\n",
       "      <td>1.0108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>2.173600</td>\n",
       "      <td>2.152784</td>\n",
       "      <td>0.0449</td>\n",
       "      <td>0.4704</td>\n",
       "      <td>7.9353</td>\n",
       "      <td>1.3423</td>\n",
       "      <td>19.6147</td>\n",
       "      <td>2.9422</td>\n",
       "      <td>2.4116</td>\n",
       "      <td>65.2845</td>\n",
       "      <td>1.2022</td>\n",
       "      <td>3.2957</td>\n",
       "      <td>59.2527</td>\n",
       "      <td>1.0060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>2.169500</td>\n",
       "      <td>2.152811</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.4705</td>\n",
       "      <td>7.9391</td>\n",
       "      <td>1.3456</td>\n",
       "      <td>19.7133</td>\n",
       "      <td>2.914</td>\n",
       "      <td>2.4146</td>\n",
       "      <td>65.2502</td>\n",
       "      <td>1.1985</td>\n",
       "      <td>3.2959</td>\n",
       "      <td>59.2205</td>\n",
       "      <td>1.0108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=17746, training_loss=2.178027170749416, metrics={'train_runtime': 5250.4503, 'train_samples_per_second': 13.519, 'train_steps_per_second': 3.38, 'total_flos': 0.0, 'train_loss': 2.178027170749416, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oanda data all pairs past 2009, overnight labels ignored\n",
    "\n",
    "# SGCONV transformer architecture! lr of 1e-3, batch size 5 hidden size 512, 4 heads\n",
    "# NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# ce loss with conditioned kelly betting\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17746' max='17746' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17746/17746 4:32:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.240400</td>\n",
       "      <td>2.185881</td>\n",
       "      <td>-0.0195</td>\n",
       "      <td>-0.1244</td>\n",
       "      <td>8.1630</td>\n",
       "      <td>0.6525</td>\n",
       "      <td>11.4893</td>\n",
       "      <td>0.5389</td>\n",
       "      <td>1.6943</td>\n",
       "      <td>22.0198</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>2.9776</td>\n",
       "      <td>41.627</td>\n",
       "      <td>1.0287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.197200</td>\n",
       "      <td>2.165978</td>\n",
       "      <td>0.0212</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>7.9601</td>\n",
       "      <td>0.6963</td>\n",
       "      <td>0.2695</td>\n",
       "      <td>1.2043</td>\n",
       "      <td>0.6476</td>\n",
       "      <td>9.0324</td>\n",
       "      <td>1.3131</td>\n",
       "      <td>3.1307</td>\n",
       "      <td>48.7494</td>\n",
       "      <td>1.1110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.195300</td>\n",
       "      <td>2.168291</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>0.2915</td>\n",
       "      <td>6.2947</td>\n",
       "      <td>0.9311</td>\n",
       "      <td>0.2464</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.4372</td>\n",
       "      <td>7.7272</td>\n",
       "      <td>1.3352</td>\n",
       "      <td>0.8152</td>\n",
       "      <td>34.3310</td>\n",
       "      <td>1.0722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.194700</td>\n",
       "      <td>2.189220</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>-0.0233</td>\n",
       "      <td>9.3727</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>19.6721</td>\n",
       "      <td>0.3869</td>\n",
       "      <td>1.5050</td>\n",
       "      <td>16.2394</td>\n",
       "      <td>0.8599</td>\n",
       "      <td>10.9010</td>\n",
       "      <td>45.5354</td>\n",
       "      <td>1.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.181100</td>\n",
       "      <td>2.175700</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.0959</td>\n",
       "      <td>7.7666</td>\n",
       "      <td>1.7328</td>\n",
       "      <td>2.7861</td>\n",
       "      <td>0.6795</td>\n",
       "      <td>0.3101</td>\n",
       "      <td>30.9011</td>\n",
       "      <td>1.2570</td>\n",
       "      <td>1.9119</td>\n",
       "      <td>50.2509</td>\n",
       "      <td>0.8763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.184100</td>\n",
       "      <td>2.175899</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0603</td>\n",
       "      <td>5.8772</td>\n",
       "      <td>1.2315</td>\n",
       "      <td>1.6679</td>\n",
       "      <td>1.7719</td>\n",
       "      <td>0.3641</td>\n",
       "      <td>14.2612</td>\n",
       "      <td>1.3344</td>\n",
       "      <td>0.6392</td>\n",
       "      <td>38.7765</td>\n",
       "      <td>0.932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.183400</td>\n",
       "      <td>2.164834</td>\n",
       "      <td>-0.0139</td>\n",
       "      <td>-0.0812</td>\n",
       "      <td>7.1664</td>\n",
       "      <td>0.6515</td>\n",
       "      <td>5.6178</td>\n",
       "      <td>0.9303</td>\n",
       "      <td>0.8768</td>\n",
       "      <td>35.6523</td>\n",
       "      <td>0.6654</td>\n",
       "      <td>3.6396</td>\n",
       "      <td>51.9754</td>\n",
       "      <td>0.8327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.190400</td>\n",
       "      <td>2.161458</td>\n",
       "      <td>-0.0071</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>7.8329</td>\n",
       "      <td>0.8018</td>\n",
       "      <td>3.6411</td>\n",
       "      <td>0.9195</td>\n",
       "      <td>0.5217</td>\n",
       "      <td>36.3982</td>\n",
       "      <td>0.5668</td>\n",
       "      <td>3.9889</td>\n",
       "      <td>53.7054</td>\n",
       "      <td>0.8903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.174900</td>\n",
       "      <td>2.153806</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.0904</td>\n",
       "      <td>7.5812</td>\n",
       "      <td>1.4889</td>\n",
       "      <td>0.3782</td>\n",
       "      <td>0.3971</td>\n",
       "      <td>0.4</td>\n",
       "      <td>53.0761</td>\n",
       "      <td>0.8308</td>\n",
       "      <td>2.8791</td>\n",
       "      <td>54.7950</td>\n",
       "      <td>0.8866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.179400</td>\n",
       "      <td>2.150973</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>0.1397</td>\n",
       "      <td>6.3284</td>\n",
       "      <td>1.6453</td>\n",
       "      <td>2.2435</td>\n",
       "      <td>1.5680</td>\n",
       "      <td>0.4902</td>\n",
       "      <td>65.4967</td>\n",
       "      <td>1.1894</td>\n",
       "      <td>2.0074</td>\n",
       "      <td>62.0507</td>\n",
       "      <td>1.0132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.173300</td>\n",
       "      <td>2.159921</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>6.7748</td>\n",
       "      <td>1.1623</td>\n",
       "      <td>0.5562</td>\n",
       "      <td>1.8368</td>\n",
       "      <td>0.156</td>\n",
       "      <td>40.3743</td>\n",
       "      <td>0.6121</td>\n",
       "      <td>1.6050</td>\n",
       "      <td>50.484</td>\n",
       "      <td>0.8419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.179300</td>\n",
       "      <td>2.148106</td>\n",
       "      <td>0.0281</td>\n",
       "      <td>0.2169</td>\n",
       "      <td>8.2726</td>\n",
       "      <td>1.5797</td>\n",
       "      <td>3.868</td>\n",
       "      <td>1.2081</td>\n",
       "      <td>1.3722</td>\n",
       "      <td>58.5866</td>\n",
       "      <td>1.0165</td>\n",
       "      <td>4.7762</td>\n",
       "      <td>54.685</td>\n",
       "      <td>0.9264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.177900</td>\n",
       "      <td>2.157047</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.1665</td>\n",
       "      <td>9.2125</td>\n",
       "      <td>1.7284</td>\n",
       "      <td>6.612</td>\n",
       "      <td>0.7732</td>\n",
       "      <td>1.459</td>\n",
       "      <td>51.5938</td>\n",
       "      <td>1.0503</td>\n",
       "      <td>4.9085</td>\n",
       "      <td>51.9076</td>\n",
       "      <td>0.9435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.172900</td>\n",
       "      <td>2.151960</td>\n",
       "      <td>0.0565</td>\n",
       "      <td>0.2377</td>\n",
       "      <td>8.8449</td>\n",
       "      <td>1.2792</td>\n",
       "      <td>0.5461</td>\n",
       "      <td>0.947</td>\n",
       "      <td>0.685</td>\n",
       "      <td>44.4512</td>\n",
       "      <td>1.1819</td>\n",
       "      <td>3.408</td>\n",
       "      <td>57.9458</td>\n",
       "      <td>0.8856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.175200</td>\n",
       "      <td>2.156182</td>\n",
       "      <td>0.0493</td>\n",
       "      <td>0.2917</td>\n",
       "      <td>10.305</td>\n",
       "      <td>1.5595</td>\n",
       "      <td>4.8141</td>\n",
       "      <td>1.1268</td>\n",
       "      <td>1.2928</td>\n",
       "      <td>57.2028</td>\n",
       "      <td>1.0692</td>\n",
       "      <td>6.7804</td>\n",
       "      <td>55.3093</td>\n",
       "      <td>0.9577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.168700</td>\n",
       "      <td>2.155533</td>\n",
       "      <td>0.0194</td>\n",
       "      <td>0.2623</td>\n",
       "      <td>7.583</td>\n",
       "      <td>1.3097</td>\n",
       "      <td>4.1637</td>\n",
       "      <td>2.2778</td>\n",
       "      <td>1.6800</td>\n",
       "      <td>45.2147</td>\n",
       "      <td>1.282</td>\n",
       "      <td>3.3639</td>\n",
       "      <td>51.0896</td>\n",
       "      <td>0.9745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.169600</td>\n",
       "      <td>2.153919</td>\n",
       "      <td>0.0315</td>\n",
       "      <td>0.1745</td>\n",
       "      <td>9.7419</td>\n",
       "      <td>1.6137</td>\n",
       "      <td>6.1385</td>\n",
       "      <td>0.9579</td>\n",
       "      <td>1.6748</td>\n",
       "      <td>60.4968</td>\n",
       "      <td>0.9402</td>\n",
       "      <td>5.9243</td>\n",
       "      <td>54.0446</td>\n",
       "      <td>0.8878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.171400</td>\n",
       "      <td>2.153551</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>0.2438</td>\n",
       "      <td>6.4395</td>\n",
       "      <td>1.0065</td>\n",
       "      <td>0.8080</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.5475</td>\n",
       "      <td>41.5159</td>\n",
       "      <td>1.3159</td>\n",
       "      <td>1.9846</td>\n",
       "      <td>50.9378</td>\n",
       "      <td>1.0087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.175500</td>\n",
       "      <td>2.153506</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0475</td>\n",
       "      <td>8.2559</td>\n",
       "      <td>1.5853</td>\n",
       "      <td>0.9471</td>\n",
       "      <td>1.6520</td>\n",
       "      <td>1.1923</td>\n",
       "      <td>53.6511</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>4.3186</td>\n",
       "      <td>53.7364</td>\n",
       "      <td>0.9536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.171500</td>\n",
       "      <td>2.154290</td>\n",
       "      <td>0.0226</td>\n",
       "      <td>0.1915</td>\n",
       "      <td>8.3307</td>\n",
       "      <td>1.6539</td>\n",
       "      <td>1.2546</td>\n",
       "      <td>1.3009</td>\n",
       "      <td>1.2945</td>\n",
       "      <td>62.4487</td>\n",
       "      <td>1.1549</td>\n",
       "      <td>3.4027</td>\n",
       "      <td>54.0710</td>\n",
       "      <td>0.9124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.170200</td>\n",
       "      <td>2.155761</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0621</td>\n",
       "      <td>8.3501</td>\n",
       "      <td>1.4069</td>\n",
       "      <td>2.4383</td>\n",
       "      <td>0.6269</td>\n",
       "      <td>1.5904</td>\n",
       "      <td>50.1213</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>4.0796</td>\n",
       "      <td>54.3105</td>\n",
       "      <td>0.8526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.171900</td>\n",
       "      <td>2.154231</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>8.7478</td>\n",
       "      <td>1.5467</td>\n",
       "      <td>1.3416</td>\n",
       "      <td>1.4183</td>\n",
       "      <td>0.8809</td>\n",
       "      <td>54.7585</td>\n",
       "      <td>0.6750</td>\n",
       "      <td>5.6406</td>\n",
       "      <td>53.7452</td>\n",
       "      <td>0.9117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.172600</td>\n",
       "      <td>2.149419</td>\n",
       "      <td>0.0264</td>\n",
       "      <td>0.3824</td>\n",
       "      <td>6.6426</td>\n",
       "      <td>1.5438</td>\n",
       "      <td>3.9646</td>\n",
       "      <td>1.5904</td>\n",
       "      <td>1.333</td>\n",
       "      <td>59.8482</td>\n",
       "      <td>1.2567</td>\n",
       "      <td>2.7737</td>\n",
       "      <td>58.7046</td>\n",
       "      <td>1.0223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.173200</td>\n",
       "      <td>2.149277</td>\n",
       "      <td>0.0284</td>\n",
       "      <td>0.2687</td>\n",
       "      <td>8.1222</td>\n",
       "      <td>1.6981</td>\n",
       "      <td>13.1471</td>\n",
       "      <td>1.6824</td>\n",
       "      <td>1.5368</td>\n",
       "      <td>59.6513</td>\n",
       "      <td>1.1323</td>\n",
       "      <td>3.5773</td>\n",
       "      <td>58.4703</td>\n",
       "      <td>0.9468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.172200</td>\n",
       "      <td>2.153776</td>\n",
       "      <td>0.0525</td>\n",
       "      <td>0.1631</td>\n",
       "      <td>11.1346</td>\n",
       "      <td>1.4071</td>\n",
       "      <td>7.2545</td>\n",
       "      <td>1.1218</td>\n",
       "      <td>2.0476</td>\n",
       "      <td>65.9215</td>\n",
       "      <td>1.0169</td>\n",
       "      <td>8.461</td>\n",
       "      <td>53.3681</td>\n",
       "      <td>0.8750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.165800</td>\n",
       "      <td>2.148746</td>\n",
       "      <td>0.0279</td>\n",
       "      <td>0.2953</td>\n",
       "      <td>9.5831</td>\n",
       "      <td>1.7434</td>\n",
       "      <td>10.4725</td>\n",
       "      <td>0.9913</td>\n",
       "      <td>2.3269</td>\n",
       "      <td>58.532</td>\n",
       "      <td>1.0634</td>\n",
       "      <td>4.7080</td>\n",
       "      <td>55.5324</td>\n",
       "      <td>0.9238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.168400</td>\n",
       "      <td>2.147997</td>\n",
       "      <td>0.0308</td>\n",
       "      <td>0.1886</td>\n",
       "      <td>10.0681</td>\n",
       "      <td>1.8502</td>\n",
       "      <td>8.2958</td>\n",
       "      <td>1.1873</td>\n",
       "      <td>1.9165</td>\n",
       "      <td>64.222</td>\n",
       "      <td>1.1519</td>\n",
       "      <td>5.6062</td>\n",
       "      <td>54.6647</td>\n",
       "      <td>0.9386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.174400</td>\n",
       "      <td>2.149174</td>\n",
       "      <td>0.0226</td>\n",
       "      <td>0.1174</td>\n",
       "      <td>9.3530</td>\n",
       "      <td>1.7132</td>\n",
       "      <td>4.224</td>\n",
       "      <td>1.2508</td>\n",
       "      <td>1.5959</td>\n",
       "      <td>63.9595</td>\n",
       "      <td>1.0536</td>\n",
       "      <td>4.5737</td>\n",
       "      <td>55.1728</td>\n",
       "      <td>0.7907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.169400</td>\n",
       "      <td>2.145653</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.0835</td>\n",
       "      <td>8.2757</td>\n",
       "      <td>1.7197</td>\n",
       "      <td>3.3711</td>\n",
       "      <td>0.4041</td>\n",
       "      <td>0.793</td>\n",
       "      <td>68.7048</td>\n",
       "      <td>0.9447</td>\n",
       "      <td>3.4313</td>\n",
       "      <td>57.4845</td>\n",
       "      <td>0.8203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.171400</td>\n",
       "      <td>2.146711</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.3074</td>\n",
       "      <td>6.6209</td>\n",
       "      <td>1.1839</td>\n",
       "      <td>8.2702</td>\n",
       "      <td>0.3295</td>\n",
       "      <td>1.0493</td>\n",
       "      <td>57.6667</td>\n",
       "      <td>1.1781</td>\n",
       "      <td>2.9521</td>\n",
       "      <td>56.2376</td>\n",
       "      <td>0.9807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>2.149042</td>\n",
       "      <td>0.0588</td>\n",
       "      <td>0.4793</td>\n",
       "      <td>8.7242</td>\n",
       "      <td>1.7321</td>\n",
       "      <td>5.2853</td>\n",
       "      <td>1.2068</td>\n",
       "      <td>1.445</td>\n",
       "      <td>67.6144</td>\n",
       "      <td>1.2138</td>\n",
       "      <td>4.1359</td>\n",
       "      <td>58.7999</td>\n",
       "      <td>0.9504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>2.168500</td>\n",
       "      <td>2.148879</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>0.3143</td>\n",
       "      <td>8.9354</td>\n",
       "      <td>1.8360</td>\n",
       "      <td>8.4450</td>\n",
       "      <td>2.6681</td>\n",
       "      <td>1.6754</td>\n",
       "      <td>67.1085</td>\n",
       "      <td>1.3585</td>\n",
       "      <td>4.499</td>\n",
       "      <td>58.1987</td>\n",
       "      <td>0.9636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>2.164600</td>\n",
       "      <td>2.144229</td>\n",
       "      <td>0.0319</td>\n",
       "      <td>0.2840</td>\n",
       "      <td>8.0839</td>\n",
       "      <td>1.5479</td>\n",
       "      <td>0.9835</td>\n",
       "      <td>0.4106</td>\n",
       "      <td>0.9168</td>\n",
       "      <td>59.3267</td>\n",
       "      <td>0.9578</td>\n",
       "      <td>4.5092</td>\n",
       "      <td>59.1814</td>\n",
       "      <td>0.9876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>2.165700</td>\n",
       "      <td>2.142980</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.2610</td>\n",
       "      <td>10.368</td>\n",
       "      <td>1.6599</td>\n",
       "      <td>12.4317</td>\n",
       "      <td>2.0080</td>\n",
       "      <td>2.5211</td>\n",
       "      <td>61.3243</td>\n",
       "      <td>1.1926</td>\n",
       "      <td>4.3022</td>\n",
       "      <td>55.9161</td>\n",
       "      <td>0.9073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.166700</td>\n",
       "      <td>2.145332</td>\n",
       "      <td>0.0491</td>\n",
       "      <td>0.3544</td>\n",
       "      <td>9.3639</td>\n",
       "      <td>1.6448</td>\n",
       "      <td>1.7561</td>\n",
       "      <td>0.9751</td>\n",
       "      <td>1.4993</td>\n",
       "      <td>63.3980</td>\n",
       "      <td>1.2511</td>\n",
       "      <td>4.4976</td>\n",
       "      <td>55.1166</td>\n",
       "      <td>0.9369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>2.168600</td>\n",
       "      <td>2.147902</td>\n",
       "      <td>0.0643</td>\n",
       "      <td>0.4187</td>\n",
       "      <td>9.4528</td>\n",
       "      <td>1.8646</td>\n",
       "      <td>10.3176</td>\n",
       "      <td>1.5444</td>\n",
       "      <td>1.74</td>\n",
       "      <td>66.9024</td>\n",
       "      <td>1.1628</td>\n",
       "      <td>4.2695</td>\n",
       "      <td>55.4950</td>\n",
       "      <td>0.8966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>2.175500</td>\n",
       "      <td>2.150563</td>\n",
       "      <td>0.0464</td>\n",
       "      <td>0.3786</td>\n",
       "      <td>8.4240</td>\n",
       "      <td>1.3676</td>\n",
       "      <td>5.3217</td>\n",
       "      <td>1.9343</td>\n",
       "      <td>1.7263</td>\n",
       "      <td>54.7173</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>3.8389</td>\n",
       "      <td>58.3562</td>\n",
       "      <td>0.9642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>2.170500</td>\n",
       "      <td>2.147200</td>\n",
       "      <td>0.0615</td>\n",
       "      <td>0.4006</td>\n",
       "      <td>9.1607</td>\n",
       "      <td>1.7775</td>\n",
       "      <td>20.3731</td>\n",
       "      <td>2.1464</td>\n",
       "      <td>2.6328</td>\n",
       "      <td>57.5921</td>\n",
       "      <td>1.0999</td>\n",
       "      <td>3.8966</td>\n",
       "      <td>55.9620</td>\n",
       "      <td>1.0040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>2.163500</td>\n",
       "      <td>2.146706</td>\n",
       "      <td>0.0253</td>\n",
       "      <td>0.2213</td>\n",
       "      <td>8.4544</td>\n",
       "      <td>1.777</td>\n",
       "      <td>5.2517</td>\n",
       "      <td>3.0229</td>\n",
       "      <td>1.1510</td>\n",
       "      <td>69.5715</td>\n",
       "      <td>1.5253</td>\n",
       "      <td>3.3689</td>\n",
       "      <td>57.0429</td>\n",
       "      <td>0.9652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.165500</td>\n",
       "      <td>2.144544</td>\n",
       "      <td>0.0402</td>\n",
       "      <td>0.3991</td>\n",
       "      <td>8.0179</td>\n",
       "      <td>1.3257</td>\n",
       "      <td>8.2901</td>\n",
       "      <td>2.0792</td>\n",
       "      <td>1.8311</td>\n",
       "      <td>62.2801</td>\n",
       "      <td>1.1983</td>\n",
       "      <td>4.3583</td>\n",
       "      <td>56.1151</td>\n",
       "      <td>0.935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>2.164100</td>\n",
       "      <td>2.144882</td>\n",
       "      <td>0.0491</td>\n",
       "      <td>0.4383</td>\n",
       "      <td>7.6615</td>\n",
       "      <td>1.8687</td>\n",
       "      <td>13.3914</td>\n",
       "      <td>2.5156</td>\n",
       "      <td>2.1037</td>\n",
       "      <td>65.0062</td>\n",
       "      <td>1.2319</td>\n",
       "      <td>3.1486</td>\n",
       "      <td>57.097</td>\n",
       "      <td>0.963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>2.169400</td>\n",
       "      <td>2.147938</td>\n",
       "      <td>0.0577</td>\n",
       "      <td>0.4387</td>\n",
       "      <td>7.7457</td>\n",
       "      <td>1.4314</td>\n",
       "      <td>12.0848</td>\n",
       "      <td>1.4763</td>\n",
       "      <td>1.6958</td>\n",
       "      <td>65.7997</td>\n",
       "      <td>1.2986</td>\n",
       "      <td>3.3048</td>\n",
       "      <td>54.3005</td>\n",
       "      <td>0.9753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>2.170800</td>\n",
       "      <td>2.143510</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>0.5845</td>\n",
       "      <td>8.1033</td>\n",
       "      <td>1.9418</td>\n",
       "      <td>13.0967</td>\n",
       "      <td>1.9542</td>\n",
       "      <td>1.9050</td>\n",
       "      <td>66.8309</td>\n",
       "      <td>1.251</td>\n",
       "      <td>3.2087</td>\n",
       "      <td>58.4831</td>\n",
       "      <td>0.9790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>2.170600</td>\n",
       "      <td>2.144104</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.4707</td>\n",
       "      <td>8.7602</td>\n",
       "      <td>1.6896</td>\n",
       "      <td>16.6440</td>\n",
       "      <td>1.6097</td>\n",
       "      <td>2.3171</td>\n",
       "      <td>59.1666</td>\n",
       "      <td>1.1266</td>\n",
       "      <td>4.2530</td>\n",
       "      <td>54.1434</td>\n",
       "      <td>0.9916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.161500</td>\n",
       "      <td>2.143021</td>\n",
       "      <td>0.0571</td>\n",
       "      <td>0.48</td>\n",
       "      <td>8.5177</td>\n",
       "      <td>1.9937</td>\n",
       "      <td>17.2228</td>\n",
       "      <td>2.4200</td>\n",
       "      <td>1.9046</td>\n",
       "      <td>63.2049</td>\n",
       "      <td>1.1929</td>\n",
       "      <td>3.4066</td>\n",
       "      <td>58.3331</td>\n",
       "      <td>0.9962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>2.164900</td>\n",
       "      <td>2.149398</td>\n",
       "      <td>0.0491</td>\n",
       "      <td>0.3383</td>\n",
       "      <td>9.52</td>\n",
       "      <td>1.8023</td>\n",
       "      <td>11.7616</td>\n",
       "      <td>1.6463</td>\n",
       "      <td>2.7217</td>\n",
       "      <td>64.6732</td>\n",
       "      <td>1.1578</td>\n",
       "      <td>4.2118</td>\n",
       "      <td>56.6955</td>\n",
       "      <td>0.9796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>2.163300</td>\n",
       "      <td>2.148417</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.2064</td>\n",
       "      <td>9.2622</td>\n",
       "      <td>1.9727</td>\n",
       "      <td>11.9668</td>\n",
       "      <td>1.9126</td>\n",
       "      <td>1.6726</td>\n",
       "      <td>61.0934</td>\n",
       "      <td>1.1395</td>\n",
       "      <td>3.947</td>\n",
       "      <td>56.87</td>\n",
       "      <td>0.9743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>2.166500</td>\n",
       "      <td>2.144852</td>\n",
       "      <td>0.0673</td>\n",
       "      <td>0.4025</td>\n",
       "      <td>9.7743</td>\n",
       "      <td>2.0051</td>\n",
       "      <td>14.1446</td>\n",
       "      <td>1.4115</td>\n",
       "      <td>2.519</td>\n",
       "      <td>63.4117</td>\n",
       "      <td>1.0076</td>\n",
       "      <td>4.7963</td>\n",
       "      <td>54.8698</td>\n",
       "      <td>0.9758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>2.166200</td>\n",
       "      <td>2.141085</td>\n",
       "      <td>0.0377</td>\n",
       "      <td>0.4315</td>\n",
       "      <td>7.5724</td>\n",
       "      <td>1.828</td>\n",
       "      <td>7.906</td>\n",
       "      <td>1.561</td>\n",
       "      <td>1.7462</td>\n",
       "      <td>67.4031</td>\n",
       "      <td>1.2349</td>\n",
       "      <td>2.871</td>\n",
       "      <td>59.9804</td>\n",
       "      <td>0.9627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.165600</td>\n",
       "      <td>2.140293</td>\n",
       "      <td>0.0608</td>\n",
       "      <td>0.297</td>\n",
       "      <td>9.3684</td>\n",
       "      <td>1.8977</td>\n",
       "      <td>12.2967</td>\n",
       "      <td>2.3548</td>\n",
       "      <td>1.8241</td>\n",
       "      <td>64.0105</td>\n",
       "      <td>1.1694</td>\n",
       "      <td>3.5496</td>\n",
       "      <td>59.2121</td>\n",
       "      <td>1.0522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>2.167400</td>\n",
       "      <td>2.141784</td>\n",
       "      <td>0.0382</td>\n",
       "      <td>0.4077</td>\n",
       "      <td>8.1397</td>\n",
       "      <td>1.7804</td>\n",
       "      <td>6.3777</td>\n",
       "      <td>3.2091</td>\n",
       "      <td>1.4582</td>\n",
       "      <td>69.1813</td>\n",
       "      <td>1.3873</td>\n",
       "      <td>3.4269</td>\n",
       "      <td>60.4838</td>\n",
       "      <td>1.0286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>2.164000</td>\n",
       "      <td>2.143473</td>\n",
       "      <td>0.0282</td>\n",
       "      <td>0.2078</td>\n",
       "      <td>9.5784</td>\n",
       "      <td>2.0426</td>\n",
       "      <td>12.7725</td>\n",
       "      <td>1.8798</td>\n",
       "      <td>2.2886</td>\n",
       "      <td>60.6816</td>\n",
       "      <td>1.0048</td>\n",
       "      <td>4.4184</td>\n",
       "      <td>54.3635</td>\n",
       "      <td>1.0091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>2.167400</td>\n",
       "      <td>2.144727</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>0.2931</td>\n",
       "      <td>8.5391</td>\n",
       "      <td>1.489</td>\n",
       "      <td>10.4272</td>\n",
       "      <td>3.2098</td>\n",
       "      <td>2.1077</td>\n",
       "      <td>56.1344</td>\n",
       "      <td>1.3163</td>\n",
       "      <td>3.7957</td>\n",
       "      <td>58.2633</td>\n",
       "      <td>0.9825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>2.163700</td>\n",
       "      <td>2.140200</td>\n",
       "      <td>0.0532</td>\n",
       "      <td>0.4120</td>\n",
       "      <td>8.8560</td>\n",
       "      <td>1.7765</td>\n",
       "      <td>14.8618</td>\n",
       "      <td>2.6877</td>\n",
       "      <td>2.4774</td>\n",
       "      <td>59.2996</td>\n",
       "      <td>1.3370</td>\n",
       "      <td>3.7571</td>\n",
       "      <td>55.9086</td>\n",
       "      <td>0.9642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.170000</td>\n",
       "      <td>2.141221</td>\n",
       "      <td>0.0517</td>\n",
       "      <td>0.4105</td>\n",
       "      <td>8.1996</td>\n",
       "      <td>1.8320</td>\n",
       "      <td>8.3608</td>\n",
       "      <td>3.1228</td>\n",
       "      <td>1.7506</td>\n",
       "      <td>65.537</td>\n",
       "      <td>1.3786</td>\n",
       "      <td>3.4678</td>\n",
       "      <td>57.8808</td>\n",
       "      <td>0.9890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>2.160100</td>\n",
       "      <td>2.140506</td>\n",
       "      <td>0.0454</td>\n",
       "      <td>0.4409</td>\n",
       "      <td>8.2931</td>\n",
       "      <td>1.8585</td>\n",
       "      <td>11.7987</td>\n",
       "      <td>2.9098</td>\n",
       "      <td>1.8839</td>\n",
       "      <td>67.0283</td>\n",
       "      <td>1.4623</td>\n",
       "      <td>3.2886</td>\n",
       "      <td>58.9492</td>\n",
       "      <td>0.983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>2.163800</td>\n",
       "      <td>2.140397</td>\n",
       "      <td>0.0521</td>\n",
       "      <td>0.5347</td>\n",
       "      <td>7.725</td>\n",
       "      <td>1.9142</td>\n",
       "      <td>13.3947</td>\n",
       "      <td>3.1921</td>\n",
       "      <td>2.0596</td>\n",
       "      <td>67.0869</td>\n",
       "      <td>1.3733</td>\n",
       "      <td>3.1282</td>\n",
       "      <td>59.1247</td>\n",
       "      <td>0.9864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>2.159300</td>\n",
       "      <td>2.140521</td>\n",
       "      <td>0.0430</td>\n",
       "      <td>0.4079</td>\n",
       "      <td>8.3581</td>\n",
       "      <td>1.8447</td>\n",
       "      <td>8.9311</td>\n",
       "      <td>3.3442</td>\n",
       "      <td>1.5714</td>\n",
       "      <td>69.1175</td>\n",
       "      <td>1.4603</td>\n",
       "      <td>3.2940</td>\n",
       "      <td>59.9664</td>\n",
       "      <td>0.9655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>2.163800</td>\n",
       "      <td>2.145157</td>\n",
       "      <td>0.0381</td>\n",
       "      <td>0.2266</td>\n",
       "      <td>8.6856</td>\n",
       "      <td>1.6937</td>\n",
       "      <td>10.423</td>\n",
       "      <td>3.5953</td>\n",
       "      <td>1.9101</td>\n",
       "      <td>62.9183</td>\n",
       "      <td>1.2593</td>\n",
       "      <td>3.4156</td>\n",
       "      <td>58.6470</td>\n",
       "      <td>0.9111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.160400</td>\n",
       "      <td>2.140543</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.3281</td>\n",
       "      <td>9.1151</td>\n",
       "      <td>1.8579</td>\n",
       "      <td>8.0144</td>\n",
       "      <td>3.2681</td>\n",
       "      <td>1.8921</td>\n",
       "      <td>68.4889</td>\n",
       "      <td>1.4545</td>\n",
       "      <td>4.1120</td>\n",
       "      <td>57.4855</td>\n",
       "      <td>0.9240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>2.163600</td>\n",
       "      <td>2.140946</td>\n",
       "      <td>0.0389</td>\n",
       "      <td>0.31</td>\n",
       "      <td>8.4516</td>\n",
       "      <td>1.5606</td>\n",
       "      <td>9.7147</td>\n",
       "      <td>2.9796</td>\n",
       "      <td>2.1488</td>\n",
       "      <td>59.1518</td>\n",
       "      <td>1.2889</td>\n",
       "      <td>3.6094</td>\n",
       "      <td>58.4556</td>\n",
       "      <td>0.9998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>2.164800</td>\n",
       "      <td>2.140594</td>\n",
       "      <td>0.0343</td>\n",
       "      <td>0.2849</td>\n",
       "      <td>9.1474</td>\n",
       "      <td>1.9640</td>\n",
       "      <td>12.2425</td>\n",
       "      <td>3.185</td>\n",
       "      <td>1.9292</td>\n",
       "      <td>66.8414</td>\n",
       "      <td>1.1847</td>\n",
       "      <td>3.881</td>\n",
       "      <td>58.9737</td>\n",
       "      <td>0.9880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>2.161000</td>\n",
       "      <td>2.139923</td>\n",
       "      <td>0.0593</td>\n",
       "      <td>0.4545</td>\n",
       "      <td>8.5297</td>\n",
       "      <td>1.9401</td>\n",
       "      <td>13.6354</td>\n",
       "      <td>3.0009</td>\n",
       "      <td>2.0427</td>\n",
       "      <td>66.0541</td>\n",
       "      <td>1.3149</td>\n",
       "      <td>3.0893</td>\n",
       "      <td>59.3628</td>\n",
       "      <td>0.9787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>2.161400</td>\n",
       "      <td>2.143853</td>\n",
       "      <td>0.0412</td>\n",
       "      <td>0.2908</td>\n",
       "      <td>9.1394</td>\n",
       "      <td>2.1705</td>\n",
       "      <td>16.8236</td>\n",
       "      <td>2.1318</td>\n",
       "      <td>2.0633</td>\n",
       "      <td>64.9181</td>\n",
       "      <td>1.3331</td>\n",
       "      <td>3.4859</td>\n",
       "      <td>56.8425</td>\n",
       "      <td>0.9867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.158800</td>\n",
       "      <td>2.139601</td>\n",
       "      <td>0.0606</td>\n",
       "      <td>0.3826</td>\n",
       "      <td>9.2153</td>\n",
       "      <td>1.9325</td>\n",
       "      <td>11.6603</td>\n",
       "      <td>2.4157</td>\n",
       "      <td>2.3076</td>\n",
       "      <td>65.1258</td>\n",
       "      <td>1.2731</td>\n",
       "      <td>3.6716</td>\n",
       "      <td>58.9543</td>\n",
       "      <td>1.0086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>2.166000</td>\n",
       "      <td>2.141308</td>\n",
       "      <td>0.0581</td>\n",
       "      <td>0.4347</td>\n",
       "      <td>8.9460</td>\n",
       "      <td>2.3808</td>\n",
       "      <td>28.9318</td>\n",
       "      <td>1.9964</td>\n",
       "      <td>2.7787</td>\n",
       "      <td>57.717</td>\n",
       "      <td>1.0921</td>\n",
       "      <td>3.0353</td>\n",
       "      <td>56.606</td>\n",
       "      <td>0.9698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>2.152700</td>\n",
       "      <td>2.139039</td>\n",
       "      <td>0.0581</td>\n",
       "      <td>0.4554</td>\n",
       "      <td>8.7391</td>\n",
       "      <td>1.8636</td>\n",
       "      <td>15.7444</td>\n",
       "      <td>2.4736</td>\n",
       "      <td>2.177</td>\n",
       "      <td>64.374</td>\n",
       "      <td>1.3225</td>\n",
       "      <td>3.4054</td>\n",
       "      <td>58.4586</td>\n",
       "      <td>0.9752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>2.163400</td>\n",
       "      <td>2.141056</td>\n",
       "      <td>0.0564</td>\n",
       "      <td>0.5533</td>\n",
       "      <td>8.8147</td>\n",
       "      <td>1.9192</td>\n",
       "      <td>19.7707</td>\n",
       "      <td>2.6858</td>\n",
       "      <td>2.4375</td>\n",
       "      <td>60.8247</td>\n",
       "      <td>1.3439</td>\n",
       "      <td>3.3541</td>\n",
       "      <td>58.445</td>\n",
       "      <td>1.0068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>2.161400</td>\n",
       "      <td>2.139030</td>\n",
       "      <td>0.0478</td>\n",
       "      <td>0.465</td>\n",
       "      <td>8.4037</td>\n",
       "      <td>1.9846</td>\n",
       "      <td>13.3242</td>\n",
       "      <td>2.9663</td>\n",
       "      <td>1.8607</td>\n",
       "      <td>65.1387</td>\n",
       "      <td>1.3141</td>\n",
       "      <td>3.3096</td>\n",
       "      <td>59.8173</td>\n",
       "      <td>1.0288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.154900</td>\n",
       "      <td>2.138509</td>\n",
       "      <td>0.0438</td>\n",
       "      <td>0.4391</td>\n",
       "      <td>8.1997</td>\n",
       "      <td>1.8533</td>\n",
       "      <td>12.2257</td>\n",
       "      <td>2.9737</td>\n",
       "      <td>2.0579</td>\n",
       "      <td>65.2310</td>\n",
       "      <td>1.3596</td>\n",
       "      <td>3.0944</td>\n",
       "      <td>58.9716</td>\n",
       "      <td>0.981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>2.159100</td>\n",
       "      <td>2.138602</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.482</td>\n",
       "      <td>8.5253</td>\n",
       "      <td>2.0947</td>\n",
       "      <td>16.6609</td>\n",
       "      <td>2.3282</td>\n",
       "      <td>2.3620</td>\n",
       "      <td>66.3532</td>\n",
       "      <td>1.1994</td>\n",
       "      <td>3.1423</td>\n",
       "      <td>58.5081</td>\n",
       "      <td>0.9913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>2.164000</td>\n",
       "      <td>2.138863</td>\n",
       "      <td>0.0562</td>\n",
       "      <td>0.5567</td>\n",
       "      <td>8.8460</td>\n",
       "      <td>2.1365</td>\n",
       "      <td>17.5744</td>\n",
       "      <td>2.4043</td>\n",
       "      <td>2.3879</td>\n",
       "      <td>65.8006</td>\n",
       "      <td>1.1794</td>\n",
       "      <td>3.3197</td>\n",
       "      <td>58.5639</td>\n",
       "      <td>1.0388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>2.148300</td>\n",
       "      <td>2.139187</td>\n",
       "      <td>0.0501</td>\n",
       "      <td>0.4386</td>\n",
       "      <td>8.6231</td>\n",
       "      <td>1.9745</td>\n",
       "      <td>17.0416</td>\n",
       "      <td>2.7269</td>\n",
       "      <td>2.3526</td>\n",
       "      <td>62.9244</td>\n",
       "      <td>1.2036</td>\n",
       "      <td>3.3737</td>\n",
       "      <td>58.8657</td>\n",
       "      <td>0.9614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>2.166900</td>\n",
       "      <td>2.139986</td>\n",
       "      <td>0.0440</td>\n",
       "      <td>0.4925</td>\n",
       "      <td>8.2921</td>\n",
       "      <td>1.97</td>\n",
       "      <td>14.5186</td>\n",
       "      <td>2.6501</td>\n",
       "      <td>2.1470</td>\n",
       "      <td>63.4452</td>\n",
       "      <td>1.1611</td>\n",
       "      <td>3.1826</td>\n",
       "      <td>59.6422</td>\n",
       "      <td>0.9903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.154500</td>\n",
       "      <td>2.139585</td>\n",
       "      <td>0.0524</td>\n",
       "      <td>0.5205</td>\n",
       "      <td>8.8527</td>\n",
       "      <td>2.3358</td>\n",
       "      <td>22.6954</td>\n",
       "      <td>2.0795</td>\n",
       "      <td>2.6323</td>\n",
       "      <td>64.1276</td>\n",
       "      <td>1.1261</td>\n",
       "      <td>3.247</td>\n",
       "      <td>56.8722</td>\n",
       "      <td>0.9848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>2.163100</td>\n",
       "      <td>2.138093</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.5805</td>\n",
       "      <td>8.557</td>\n",
       "      <td>2.1183</td>\n",
       "      <td>16.2491</td>\n",
       "      <td>2.3305</td>\n",
       "      <td>2.4028</td>\n",
       "      <td>65.0234</td>\n",
       "      <td>1.1489</td>\n",
       "      <td>3.2649</td>\n",
       "      <td>58.7666</td>\n",
       "      <td>1.0438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>2.156000</td>\n",
       "      <td>2.137398</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.5368</td>\n",
       "      <td>8.8216</td>\n",
       "      <td>2.1882</td>\n",
       "      <td>19.7274</td>\n",
       "      <td>2.3517</td>\n",
       "      <td>2.5144</td>\n",
       "      <td>64.3983</td>\n",
       "      <td>1.1238</td>\n",
       "      <td>3.3693</td>\n",
       "      <td>57.9055</td>\n",
       "      <td>0.9992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>2.155400</td>\n",
       "      <td>2.137542</td>\n",
       "      <td>0.0537</td>\n",
       "      <td>0.5318</td>\n",
       "      <td>8.7059</td>\n",
       "      <td>2.1577</td>\n",
       "      <td>18.6778</td>\n",
       "      <td>2.3227</td>\n",
       "      <td>2.4168</td>\n",
       "      <td>64.462</td>\n",
       "      <td>1.1299</td>\n",
       "      <td>3.2815</td>\n",
       "      <td>58.5873</td>\n",
       "      <td>1.0409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>2.153800</td>\n",
       "      <td>2.137743</td>\n",
       "      <td>0.0528</td>\n",
       "      <td>0.4911</td>\n",
       "      <td>8.6157</td>\n",
       "      <td>2.2581</td>\n",
       "      <td>20.5116</td>\n",
       "      <td>2.2375</td>\n",
       "      <td>2.5453</td>\n",
       "      <td>65.0676</td>\n",
       "      <td>1.1189</td>\n",
       "      <td>3.1076</td>\n",
       "      <td>58.1485</td>\n",
       "      <td>1.0275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.159300</td>\n",
       "      <td>2.137935</td>\n",
       "      <td>0.0456</td>\n",
       "      <td>0.4654</td>\n",
       "      <td>8.57</td>\n",
       "      <td>2.0910</td>\n",
       "      <td>16.0118</td>\n",
       "      <td>2.4210</td>\n",
       "      <td>2.3656</td>\n",
       "      <td>65.3855</td>\n",
       "      <td>1.1595</td>\n",
       "      <td>3.1958</td>\n",
       "      <td>58.4195</td>\n",
       "      <td>1.0251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>2.161500</td>\n",
       "      <td>2.138137</td>\n",
       "      <td>0.0463</td>\n",
       "      <td>0.4624</td>\n",
       "      <td>8.6675</td>\n",
       "      <td>2.1676</td>\n",
       "      <td>18.8907</td>\n",
       "      <td>2.4108</td>\n",
       "      <td>2.4221</td>\n",
       "      <td>64.4582</td>\n",
       "      <td>1.1606</td>\n",
       "      <td>3.2281</td>\n",
       "      <td>57.9831</td>\n",
       "      <td>0.9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>2.156400</td>\n",
       "      <td>2.138716</td>\n",
       "      <td>0.0434</td>\n",
       "      <td>0.4201</td>\n",
       "      <td>8.7637</td>\n",
       "      <td>2.1105</td>\n",
       "      <td>19.2530</td>\n",
       "      <td>2.3178</td>\n",
       "      <td>2.5416</td>\n",
       "      <td>63.2492</td>\n",
       "      <td>1.1501</td>\n",
       "      <td>3.2808</td>\n",
       "      <td>57.7613</td>\n",
       "      <td>1.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>2.162100</td>\n",
       "      <td>2.138059</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.432</td>\n",
       "      <td>8.6745</td>\n",
       "      <td>2.1579</td>\n",
       "      <td>18.8113</td>\n",
       "      <td>2.3051</td>\n",
       "      <td>2.5385</td>\n",
       "      <td>64.6334</td>\n",
       "      <td>1.123</td>\n",
       "      <td>3.2650</td>\n",
       "      <td>57.7651</td>\n",
       "      <td>1.0083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>2.152600</td>\n",
       "      <td>2.138129</td>\n",
       "      <td>0.0455</td>\n",
       "      <td>0.4384</td>\n",
       "      <td>8.6679</td>\n",
       "      <td>2.1563</td>\n",
       "      <td>18.1915</td>\n",
       "      <td>2.3298</td>\n",
       "      <td>2.4815</td>\n",
       "      <td>64.9901</td>\n",
       "      <td>1.1407</td>\n",
       "      <td>3.2527</td>\n",
       "      <td>57.8362</td>\n",
       "      <td>1.0039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>2.160100</td>\n",
       "      <td>2.138243</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.4443</td>\n",
       "      <td>8.7150</td>\n",
       "      <td>2.1781</td>\n",
       "      <td>18.7611</td>\n",
       "      <td>2.2461</td>\n",
       "      <td>2.5084</td>\n",
       "      <td>64.6839</td>\n",
       "      <td>1.1299</td>\n",
       "      <td>3.2356</td>\n",
       "      <td>57.6714</td>\n",
       "      <td>1.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>2.154400</td>\n",
       "      <td>2.138057</td>\n",
       "      <td>0.0466</td>\n",
       "      <td>0.4432</td>\n",
       "      <td>8.7396</td>\n",
       "      <td>2.187</td>\n",
       "      <td>18.7136</td>\n",
       "      <td>2.2299</td>\n",
       "      <td>2.5113</td>\n",
       "      <td>64.8439</td>\n",
       "      <td>1.1271</td>\n",
       "      <td>3.2647</td>\n",
       "      <td>57.6525</td>\n",
       "      <td>1.0208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>2.164000</td>\n",
       "      <td>2.137875</td>\n",
       "      <td>0.0463</td>\n",
       "      <td>0.4461</td>\n",
       "      <td>8.694</td>\n",
       "      <td>2.1581</td>\n",
       "      <td>18.1669</td>\n",
       "      <td>2.2893</td>\n",
       "      <td>2.4856</td>\n",
       "      <td>64.9719</td>\n",
       "      <td>1.1346</td>\n",
       "      <td>3.2515</td>\n",
       "      <td>57.8617</td>\n",
       "      <td>1.0191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>2.157200</td>\n",
       "      <td>2.137907</td>\n",
       "      <td>0.0465</td>\n",
       "      <td>0.4462</td>\n",
       "      <td>8.7045</td>\n",
       "      <td>2.1635</td>\n",
       "      <td>18.2418</td>\n",
       "      <td>2.2616</td>\n",
       "      <td>2.4899</td>\n",
       "      <td>64.986</td>\n",
       "      <td>1.1322</td>\n",
       "      <td>3.2558</td>\n",
       "      <td>57.7601</td>\n",
       "      <td>1.0216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=17746, training_loss=2.16817844699983, metrics={'train_runtime': 16366.3096, 'train_samples_per_second': 4.337, 'train_steps_per_second': 1.084, 'total_flos': 0.0, 'train_loss': 2.16817844699983, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oanda data all pairs past 2009\n",
    "\n",
    "# SGCONV transformer architecture! lr of 1e-3, batch size 5 hidden size 512, 4 heads\n",
    "# NO dropout, weight decay\n",
    "# NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# ce loss with conditioned kelly betting\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4402' max='4402' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4402/4402 41:26, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Day profit</th>\n",
       "      <th>Day sharpe</th>\n",
       "      <th>Trade %</th>\n",
       "      <th>Full trade %</th>\n",
       "      <th>Full trade accuracy</th>\n",
       "      <th>Full trade g/l</th>\n",
       "      <th>Medium trade %</th>\n",
       "      <th>Medium trade accuracy</th>\n",
       "      <th>Medium trade g/l</th>\n",
       "      <th>Small trade %</th>\n",
       "      <th>Small trade accuracy</th>\n",
       "      <th>Small trade g/l</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.210500</td>\n",
       "      <td>2.171260</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.0719</td>\n",
       "      <td>5.3201</td>\n",
       "      <td>0.4756</td>\n",
       "      <td>1.2936</td>\n",
       "      <td>0.7324</td>\n",
       "      <td>0.5752</td>\n",
       "      <td>8.1581</td>\n",
       "      <td>1.1005</td>\n",
       "      <td>0.9064</td>\n",
       "      <td>30.9789</td>\n",
       "      <td>1.1299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.170500</td>\n",
       "      <td>2.160216</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.3422</td>\n",
       "      <td>6.9082</td>\n",
       "      <td>0.8293</td>\n",
       "      <td>0.0251</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.1713</td>\n",
       "      <td>2.4954</td>\n",
       "      <td>0.8416</td>\n",
       "      <td>1.1363</td>\n",
       "      <td>49.9541</td>\n",
       "      <td>1.2737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.168800</td>\n",
       "      <td>2.160419</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.1641</td>\n",
       "      <td>7.8999</td>\n",
       "      <td>0.8098</td>\n",
       "      <td>0.0129</td>\n",
       "      <td>0.2321</td>\n",
       "      <td>0.3131</td>\n",
       "      <td>5.2614</td>\n",
       "      <td>0.7012</td>\n",
       "      <td>3.2679</td>\n",
       "      <td>51.7262</td>\n",
       "      <td>0.9372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.162600</td>\n",
       "      <td>2.153858</td>\n",
       "      <td>-0.0048</td>\n",
       "      <td>-0.0378</td>\n",
       "      <td>6.3985</td>\n",
       "      <td>0.942</td>\n",
       "      <td>0.3099</td>\n",
       "      <td>1.1583</td>\n",
       "      <td>0.5256</td>\n",
       "      <td>14.5606</td>\n",
       "      <td>0.9703</td>\n",
       "      <td>1.4190</td>\n",
       "      <td>55.1988</td>\n",
       "      <td>0.7953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.155600</td>\n",
       "      <td>2.152036</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>6.4033</td>\n",
       "      <td>1.1714</td>\n",
       "      <td>4.1570</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.5486</td>\n",
       "      <td>37.9966</td>\n",
       "      <td>0.7135</td>\n",
       "      <td>1.8075</td>\n",
       "      <td>53.5275</td>\n",
       "      <td>0.7664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.161800</td>\n",
       "      <td>2.148250</td>\n",
       "      <td>0.0249</td>\n",
       "      <td>0.3081</td>\n",
       "      <td>6.5952</td>\n",
       "      <td>1.4711</td>\n",
       "      <td>0.3473</td>\n",
       "      <td>0.1748</td>\n",
       "      <td>0.317</td>\n",
       "      <td>60.0987</td>\n",
       "      <td>1.0863</td>\n",
       "      <td>2.4341</td>\n",
       "      <td>61.0906</td>\n",
       "      <td>0.9494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.153100</td>\n",
       "      <td>2.147601</td>\n",
       "      <td>0.0361</td>\n",
       "      <td>0.2121</td>\n",
       "      <td>8.5314</td>\n",
       "      <td>1.4307</td>\n",
       "      <td>0.2988</td>\n",
       "      <td>1.0577</td>\n",
       "      <td>0.2738</td>\n",
       "      <td>62.7951</td>\n",
       "      <td>0.7730</td>\n",
       "      <td>3.4826</td>\n",
       "      <td>59.5719</td>\n",
       "      <td>0.9755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.158700</td>\n",
       "      <td>2.147372</td>\n",
       "      <td>-0.0039</td>\n",
       "      <td>-0.0418</td>\n",
       "      <td>6.9273</td>\n",
       "      <td>1.2688</td>\n",
       "      <td>0.</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.2213</td>\n",
       "      <td>4.3827</td>\n",
       "      <td>0.2958</td>\n",
       "      <td>2.2188</td>\n",
       "      <td>53.5598</td>\n",
       "      <td>0.8021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.161500</td>\n",
       "      <td>2.148000</td>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.2717</td>\n",
       "      <td>6.3906</td>\n",
       "      <td>1.4182</td>\n",
       "      <td>1.9778</td>\n",
       "      <td>3.6387</td>\n",
       "      <td>0.5749</td>\n",
       "      <td>62.2778</td>\n",
       "      <td>1.3274</td>\n",
       "      <td>2.2574</td>\n",
       "      <td>60.0166</td>\n",
       "      <td>1.0657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.153700</td>\n",
       "      <td>2.143505</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.2621</td>\n",
       "      <td>7.5424</td>\n",
       "      <td>1.4698</td>\n",
       "      <td>3.3698</td>\n",
       "      <td>0.5353</td>\n",
       "      <td>1.4021</td>\n",
       "      <td>62.7649</td>\n",
       "      <td>1.1608</td>\n",
       "      <td>3.9239</td>\n",
       "      <td>57.2066</td>\n",
       "      <td>0.9270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.155400</td>\n",
       "      <td>2.141653</td>\n",
       "      <td>0.0389</td>\n",
       "      <td>0.3555</td>\n",
       "      <td>6.9689</td>\n",
       "      <td>1.4902</td>\n",
       "      <td>0.8676</td>\n",
       "      <td>1.1632</td>\n",
       "      <td>0.3681</td>\n",
       "      <td>63.8810</td>\n",
       "      <td>0.9017</td>\n",
       "      <td>2.1250</td>\n",
       "      <td>62.4681</td>\n",
       "      <td>0.9356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.153700</td>\n",
       "      <td>2.146128</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.1889</td>\n",
       "      <td>7.2357</td>\n",
       "      <td>1.4238</td>\n",
       "      <td>0.5932</td>\n",
       "      <td>0.2415</td>\n",
       "      <td>0.9415</td>\n",
       "      <td>57.3707</td>\n",
       "      <td>1.1089</td>\n",
       "      <td>2.8615</td>\n",
       "      <td>57.1585</td>\n",
       "      <td>0.9363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.151100</td>\n",
       "      <td>2.144964</td>\n",
       "      <td>0.0275</td>\n",
       "      <td>0.3172</td>\n",
       "      <td>7.0741</td>\n",
       "      <td>1.425</td>\n",
       "      <td>1.9684</td>\n",
       "      <td>1.4257</td>\n",
       "      <td>0.7486</td>\n",
       "      <td>66.2906</td>\n",
       "      <td>1.3606</td>\n",
       "      <td>2.6462</td>\n",
       "      <td>61.8882</td>\n",
       "      <td>1.0091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.147400</td>\n",
       "      <td>2.143468</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.3825</td>\n",
       "      <td>6.6906</td>\n",
       "      <td>1.5142</td>\n",
       "      <td>2.6098</td>\n",
       "      <td>3.4337</td>\n",
       "      <td>1.3664</td>\n",
       "      <td>64.301</td>\n",
       "      <td>1.4086</td>\n",
       "      <td>3.0323</td>\n",
       "      <td>57.8041</td>\n",
       "      <td>1.0586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.143000</td>\n",
       "      <td>2.144000</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.3968</td>\n",
       "      <td>6.7070</td>\n",
       "      <td>1.4933</td>\n",
       "      <td>1.4454</td>\n",
       "      <td>3.2272</td>\n",
       "      <td>0.7705</td>\n",
       "      <td>63.8381</td>\n",
       "      <td>1.3251</td>\n",
       "      <td>2.3704</td>\n",
       "      <td>61.6505</td>\n",
       "      <td>1.0427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.150100</td>\n",
       "      <td>2.143867</td>\n",
       "      <td>0.0413</td>\n",
       "      <td>0.4524</td>\n",
       "      <td>7.7395</td>\n",
       "      <td>1.6211</td>\n",
       "      <td>7.2168</td>\n",
       "      <td>3.0946</td>\n",
       "      <td>1.9193</td>\n",
       "      <td>61.1344</td>\n",
       "      <td>1.1724</td>\n",
       "      <td>3.4442</td>\n",
       "      <td>56.5499</td>\n",
       "      <td>1.0828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.148800</td>\n",
       "      <td>2.141060</td>\n",
       "      <td>0.0507</td>\n",
       "      <td>0.5040</td>\n",
       "      <td>7.86</td>\n",
       "      <td>1.6633</td>\n",
       "      <td>7.8987</td>\n",
       "      <td>2.0997</td>\n",
       "      <td>1.337</td>\n",
       "      <td>66.8850</td>\n",
       "      <td>1.2211</td>\n",
       "      <td>3.1566</td>\n",
       "      <td>60.3508</td>\n",
       "      <td>1.0750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.147500</td>\n",
       "      <td>2.141384</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.4441</td>\n",
       "      <td>7.2507</td>\n",
       "      <td>1.5270</td>\n",
       "      <td>3.9604</td>\n",
       "      <td>3.4903</td>\n",
       "      <td>1.0248</td>\n",
       "      <td>64.4078</td>\n",
       "      <td>1.1422</td>\n",
       "      <td>2.7959</td>\n",
       "      <td>61.0577</td>\n",
       "      <td>1.0336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.146600</td>\n",
       "      <td>2.141815</td>\n",
       "      <td>0.0471</td>\n",
       "      <td>0.461</td>\n",
       "      <td>7.4397</td>\n",
       "      <td>1.5463</td>\n",
       "      <td>3.8031</td>\n",
       "      <td>4.6073</td>\n",
       "      <td>1.0208</td>\n",
       "      <td>66.8846</td>\n",
       "      <td>1.2172</td>\n",
       "      <td>2.7361</td>\n",
       "      <td>61.8064</td>\n",
       "      <td>1.0491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.144400</td>\n",
       "      <td>2.140702</td>\n",
       "      <td>0.0409</td>\n",
       "      <td>0.46</td>\n",
       "      <td>7.6006</td>\n",
       "      <td>1.6464</td>\n",
       "      <td>7.5364</td>\n",
       "      <td>2.9991</td>\n",
       "      <td>1.4158</td>\n",
       "      <td>67.3000</td>\n",
       "      <td>1.1915</td>\n",
       "      <td>2.9524</td>\n",
       "      <td>60.7452</td>\n",
       "      <td>1.0891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.148500</td>\n",
       "      <td>2.140399</td>\n",
       "      <td>0.0388</td>\n",
       "      <td>0.4419</td>\n",
       "      <td>7.5572</td>\n",
       "      <td>1.6162</td>\n",
       "      <td>6.271</td>\n",
       "      <td>3.3312</td>\n",
       "      <td>1.3841</td>\n",
       "      <td>67.184</td>\n",
       "      <td>1.1918</td>\n",
       "      <td>3.0274</td>\n",
       "      <td>60.7701</td>\n",
       "      <td>1.0799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.148000</td>\n",
       "      <td>2.140455</td>\n",
       "      <td>0.0376</td>\n",
       "      <td>0.4272</td>\n",
       "      <td>7.5338</td>\n",
       "      <td>1.6079</td>\n",
       "      <td>5.6939</td>\n",
       "      <td>3.7648</td>\n",
       "      <td>1.3517</td>\n",
       "      <td>67.1912</td>\n",
       "      <td>1.2</td>\n",
       "      <td>3.0048</td>\n",
       "      <td>60.7766</td>\n",
       "      <td>1.0807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-bb1c47517fd0>:16: RuntimeWarning: Mean of empty slice.\n",
      "  'full trade g/l': soft_profit[(abs_trade >= .7) & (soft_profit > 0)].mean()\n",
      "C:\\Users\\micha\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "<ipython-input-2-bb1c47517fd0>:17: RuntimeWarning: Mean of empty slice.\n",
      "  / -soft_profit[(abs_trade >= .7) & (soft_profit < 0)].mean(),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4402, training_loss=2.156417083003639, metrics={'train_runtime': 2489.5752, 'train_samples_per_second': 14.144, 'train_steps_per_second': 1.768, 'total_flos': 0.0, 'train_loss': 2.156417083003639, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oanda data\n",
    "\n",
    "# SGCONV transformer architecture! lr of 1e-3, batch size 8 hidden size 320, 1 head\n",
    "# NO dropout, weight decay\n",
    "# NO diagonal attention allowed, NO rotary embed, norm or residual on conv embed, kernel size of 5\n",
    "\n",
    "# ce loss with conditioned kelly betting\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
